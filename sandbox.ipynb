{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-08T20:41:26.879421Z",
     "start_time": "2021-06-08T20:41:26.876140Z"
    }
   },
   "source": [
    "# Fairness Sandox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "Please run the code block below to install the necessary packages (if needed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# uncomment as needed\n",
    "\n",
    "# %pip install aif360\n",
    "# %pip install fairlearn\n",
    "# %pip install imbalanced-learn\n",
    "# %pip install matplotlib\n",
    "# %pip install numpy\n",
    "# %pip install pandas\n",
    "# %pip install seaborn\n",
    "# %pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T20:42:02.586362Z",
     "start_time": "2021-06-10T20:42:02.581026Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, roc_curve, auc\n",
    "from numpy import percentile\n",
    "\n",
    "import fairlearn\n",
    "from fairlearn.metrics import *\n",
    "from fairlearn.reductions import *\n",
    "import aif360\n",
    "\n",
    "import copy, random\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T20:42:06.040700Z",
     "start_time": "2021-06-10T20:42:06.035368Z"
    }
   },
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, short_name = '', path = '', cat_cols = [], num_cols = [],\n",
    "                 sens_attr = '', has_sens_attr = True,\n",
    "                 sep = '', synthetic = False):\n",
    "        self.short_name = short_name\n",
    "        self.path = path\n",
    "        self.cat_cols = cat_cols\n",
    "        self.num_cols = num_cols\n",
    "        self.has_sens_attr = has_sens_attr\n",
    "        if has_sens_attr:\n",
    "            self.sens_attr = sens_attr\n",
    "        if not synthetic:\n",
    "            self.df = pd.read_csv(path, sep = sep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T20:42:06.330948Z",
     "start_time": "2021-06-10T20:42:06.325325Z"
    }
   },
   "outputs": [],
   "source": [
    "# collection of datasets is a dictionary where keys = short name, values = Dataset object\n",
    "\n",
    "datasets = dict()\n",
    "\n",
    "def add_dataset(dataset):\n",
    "    if not isinstance(dataset, Dataset):\n",
    "        raise TypeError(\"Please enter a valid Dataset object\")\n",
    "    else:\n",
    "        if dataset.short_name not in datasets.keys():\n",
    "            datasets[dataset.short_name] = dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Popular Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T20:42:06.534123Z",
     "start_time": "2021-06-10T20:42:06.464059Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# example - adding a dataset\n",
    "path_adult_income = 'Datasets/adult.csv'\n",
    "cat_cols_adult = ['workclass', 'education','marital-status', 'occupation', 'relationship', 'race',\n",
    "            'gender', 'native-country','income']\n",
    "num_cols_adult = ['age', 'fnlwgt', 'educational-num', 'capital-gain', 'capital-loss', 'hours-per-week']\n",
    "adult_income = Dataset('adult_income', path_adult_income, cat_cols_adult, num_cols_adult, sep = \",\", sens_attr = 'race')\n",
    "\n",
    "add_dataset(adult_income)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T20:42:06.621748Z",
     "start_time": "2021-06-10T20:42:06.598777Z"
    }
   },
   "outputs": [],
   "source": [
    "cat_cols_por = ['school', 'sex', 'address','famsize','Pstatus','Mjob','Fjob','reason',\n",
    "       'guardian','schoolsup','famsup','paid', 'activities','nursery','higher', 'internet','romantic']\n",
    "num_cols_por = ['age', 'Medu', 'Fedu','traveltime','studytime','failures', 'famrel',\n",
    "       'freetime','goout','Dalc','Walc','health','absences','G1', 'G2', 'G3']\n",
    "\n",
    "add_dataset(Dataset(\"student_por\", path='Datasets/student-por.csv',\n",
    "                    cat_cols=cat_cols_por, num_cols=num_cols_por, sep = \";\", sens_attr = 'sex'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthetic Data Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create binary sensitive attribute\n",
    "def get_sensitive_feat(n, r):\n",
    "    num_minority = int(r * n)\n",
    "    num_majority = n - num_minority\n",
    "    \n",
    "    minority = np.zeros((num_minority, 1))\n",
    "    majority = np.ones((num_majority, 1))\n",
    "    \n",
    "    sens_feat = np.vstack((minority, majority))\n",
    "    \n",
    "    # shuffle so as to ensure randomness\n",
    "    np.random.shuffle(sens_feat)\n",
    "    \n",
    "    return sens_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cat_feats(num_cat_feats, cat_feats_levels, n):\n",
    "    cat_feats = []\n",
    "    for i in range(num_cat_feats):\n",
    "        levels = cat_feats_levels[i]\n",
    "        if levels < 2:\n",
    "            raise ValueError(\"Categorical features must have at least 2 classes!\")\n",
    "        vals = np.arange(levels)\n",
    "        cat = np.random.choice(vals, n, [0.5,0.5]).reshape(n, 1)\n",
    "        cat_feats.append(cat)\n",
    "    return np.hstack((cat_feats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distribution_plot(outcome_min = [], outcome_maj = [], outcome = [],\n",
    "                      threshold_min = 0.5, threshold_maj = 0.5,\n",
    "                      diff_dist = False):\n",
    "    \n",
    "    if diff_dist:\n",
    "\n",
    "        plt.figure(figsize=(17,7))\n",
    "\n",
    "        plt.subplot(1,2,1)\n",
    "        plt.hist(outcome_min,label='continuous outcome',bins='auto')\n",
    "        plt.axvline(threshold_min,color='red',label='threshold')\n",
    "        plt.xlabel(\"Continuous Outcome\")\n",
    "        plt.ylabel(\"Number of Samples\")\n",
    "        plt.title(\"Minority\")\n",
    "\n",
    "        plt.subplot(1,2,2)\n",
    "        plt.hist(outcome_maj,label='continuous outcome',bins='auto')\n",
    "        plt.axvline(threshold_maj,color='red',label='threshold')\n",
    "        plt.xlabel(\"Continuous Outcome\")\n",
    "        plt.ylabel(\"Number of Samples\")\n",
    "        plt.title(\"Majority\")\n",
    "        plt.show()\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        plt.figure(figsize=(17,7))\n",
    "        plt.subplot(1,2,1)\n",
    "        plt.hist(outcome,label='continuous outcome',bins='auto')\n",
    "        plt.axvline(threshold_min,color='red',label='threshold')\n",
    "        plt.xlabel(\"Continuous Outcome\")\n",
    "        plt.ylabel(\"Number of Samples\")\n",
    "        plt.title(\"Distribution of Outcomes\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attribute_names(df, num_numerical_cols, num_cat_cols):\n",
    "    col_names = []\n",
    "    for i in range(num_numerical_cols):\n",
    "        col_names.append('num' + str(i+1))\n",
    "    for i in range(num_cat_cols):\n",
    "        col_names.append('cat' + str(i+1))\n",
    "    col_names.append('sens_feat')\n",
    "    col_names.append('outcome')\n",
    "    \n",
    "    return col_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flip labels with probability eta\n",
    "def flip_labels(df_synthetic, label_noise):\n",
    "    labels = df_synthetic['outcome']\n",
    "    \n",
    "    for i in range(len(labels)):\n",
    "        if random.uniform(0,1) <= label_noise:\n",
    "            labels[i] = 1 if labels[i] == 0 else 0\n",
    "    df_synthetic['outcome'] = labels\n",
    "    \n",
    "    return df_synthetic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Parameters:\n",
    "\n",
    "    n is the total number of examples in the dataset\n",
    "    \n",
    "    num_numerical_feats is number of numerical features\n",
    "        each numerical feature is drawn from a\n",
    "        multivariate normal distribution with mean 0\n",
    "        \n",
    "    num_cat_feats is number of categorical features\n",
    "    \n",
    "    cat_levels is an array where each element is the number\n",
    "        of levels for each categorical feature\n",
    "        len(cat_levels) = num_cat_feats\n",
    "        \n",
    "    r is the proportion of examples in the minority group\n",
    "        (1-r) is proportion of examples in majority group\n",
    "        \n",
    "    label_noise is in [0,1]\n",
    "    \n",
    "    diff_dist is true if minority and majority have different\n",
    "        underlying sampling distributions\n",
    "        \n",
    "    show_vis displays the distribution of outcomes\n",
    "\n",
    "'''\n",
    "\n",
    "def get_synthetic_data(n, r, num_numerical_feats, num_cat_feats,\n",
    "                       cat_levels = [], label_noise = 0, \n",
    "                       diff_dist = False, show_vis = False):\n",
    "    \n",
    "    assert 0 < r < 1, \"R must be in [0,1]\"\n",
    "    num_min = int(n*r)\n",
    "    num_maj = n - num_min\n",
    "    \n",
    "    cat_probs = list(np.multiply(np.ones(num_cat_feats),0.5))\n",
    "    \n",
    "    # numerical feature params\n",
    "    means = list(np.zeros(num_numerical_feats))\n",
    "    cov_matrix = list(np.identity(num_numerical_feats))\n",
    "    \n",
    "    # generating the features\n",
    "    \n",
    "    num_features_min = np.random.multivariate_normal(means, cov_matrix, num_min)\n",
    "    num_features_maj = np.random.multivariate_normal(means, cov_matrix, num_maj)\n",
    "    num_features = np.concatenate((num_features_min, num_features_maj))\n",
    "        \n",
    "    # binary sensitive attribute, 0: minority, 1: majority\n",
    "    sens_feat = get_sensitive_feat(r=r, n=n) \n",
    "    \n",
    "    assert len(cat_levels) == num_cat_feats, \\\n",
    "    \"Each categorical feature must have a specification for its number of levels\"\n",
    "    cat_feats = get_cat_feats(num_cat_feats, cat_levels, n)\n",
    "    \n",
    "    # generating outcomes (continuous and binary)\n",
    "    if diff_dist:\n",
    "        # causal effect params\n",
    "        effect_param_min = [0.5, -0.2, 0.1] \n",
    "        effect_param_maj = [-0.7, 0.5, 1.5]\n",
    "        outcome_continuous_min = 1/(1+np.exp(-np.matmul(num_features_min,effect_param_min))) # logit model + no added noise\n",
    "        outcome_continuous_maj = 1/(1+np.exp(-np.matmul(num_features_maj,effect_param_maj)))\n",
    "        outcome_binary_min = np.where(outcome_continuous_min >= 0.5, 1, 0) # logistic decision boundary\n",
    "        outcome_binary_maj = np.where(outcome_continuous_maj >= 0.5, 1, 0)\n",
    "        outcome_binary = np.hstack((outcome_binary_min, outcome_binary_maj)).reshape(n,1)\n",
    "        if show_vis:\n",
    "            distribution_plot(outcome_continuous_min, outcome_continuous_maj, diff_dist=True)\n",
    "    else:\n",
    "        effect_param = [0.5, -0.2, 0.1] \n",
    "        outcome_continuous = 1/(1+np.exp(-np.matmul(num_features,effect_param))) # logit model + no added noise\n",
    "        outcome_binary = np.where(outcome_continuous >= 0.5, 1, 0).reshape(n,1) # logistic decision boundary\n",
    "        if show_vis:\n",
    "            distribution_plot(outcome=outcome_continuous, diff_dist=False)\n",
    "\n",
    "    \n",
    "    temp_data = np.hstack((num_features, cat_feats, sens_feat, outcome_binary))\n",
    "    np.random.shuffle(temp_data) # randomly shuffle the data\n",
    "    \n",
    "    df_synthetic = pd.DataFrame(temp_data)\n",
    "    df_synthetic.columns = get_attribute_names(df_synthetic, num_numerical_feats, num_cat_feats)\n",
    "    \n",
    "    assert 0 <= label_noise < 1, \"Label noise must be in [0, 1)\"\n",
    "    if label_noise != 0:\n",
    "        df_synthetic = flip_labels(df_synthetic, label_noise)\n",
    "    \n",
    "    # add to dictionary of datasets\n",
    "    path_synthetic = 'Datasets/synthetic_data.csv'\n",
    "    df_synthetic.to_csv(path_synthetic)\n",
    "    add_dataset(Dataset('synthetic', path_synthetic, cat_cols=[], num_cols=[], synthetic=True, sens_attr = \"sens_feat\"))\n",
    "    \n",
    "    return df_synthetic\n",
    "\n",
    "# example usage\n",
    "df_synthetic = get_synthetic_data(1000, 0.25, 3, 2, [2,3], diff_dist=True, label_noise = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to use any/all of the following EDA functions and/or add your own!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T20:42:07.151553Z",
     "start_time": "2021-06-10T20:42:07.125905Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# take a peek at the first few data points\n",
    "df_synthetic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T20:42:07.426895Z",
     "start_time": "2021-06-10T20:42:07.269718Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_counts(df, attr):\n",
    "    if attr in df.columns:\n",
    "        df[attr].value_counts(normalize=True).plot.barh()\n",
    "    else:\n",
    "        print(\"Error! Please enter a valid feature.\")\n",
    "\n",
    "# example\n",
    "plot_counts(df_synthetic, 'sens_feat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T20:42:08.016753Z",
     "start_time": "2021-06-10T20:42:07.428807Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def group_by_plot(df, attr1, attr2):\n",
    "    for val in list(df[attr1].unique()):\n",
    "        print(val)\n",
    "        temp = df[df[attr1] == val]\n",
    "        sns.displot(temp[attr2])\n",
    "\n",
    "# example\n",
    "group_by_plot(df_synthetic, 'sens_feat', 'outcome')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test Split\n",
    "\n",
    "NOTES: \n",
    "1. Whether you input your own data, choose one of our provided datasets, or generate synthetic data, we will consider this to be the UNBIASED GROUND TRUTH.\n",
    "2. Then, we will split the data, with the first part being the data which we will inject bias into and the second part being the unbiased ground truth testing data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    train_ratio: is the proportion of data examples in the training set\n",
    "        (1-train_ratio is proportion in unbiased testing set)\n",
    "'''\n",
    "def train_test_split(df, train_ratio = 0.5):\n",
    "    \n",
    "    df_train = df.loc[range(0,int(len(df_synthetic)*train_ratio)), :]\n",
    "    df_test = df_synthetic.loc[range(int(len(df_synthetic)*train_ratio)+1, len(df_synthetic)), :]\n",
    "    \n",
    "    return df_train, df_test\n",
    "\n",
    "df_train, df_test = train_test_split(df_synthetic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# format data\n",
    "X_train = df_train.iloc[:, :-1].values\n",
    "y_train = df_train.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "This function separates the minority and majority classes\n",
    "\n",
    "Parameters:\n",
    "    \n",
    "    sens_attr: sensitive attribute\n",
    "    maj_val: value of sens_attr which indicates majority class\n",
    "    min_val: value of sens_attr which indicates minority class\n",
    "\n",
    "'''\n",
    "def get_maj_min(df, sens_attr, maj_val, min_val):\n",
    "    assert sens_attr in list(df.columns), \"Sensitive attribute must be a column in the dataframe!\"\n",
    "    df_majority = df_train[df_train[sens_attr] == maj_val]\n",
    "    df_minority = df_train[df_train[sens_attr] == min_val]\n",
    "    \n",
    "    return df_majority, df_minority\n",
    "\n",
    "df_majority, df_minority = get_maj_min(df_train, 'sens_feat', 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-Hot Encoding (ONLY for non-synthetic data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T20:42:08.750322Z",
     "start_time": "2021-06-10T20:42:08.728346Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# OHE categorical features (prompt for user's choice here?)\n",
    "\n",
    "# get indices of categorical columns\n",
    "def get_cat_cols(dataset):\n",
    "    df = dataset.df\n",
    "    res = []\n",
    "    for col in dataset.cat_cols:\n",
    "        res.append(df.columns.get_loc(col))\n",
    "    return res\n",
    "\n",
    "# example usage\n",
    "#cat_cols = get_cat_cols(datasets['student_por'])\n",
    "#ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), cat_cols)], remainder='passthrough')\n",
    "#X_train = np.array(ct.fit_transform(X_train))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format data\n",
    "X_true = df_test.iloc[:, :-1].values\n",
    "y_true = df_test.iloc[:, -1].values\n",
    "\n",
    "sens_attrs_true = [df_test[datasets['synthetic'].sens_attr]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias Injection (TODO - put different biases into another file?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of Biases\n",
    "1. Under-Sampling (of Minority)\n",
    "2. Over-Sampling (of Majority)\n",
    "3. Omitted Variable\n",
    "4. Label Noise\n",
    "5. Measurement\n",
    "6. Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Under-Sampling Minority Class\n",
    "\n",
    "Note: you will need to input $\\beta$, which is the probability of deleting an example from the minority class. For example, if $\\beta = 0.25$ then each example in the training data will be deleted with probability $0.25$, which will result in approximately $25\\%$ of the total minority class examples being deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "This function performs the under-sampling bias injection\n",
    "\n",
    "'''\n",
    "def under_sample(df_minority, beta):\n",
    "    X_min = df_minority.iloc[:, :].values\n",
    "    cols = df_minority.columns\n",
    "    \n",
    "    # delete each example with probability beta\n",
    "    for i in range(len(X_min)):\n",
    "        if random.uniform(0,1) <= beta:\n",
    "            X_min = np.delete(X_min, 0, axis=0)\n",
    "    \n",
    "    df_minority = pd.DataFrame(pd.DataFrame(X_min))\n",
    "    df_minority.columns = cols\n",
    "    return df_minority\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "Parameters:\n",
    "\n",
    "    beta: probability of deleting example from minority\n",
    "    sens_attr: sensitive attribute\n",
    "    maj_val: value of sens_attr which indicates majority class\n",
    "    min_val: value of sens_attr which indicates minority class\n",
    "\n",
    "'''\n",
    "def under_sampling(df_train, beta, sens_attr,\n",
    "                    maj_val, min_val):\n",
    "    df_majority = df_train[df_train[sens_attr] == maj_val]\n",
    "    df_minority = df_train[df_train[sens_attr] == min_val]\n",
    "    \n",
    "    df_total = df_majority\n",
    "    df_undersampled = under_sample(df_minority, beta)\n",
    "\n",
    "    # combine undersampled and original majority class to create dataset\n",
    "    df_concat = pd.concat([df_total,df_undersampled])\n",
    "    \n",
    "    return df_concat.sample(frac=1) # reshuffle rows of dataframe randomly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Over-Sampling Majority Class\n",
    "\n",
    "Note: you can either choose to randomly over-sample existing examples or generate new samples by interpolation using SMOTE and ADASYN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Parameters:\n",
    "\n",
    "    maj_val: value of sens_attr which indicates majority class\n",
    "    min_val: value of sens_attr which indicates minority class\n",
    "    sens_attr: sensitive attribute\n",
    "    over_amt: amount of over-sampling to be applied to majority\n",
    "        e.g. over_amt = 2 means twice as many samples in majority\n",
    "\n",
    "'''\n",
    "def random_over_sampling(df_train, sens_attr, \n",
    "                         maj_val, min_val, over_amt = 2):\n",
    "    df_majority = df_train[df_train[sens_attr] == maj_val]\n",
    "    df_minority = df_train[df_train[sens_attr] == min_val]\n",
    "    \n",
    "    df_oversampled = df_majority.sample(int(over_amt)*len(df_majority), replace = True)\n",
    "    \n",
    "    # combine oversampled and original majority class to create dataset\n",
    "    df_concat = pd.concat([df_oversampled,df_minority])\n",
    "    \n",
    "    return df_concat.sample(frac=1) # reshuffle rows of dataframe randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import *\n",
    "# to avoid warning\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "'''\n",
    "\n",
    "Parameters:\n",
    "\n",
    "    maj_val: value of sens_attr which indicates majority class\n",
    "    min_val: value of sens_attr which indicates minority class\n",
    "    sens_attr: sensitive attribute\n",
    "    over_amt: amount of over-sampling to be applied to majority\n",
    "        e.g. over_amt = 2 means twice as many samples in majority\n",
    "    type: if 1 then SMOTE, if 2 then ADASYN\n",
    "\n",
    "'''\n",
    "def over_sampling(df_train, sens_attr, \n",
    "          maj_val, min_val, over_amt = 2, type = 1):\n",
    "    \n",
    "    assert type in [1,2], \"Type must be 1 or 2, see comments!\"\n",
    "    \n",
    "    cols = df_train.columns\n",
    "    \n",
    "    df_majority_X = df_train[df_train[sens_attr] == maj_val].drop('outcome', axis = 1)\n",
    "    df_majority_y = df_train[df_train[sens_attr] == maj_val]['outcome']\n",
    "    df_minority_X = df_train[df_train[sens_attr] == min_val].drop('outcome', axis = 1)\n",
    "    df_minority_y = df_train[df_train[sens_attr] == min_val]['outcome']\n",
    "    \n",
    "    over_sample_amt = int(len(df_majority_X) * over_amt)\n",
    "    \n",
    "    # make original minority class into majority with label 0\n",
    "    df_minority_flipped = df_minority_X.sample(over_sample_amt, replace = True)\n",
    "    df_minority_flipped['outcome'] = np.zeros((len(df_minority_flipped),1))\n",
    "    \n",
    "    # make original majority have all label 0 (so it's the minority now)\n",
    "    df_majority_X['outcome'] = np.ones((len(df_majority_X),1))\n",
    "    df_majority = df_majority_X\n",
    "    \n",
    "    df_total = pd.concat([df_minority_flipped, df_majority])\n",
    "    \n",
    "    X_total = df_total.iloc[:, :-1].values\n",
    "    y_total = df_total.iloc[:, -1].values\n",
    "    \n",
    "    if type == 1:\n",
    "        over_sampler = SMOTE(random_state = 42, sampling_strategy = 'minority')\n",
    "    else:\n",
    "        over_sampler = ADASYN(random_state = 42, sampling_strategy = 'minority')\n",
    "    \n",
    "    X_total_resampled, y_total_resampled = over_sampler.fit_resample(X_total, y_total)\n",
    "    \n",
    "    df_res = pd.DataFrame(X_total_resampled)\n",
    "    df_res['outcome'] = y_total_resampled\n",
    "    df_res.columns = cols\n",
    "    \n",
    "    df_oversampled = df_res[df_res[sens_attr] == maj_val]\n",
    "    oversampled_labels = df_majority_y.sample(len(df_oversampled), replace = True).values\n",
    "    \n",
    "    labels = oversampled_labels\n",
    "        \n",
    "    df_oversampled['outcome'] = labels\n",
    "    \n",
    "    # combine oversampled and original majority class to create dataset\n",
    "    df_concat = pd.concat([df_oversampled,df_minority])\n",
    "    \n",
    "    return df_concat.sample(frac=1) # reshuffle rows of dataframe randomly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Omitted Variable\n",
    "\n",
    "Note: if you choose to remove the sensitive feature, you will no longer be able to impose a fairness intervention! Resulting comparisons will simply be between regular ml models trained with and without the sensitive attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# must input a Dataset object\n",
    "def omitted_variable(df, short_name, col_to_del, is_sens_attr = False):\n",
    "    assert col_to_del in list(df.columns), \"Column to delete must be a column in the dataframe!\"\n",
    "    assert short_name in datasets.keys(), \"Dataset with that short name doesn't exist!\"\n",
    "    \n",
    "    if is_sens_attr:\n",
    "        datasets[short_name].has_sens_attr = False\n",
    "        \n",
    "    return df.drop(col_to_del, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label Noise Bias\n",
    "\n",
    "add noise to labels for a specific subset of the data (conditioned on another feature or subgroup of another feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "feature: must be a column in the dataframe\n",
    "feature_type: numeric or categorical\n",
    "subgroup_val: subgroup value for feature\n",
    "    if subgroup_type == numerical:\n",
    "        subgroup is a range of values (x, y)\n",
    "    if subgroup_type == categorical:\n",
    "        subgroup is a single numerical value\n",
    "            (refer to OHE if needed)\n",
    "\n",
    "label_noise: flip labels with probability label_noise\n",
    "\n",
    "'''\n",
    "def label_noise(df, feature, feature_type, subgroup_val, label_noise):\n",
    "    \n",
    "    assert feature in list(df.columns), \"feature must be a column in the dataframe!\"\n",
    "    \n",
    "    err_msg = \"Error! feature_type must be either numeric or categorical\"\n",
    "    \n",
    "    df_bias = df.copy()\n",
    "    \n",
    "    if feature_type == 'numeric':\n",
    "        lower, upper = subgroup_val\n",
    "        df_bias = df_bias[(df_bias[feature] >= lower) &\n",
    "                          (df_bias[feature] <= upper)]\n",
    "    \n",
    "    else:\n",
    "        assert feature_type == 'categorical', err_msg\n",
    "        df_bias = df_bias[df_bias[feature] == subgroup_val]\n",
    "        \n",
    "    labels = list(df_bias['outcome'])\n",
    "    \n",
    "    for i in range(len(labels)):\n",
    "        if random.uniform(0,1) <= label_noise:\n",
    "            labels[i] = 1 if labels[i] == 0 else 0\n",
    "    df_bias['outcome'] = labels\n",
    "    \n",
    "    if feature_type == 'numeric':\n",
    "        lower, upper = subgroup_val\n",
    "        df[(df[feature] >= lower) & (df[feature] <= upper)] = df_bias\n",
    "    else:\n",
    "        df[df[feature] == subgroup_val] = df_bias\n",
    "    \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Measurement Bias\n",
    "\n",
    "Add noise to an attribute, either entirely or on various subgroups\n",
    "\n",
    "Refer to [numpy.random documentation](https://numpy.org/doc/1.16/reference/routines.random.html) for types of sampling distributions for the noise.\n",
    "\n",
    "Helper Functions:\n",
    "1. Get unique values for categorical features\n",
    "2. Get 5 number summary for numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precondition: feature MUST be categorical\n",
    "def get_unique_cat(df, feature):\n",
    "    arr = list(np.sort(df[feature].unique()))\n",
    "    return arr\n",
    "\n",
    "# Precondition: feature MUST be numerical\n",
    "def get_summary_num(df, feature):\n",
    "    res = dict()\n",
    "    res['attribute'] = feature\n",
    "    \n",
    "    data_min, data_max = df[feature].min(), df[feature].max()\n",
    "    res['min'] = data_min\n",
    "    res['max'] = data_max\n",
    "    \n",
    "    quartiles = percentile(df[feature], [25,50,75])\n",
    "    res['1st Quartile'] = quartiles[0]\n",
    "    res['2nd Quartile'] = quartiles[1]\n",
    "    res['3rd Quartile'] = quartiles[2]\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_noise(df, feature, noise_dist, noise_dist_params):\n",
    "    num_params = len(noise_dist_params)\n",
    "    \n",
    "    if num_params == 1:\n",
    "        noise = noise_dist(noise_dist_params[0])\n",
    "    elif num_params == 2:\n",
    "        noise = noise_dist(noise_dist_params[0], noise_dist_params[1])\n",
    "    elif num_params == 3:\n",
    "        noise = noise_dist(noise_dist_params[0], noise_dist_params[1],\n",
    "                           noise_dist_params[2])\n",
    "    else:\n",
    "        raise ValueError(\"check your parameters for the noise distribution!\")\n",
    "        \n",
    "    return noise\n",
    "\n",
    "# inject noise with probability noise_prob\n",
    "def inject_noise_cat(df_noise, feature, noise_prob):\n",
    "    # get unique values for feature (from training dataset)\n",
    "    unique_vals = get_unique_cat(df_train, feature)\n",
    "    for i in range(len(df_noise[feature])):\n",
    "        # noise injection criteria\n",
    "        if random.uniform(0,1) <= noise_prob:\n",
    "            # perturb specific noise value\n",
    "            df_noise[feature].iloc[i] = np.random.choice(unique_vals)\n",
    "    \n",
    "    return df_noise\n",
    "\n",
    "# inject noise with probability noise_prob\n",
    "def inject_noise_num(df_noise, feature, noise_prob,\n",
    "                     noise_dist, noise_dist_params):\n",
    "    for i in range(len(df_noise[feature])):\n",
    "        # noise injection criteria\n",
    "        if random.uniform(0,1) <= noise_prob:\n",
    "            # perturb specific noise value\n",
    "            df_noise[feature].iloc[i] += get_noise(df_noise, feature,\n",
    "                                      noise_dist, noise_dist_params)\n",
    "    \n",
    "    return df_noise\n",
    "\n",
    "def total_measurement(df, feature, feature_type, noise_prob,\n",
    "                      noise_dist, noise_dist_params, subgroups):\n",
    "    err_msg = \"Error! feature_type must be either numeric or categorical\"\n",
    "    noise = get_noise(df, feature, noise_dist, noise_dist_params)\n",
    "    \n",
    "    if feature_type == 'categorical':\n",
    "        return inject_noise_cat(df, feature, noise_prob)\n",
    "    else:\n",
    "        assert feature_type == 'numeric', err_msg\n",
    "        return inject_noise_num(df, feature, noise_prob, \n",
    "                                noise_dist, noise_dist_params)\n",
    "\n",
    "# NOTE: we assume that you have provided the proper parameters for each noise distribution call\n",
    "#       for each subgroup (if applicable)\n",
    "def subgroup_measurement(df, feature, feature_type, noise_prob,\n",
    "                         noise_dist, noise_dist_params, subgroups):\n",
    "    err_msg = \"Must have as many noise_dist_param lists as number of categories for feature\"\n",
    "    err_msg_2 = \"Error! feature_type must be either numeric or categorical\"\n",
    "    \n",
    "    if feature_type == 'numeric':\n",
    "        for i in range(len(subgroups)):\n",
    "            lower, upper = subgroups[i]\n",
    "            # isolate subgroup\n",
    "            df_noise = df[(df[feature] >= lower) & (df[feature] <= upper)]\n",
    "            # apply noise to subgroup\n",
    "            df_noise = inject_noise_num(df_noise, feature, noise_prob,\n",
    "                                        noise_dist, noise_dist_params[i])\n",
    "            # modify subgroup in original dataframe\n",
    "            df[(df[feature] >= lower) & (df[feature] <= upper)] = df_noise\n",
    "    else:\n",
    "        # maybe randomly select from get_unique_cat for the specific groups?\n",
    "        assert feature_type == 'categorical', err_msg_2\n",
    "        for i in range(len(subgroups)):\n",
    "            # isolate subgroup\n",
    "            df_noise = df[df[feature] == subgroups[i]]\n",
    "            # apply noise to subgroup\n",
    "            df_noise = inject_noise_cat(df_noise, feature, noise_prob)\n",
    "            # modify subgroup in original dataframe\n",
    "            df[df[feature] == subgroups[i]] = df_noise\n",
    "\n",
    "    return df\n",
    "\n",
    "'''\n",
    "\n",
    "feature: must be a column in the dataframe\n",
    "feature_type: numeric or categorical\n",
    "noise_dist: distribution function from np.random library\n",
    "    e.g. np.random.beta, np.random.logistic, etc.\n",
    "    \n",
    "noise_dist_params: parameters for noise distribution\n",
    "    e.g. if noise_dist == np.random.normal\n",
    "        then noist_dist_params could be [0, 0.1], i.e. noise w/ mean 0 and sigma 0.1\n",
    "    NOTE 1: make sure number of parameters is consistent with function documentation\n",
    "            EXCLUDING the final param (length of column)\n",
    "    NOTE 2: if using subgroups, noise_dist_params[i] should be a list of the parameters \n",
    "            for the noise distribution of subgroup[i]\n",
    "        \n",
    "noise_type: 0 if noise applied to entire attribute,\n",
    "            1 if noise applied to subgroups\n",
    "            \n",
    "noise_prob: probability of applying noise to an example\n",
    "\n",
    "subgroups: subgroups for feature\n",
    "    if feature_type == numerical:\n",
    "        subgroups[i] is a range of values (x, y) for subgroup i\n",
    "    if feature_type == categorical:\n",
    "        subgroups[i] is a single numerical value for subgroup i\n",
    "            (refer to OHE if needed)\n",
    "\n",
    "'''\n",
    "def measurement(df, feature, feature_type, noise_dist = np.random.normal, \n",
    "                noise_prob = 0.25, noise_dist_params = [0, 1],\n",
    "                noise_type = 0, subgroups = []):\n",
    "    \n",
    "    assert noise_type in [0,1], \"noise_type must be 0 or 1, see comments!\"\n",
    "    assert feature in list(df.columns), \"feature must be a column in the dataframe!\"\n",
    "    \n",
    "    df_bias = df.copy()\n",
    "    \n",
    "    if noise_type == 0:\n",
    "        df_bias = total_measurement(df_bias, feature, feature_type, noise_prob,\n",
    "                                             noise_dist, noise_dist_params, subgroups)\n",
    "    else:\n",
    "        df_bias = subgroup_measurement(df_bias, feature, feature_type, noise_prob,\n",
    "                                       noise_dist, noise_dist_params, subgroups)\n",
    "        \n",
    "        \n",
    "    return df_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example: adds different measurement bias to, e.g. num1 < 0 and num1 > 0\n",
    "df_example = measurement(df=df_train, feature='num1', feature_type='numeric', \n",
    "                         noise_dist=np.random.normal, noise_prob=1,\n",
    "                         noise_dist_params=[(20, 1), (50, 1)], noise_type=1, \n",
    "                         subgroups=[(-1, 0), (0, 1)])\n",
    "\n",
    "# example: adds measurement bias to examples with cat2 == 2\n",
    "#print(df_train['cat2'].value_counts())\n",
    "df_example = measurement(df_train, 'cat2', 'categorical', noise_prob=1, noise_type=1, subgroups=[2])\n",
    "#df_example['cat2'].value_counts()\n",
    "\n",
    "# example: adds measurement bias to num1\n",
    "df_example = measurement(df_train, 'num1', 'numeric', np.random.normal, noise_prob=1,\n",
    "                         noise_dist_params=[40, 1], noise_type=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Representation Bias\n",
    "\n",
    "Under-sample an attribute (conditioned on subgroups for that feature and/or subgroups for another feature)\n",
    "\n",
    "Note: you will need to input $\\beta$, which is the probability of deleting an example from the desired group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def representation(df, feat_conditions, beta):\n",
    "    '''\n",
    "    randomly deletes samples which meet feat_conditions with probability beta\n",
    "    \n",
    "    inputs:\n",
    "        df: original dataframe\n",
    "        feat_conditions: conditions of which data to undersample,\n",
    "            input clauses as (df['column_name'] boolean) combined with &,\n",
    "                e.g. (df['num1'] > 0) & (df['num1'] <= 1) & (df['cat1'] == 0)\n",
    "        beta: probability of deleting any sample that meets feat_conditions\n",
    "        \n",
    "    outputs:\n",
    "        df_bias: dataframe df with undersampling\n",
    "    '''\n",
    "    \n",
    "    df_bias = df.copy()\n",
    "    drop_idx = []\n",
    "\n",
    "    for i in df_bias.index[df_bias['num1']>0]:\n",
    "        if random.uniform(0,1) <= beta: drop_idx.append(i)\n",
    "        \n",
    "    return df_bias.drop(drop_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biases = dict()\n",
    "\n",
    "def add_bias(bias_func, short_name):\n",
    "    biases[short_name] = bias_func\n",
    "\n",
    "# example usage\n",
    "add_bias(under_sampling, 'under_sampling')\n",
    "add_bias(omitted_variable, 'omitted_variable')\n",
    "add_bias(random_over_sampling, 'random_over_sampling')\n",
    "add_bias(over_sampling, 'over_sampling')\n",
    "add_bias(label_noise, 'label_noise')\n",
    "add_bias(measurement, 'measurement')\n",
    "add_bias(representation, 'representation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias Injection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_bias = biases['under_sampling'](df_train, 0.5, 'sens_feat', 1, 0, True)\n",
    "#df_bias = biases['omitted_variable'](df_train, 'synthetic', 'num1', is_sens_attr=False)\n",
    "#df_bias = biases['random_over_sampling'](df_train, 'sens_feat', 1, 0, 2)\n",
    "#df_bias = biases['over_sampling'](df_train, 'sens_feat', 1, 0, 2, type=2)\n",
    "#df_bias = biases['label_noise'](df_train, 'sens_feat', 'categorical', 1, 0.2)\n",
    "df_bias = biases['representation'](df_train, (df_train['num1'] > 0) & (df_train['num1'] <= 1) & (df_train['cat1'] == 0), beta=0.5)\n",
    "\n",
    "# for fairness measures later\n",
    "if datasets['synthetic'].has_sens_attr:\n",
    "    df_sens = df_bias[datasets['synthetic'].sens_attr]\n",
    "\n",
    "# format data\n",
    "X_bias = df_bias.iloc[:, :-1].values\n",
    "y_bias = df_bias.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Selection + Training (TODO: modularize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T20:42:09.057618Z",
     "start_time": "2021-06-10T20:42:08.942574Z"
    }
   },
   "outputs": [],
   "source": [
    "# modularize and add data struct of different ml techniques\n",
    "\n",
    "classifier = DecisionTreeClassifier(min_samples_leaf = 10, max_depth = 4)\n",
    "\n",
    "classifier_true = classifier.fit(X_true, y_true)\n",
    "y_pred_truth = classifier_true.predict(X_true)\n",
    "\n",
    "classifier_bias = classifier.fit(X_bias, y_bias)\n",
    "y_pred_bias = classifier_bias.predict(X_bias)\n",
    "y_pred_bias_on_true = classifier_bias.predict(X_true)\n",
    "\n",
    "sens_feat_true = df_test['sens_feat']\n",
    "sens_feat_bias = df_sens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Performance (TODO: modularize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T20:42:09.231352Z",
     "start_time": "2021-06-10T20:42:09.225108Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Accuracy of Ground Truth Model on Ground Truth Data: \", accuracy_score(y_pred_truth, y_true))\n",
    "print(\"Accuracy of Biased Model on Biased Data: \", accuracy_score(y_pred_bias, y_bias))\n",
    "print(\"Accuracy of Biased Model on Ground Truth Data: \", accuracy_score(y_pred_bias_on_true, y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T21:16:04.515563Z",
     "start_time": "2021-06-10T21:16:04.499948Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ground Truth Model on Ground Truth Data\n",
    "\n",
    "gm_true = MetricFrame(metrics=accuracy_score,y_true=y_true, y_pred=y_pred_truth, sensitive_features = sens_feat_true)\n",
    "print(\"Overall Accuracy: \", gm_true.overall)\n",
    "print(\"Group Accuracy : \", gm_true.by_group)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "sr_true = MetricFrame(metrics=selection_rate, y_true=y_true, y_pred=y_pred_truth, sensitive_features = sens_feat_true)\n",
    "print(\"Overall Selection Rate: \", sr_true.overall)\n",
    "print(\"Group Selection Rate : \", sr_true.by_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fairness Intervention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "\n",
    "from fairlearn.preprocessing import CorrelationRemover\n",
    "\n",
    "remover = CorrelationRemover(sensitive_feature_ids=[5])\n",
    "remover_df = CorrelationRemover(sensitive_feature_ids=['sens_feat'])\n",
    "\n",
    "X_train_corr = remover.fit_transform(X_train)\n",
    "df_corr = pd.DataFrame(remover_df.fit_transform(df_train))\n",
    "\n",
    "df_temp = df_train.drop('sens_feat', axis=1)\n",
    "df_corr.columns = df_temp.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constraints = [EqualizedOdds(), DemographicParity(), ErrorRateParity(),\n",
    "               FalsePositiveRateParity(), TruePositiveRateParity()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exponentiated Gradient\n",
    "constraint = DemographicParity()\n",
    "mitigator_bias = ExponentiatedGradient(classifier_bias, constraint)\n",
    "mitigator_bias.fit(X_bias, y_bias, sensitive_features = sens_feat_bias)\n",
    "y_pred_mitigated_bias_on_true = mitigator_bias.predict(X_true)\n",
    "\n",
    "print(\"Accuracy of Biased Model + Fairness Intervention on Ground Truth Data: \",\n",
    "      accuracy_score(y_pred_mitigated_bias_on_true, y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Search\n",
    "constraint = DemographicParity()\n",
    "mitigator_grid_bias = GridSearch(classifier_bias, constraint)\n",
    "mitigator_grid_bias.fit(X_bias, y_bias, sensitive_features = sens_feat_bias)\n",
    "y_pred_mitigated_grid_bias_on_true = mitigator_grid_bias.predict(X_true)\n",
    "\n",
    "print(\"Accuracy of Biased Model + Fairness Intervention on Ground Truth Data: \",\n",
    "      accuracy_score(y_pred_mitigated_grid_bias_on_true, y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# postprocessing\n",
    "\n",
    "from fairlearn.postprocessing import ThresholdOptimizer\n",
    "\n",
    "thresh_optim = ThresholdOptimizer(estimator=classifier_bias, constraints= 'equalized_odds', predict_method='auto')\n",
    "thresh_optim.fit(X_bias, y_bias, sensitive_features=sens_feat_bias)\n",
    "y_pred_thresh_on_true = thresh_optim.predict(X_true, sensitive_features=sens_feat_true)\n",
    "\n",
    "print(\"Accuracy of Biased Model + Fairness Intervention on Ground Truth Data: \",\n",
    "      accuracy_score(y_pred_thresh_on_true, y_true))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy of Ground Truth Model + Fairness Intervention on Ground Truth Data: \",\n",
    "      accuracy_score(y_pred_mitigated_true, y_true))\n",
    "\n",
    "print(\"Accuracy of Biased Model + Fairness Intervention on Ground Truth Data: \",\n",
    "      accuracy_score(y_pred_mitigated_bias_on_true, y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-09T17:51:42.986098Z",
     "start_time": "2021-06-09T17:51:42.973042Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ground Truth Model + Fairness Intervention on Ground Truth Data\n",
    "\n",
    "gm_mitigated = MetricFrame(metrics=accuracy_score, y_true=y_true, y_pred=y_pred_mitigated_true, sensitive_features = sens_feat_true)\n",
    "print(\"Overall Accuracy: \", gm_mitigated.overall)\n",
    "print(\"Group Accuracy : \", gm_mitigated.by_group)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "sr_mitigated = MetricFrame(metrics=selection_rate, y_true=y_true, y_pred=y_pred_mitigated_true, sensitive_features = sens_feat_true)\n",
    "print(\"Overall Selection Rate: \", sr_mitigated.overall)\n",
    "print(\"Group Selection Rate : \", sr_mitigated.by_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Biased Model + Fairness Intervention on Ground Truth Data\n",
    "\n",
    "gm_mitigated_bias_on_true = MetricFrame(metrics=accuracy_score, y_true=y_true, y_pred=y_pred_mitigated_bias_on_true, sensitive_features = sens_feat_true)\n",
    "print(\"Overall Accuracy: \", gm_mitigated_bias_on_true.overall)\n",
    "print(\"Group Accuracy : \", gm_mitigated_bias_on_true.by_group)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "sr_mitigated_bias_on_true = MetricFrame(metrics=selection_rate, y_true=y_true, y_pred=y_pred_mitigated_bias_on_true, sensitive_features = sens_feat_true)\n",
    "print(\"Overall Selection Rate: \", sr_mitigated_bias_on_true.overall)\n",
    "print(\"Group Selection Rate : \", sr_mitigated_bias_on_true.by_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trade-Off Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if verbose, shows \"Finished iteration: ... \"\n",
    "# if apply_fairness, uses fairness intervention\n",
    "def tradeoff_visualization(bias_amts, classifier, X_true, y_true, \n",
    "                           df_train, sensitive_feature = \"cat\",\n",
    "                           is_synthetic = False,\n",
    "                           apply_fairness = False, verbose = False):\n",
    "    \n",
    "    accuracy_on_true = []\n",
    "    accuracy_on_biased = []\n",
    "    accuracy_on_true_mitigated = []\n",
    "    accuracy_on_biased_mitigated = []\n",
    "    \n",
    "    count = 0\n",
    "\n",
    "    classifier_true = classifier.fit(X_true, y_true)\n",
    "    y_pred_truth = classifier_true.predict(X_true)\n",
    "\n",
    "    for bias in bias_amts:\n",
    "        \n",
    "        df_bias = biases['over_sampling'](df_train, 'sens_feat', 1, 0, 2, type=2)\n",
    "        df_sens = df_bias[sensitive_feature]\n",
    "\n",
    "        # format data\n",
    "        X_bias = df_bias.iloc[:, :-1].values\n",
    "        y_bias = df_bias.iloc[:, -1].values\n",
    "        \n",
    "        if not is_synthetic:\n",
    "            # OHE\n",
    "            ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), cat_cols)], remainder='passthrough')\n",
    "            X_bias_true = np.array(ct.fit_transform(X_bias))\n",
    "        else:\n",
    "            X_bias_true = X_bias\n",
    "        \n",
    "        y_bias_true = df_bias.iloc[:, -1].values\n",
    "        \n",
    "        classifier_bias = classifier.fit(X_bias_true, y_bias_true)\n",
    "        \n",
    "        if apply_fairness:\n",
    "            constraint = DemographicParity()\n",
    "            classifier_mitigated_bias = ExponentiatedGradient(classifier_bias, constraint)\n",
    "            classifier_mitigated_bias.fit(X_bias_true, y_bias_true, sensitive_features = df_sens)\n",
    "            \n",
    "            # testing on biased data WITH fairness intervention\n",
    "            y_pred_mitigated_bias = classifier_mitigated_bias.predict(X_bias_true)\n",
    "            \n",
    "            # testing on GT data WITH fairness intervention\n",
    "            y_pred_mitigated_bias_on_true = classifier_mitigated_bias.predict(X_true)\n",
    "        \n",
    "        # testing on biased data withOUT fairness intervention\n",
    "        y_pred_bias = classifier_bias.predict(X_bias_true)\n",
    "        \n",
    "        # testing on GT data withOUT fairness intervention\n",
    "        y_pred_bias_on_true = classifier_bias.predict(X_true)\n",
    "\n",
    "        # model performance\n",
    "        \n",
    "        if apply_fairness:\n",
    "            # on biased data\n",
    "            acc_bias_mitigated = accuracy_score(y_pred=y_pred_mitigated_bias, y_true=y_bias_true)\n",
    "            accuracy_on_biased_mitigated.append(acc_bias_mitigated)\n",
    "            # on GT data\n",
    "            acc_bias_mitigated_on_true = accuracy_score(y_pred=y_pred_mitigated_bias_on_true, y_true=y_true)\n",
    "            accuracy_on_true_mitigated.append(acc_bias_mitigated_on_true)\n",
    "        \n",
    "        # on biased data\n",
    "        acc_bias = accuracy_score(y_pred=y_pred_bias, y_true=y_bias_true)\n",
    "        accuracy_on_biased.append(acc_bias)\n",
    "        # on GT data\n",
    "        acc_bias_on_true = accuracy_score(y_pred=y_pred_bias_on_true, y_true=y_true)\n",
    "        accuracy_on_true.append(acc_bias_on_true)\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"Finished Iteration: \", count)\n",
    "            count +=1\n",
    "\n",
    "    return bias_amts, accuracy_on_biased, accuracy_on_true, \\\n",
    "           accuracy_on_biased_mitigated, accuracy_on_true_mitigated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_visualizations(bias_amts,\n",
    "                            accuracy_on_biased = [], accuracy_on_true = [],\n",
    "                            accuracy_on_biased_mitigated = [],\n",
    "                            accuracy_on_true_mitigated = [], fairness = False):\n",
    "    \n",
    "    if fairness:\n",
    "        plt.figure(figsize=(17,7))\n",
    "\n",
    "        plt.plot(bias_amts, accuracy_on_true_mitigated, label = 'Ground Truth')\n",
    "        plt.plot(bias_amts, accuracy_on_biased_mitigated, label = 'Biased Data')\n",
    "        plt.xlabel(\"Amount of Bias (number of minority samples removed)\")\n",
    "        plt.ylabel(\"Accuracy Score\")\n",
    "        plt.axhline(y=accuracy_score(y_pred_truth, y_true), color = \"green\", label = \"Ground Truth Model Accuracy\", alpha = 0.5)\n",
    "        plt.title(\"Biased Model Accuracy\")\n",
    "        plt.ylim(0.92, 0.99)\n",
    "        plt.legend()\n",
    "\n",
    "        plt.show()\n",
    "        \n",
    "    else:\n",
    "        plt.figure(figsize=(17,7))\n",
    "\n",
    "        plt.plot(bias_amts, accuracy_on_true, label = 'Ground Truth')\n",
    "        plt.plot(bias_amts, accuracy_on_biased, label = 'Biased Data')\n",
    "        plt.xlabel(\"Amount of Bias (number of minority samples removed)\")\n",
    "        plt.ylabel(\"Accuracy Score\")\n",
    "        plt.axhline(y=accuracy_score(y_pred_truth, y_true), color = \"green\", label = \"Ground Truth Model Accuracy\", alpha = 0.5)\n",
    "        plt.title(\"Biased Model Accuracy\")\n",
    "        plt.ylim(0.92, 0.99)\n",
    "        plt.legend()\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_visualizations(bias_amts, accuracy_on_biased, accuracy_on_true,\n",
    "                        accuracy_on_biased_mitigated, accuracy_on_true_mitigated):\n",
    "    plt.figure(figsize=(17,7))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(bias_amts, accuracy_on_biased, label = 'Tested On Biased Data + No Fairness Intervention', color = \"red\")\n",
    "    plt.plot(bias_amts, accuracy_on_biased_mitigated, label = 'Tested On Biased Data + Fairness Intervention', color = \"green\")\n",
    "    plt.plot(bias_amts, accuracy_on_true, label = 'Tested On Ground Truth + No Fairness Intervention', color = \"blue\")\n",
    "    plt.plot(bias_amts, accuracy_on_true_mitigated, label = 'Tested On Ground Truth + Fairness Intervention', color = \"purple\")\n",
    "    plt.xlabel(\"Amount of Bias (number of minority samples removed)\")\n",
    "    plt.ylabel(\"Accuracy Score\")\n",
    "    #plt.axhline(y=accuracy_score(y_pred_truth, y_true), color = \"green\", label = \"Ground Truth Model On Ground Truth Data\", alpha = 0.5)\n",
    "    plt.title(\"Accuracy of Biased Model (trained on biased data)\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = LogisticRegression()\n",
    "\n",
    "bias_amts = np.divide(list(range(10,-1,-1)),10)\n",
    "\n",
    "bias_amts, accuracy_on_biased, accuracy_on_true, \\\n",
    "           accuracy_on_biased_mitigated, accuracy_on_true_mitigated = \\\n",
    "tradeoff_visualization(bias_amts, classifier, X_true, y_true,\n",
    "                       df_train, \"sens_feat\", is_synthetic=True,\n",
    "                       apply_fairness=True, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_visualizations(bias_amts, accuracy_on_biased, accuracy_on_true,\n",
    "                     accuracy_on_biased_mitigated, accuracy_on_true_mitigated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
