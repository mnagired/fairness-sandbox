{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-08T20:41:26.879421Z",
     "start_time": "2021-06-08T20:41:26.876140Z"
    }
   },
   "source": [
    "# Fairness Sandox Prototype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This rudimentary version of the framework will contain one dataset, one model, one bias mitigation algorithm, and a limited amount of validation metrics. Emphasis will be on modularity and abstraction so as to encourage more features in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "Please run the code block below to install the necessary packages (if needed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pip install aif360\n",
    "%pip install fairlearn\n",
    "%pip install imbalanced-learn\n",
    "%pip install matplotlib\n",
    "%pip install numpy\n",
    "%pip install pandas\n",
    "%pip install seaborn\n",
    "%pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T20:42:02.586362Z",
     "start_time": "2021-06-10T20:42:02.581026Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn as sk\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, roc_curve, auc\n",
    "from collections import Counter\n",
    "\n",
    "import fairlearn\n",
    "from fairlearn.metrics import *\n",
    "import aif360\n",
    "\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data + EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T20:42:06.040700Z",
     "start_time": "2021-06-10T20:42:06.035368Z"
    }
   },
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, short_name = '', path = '', cat_cols = [], num_cols = []):\n",
    "        self.short_name = short_name\n",
    "        self.path = path\n",
    "        self.cat_cols = cat_cols\n",
    "        self.num_cols = num_cols\n",
    "        self.df = pd.read_csv(path, sep = ';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Nice Gaussian Synthetic Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create synthetic data as such:\n",
    "# 3 numerical features (Gaussian), 1 categorical (sensitive attribute), \n",
    "# linear outcome model, s.t. outcome = Ind[effect_param*features > threshold]\n",
    "\n",
    "# parameters\n",
    "threshold = 0 # binary label threshold (outcome > threshold => label=1, else 0)\n",
    "effect_param = [0.5, -0.2, 0.1] # causal effect parameter (to create outcomes)\n",
    "n = 2000 # sample size\n",
    "prop_noise = 0.01 # proportion of noisy binary labels (set to 0 for totally linear)\n",
    "\n",
    "# categorical feature params\n",
    "# required: len(cat_probabilities) = n_cat_features\n",
    "n_cat_features = 2\n",
    "cat_probabilities = [0.5, 0.5] \n",
    "\n",
    "# numerical feature params\n",
    "num_feature_mean = [0, 0, 0]\n",
    "num_feature_cov = [[1, 0, 0], [0, 1, 0], [0, 0, 1]]\n",
    "\n",
    "# features\n",
    "num_features = np.random.multivariate_normal(num_feature_mean, num_feature_cov, n)\n",
    "cat_features = np.random.choice(np.arange(n_cat_features), n, cat_probabilities).reshape(n,1)\n",
    "\n",
    "# outcomes\n",
    "outcome_continuous = np.matmul(features,effect_param) # linear + no added noise\n",
    "outcome_binary = np.where(outcome_continuous > threshold, 1, 0).reshape(n,1)\n",
    "\n",
    "# binary label noise (flip prop_noise labels)\n",
    "flip_idx = np.random.randint(0,n,int(n*prop_noise))\n",
    "outcome_binary[flip_idx] = (outcome_binary[flip_idx]-1)*-1\n",
    "\n",
    "# joint = [numerical features, categorical features, & binary outcome]\n",
    "joint = np.hstack((features, cat_features, outcome_binary))\n",
    "\n",
    "# print pct of positive labels (to check threshold)\n",
    "print(round(np.mean(outcome_binary)*100,2), \"% positively labeled\")\n",
    "\n",
    "# plot distribution of continuous outcomes & binary threshold \n",
    "plt.hist(np.matmul(features,effect_param),label='continuous outcome',bins='auto')\n",
    "plt.axvline(threshold,color='red',label='threshold')\n",
    "plt.xlabel(\"continuous outcome (linear combination of numerical features)\")\n",
    "plt.ylabel(\"number of samples\")\n",
    "plt.title(\"distribution of outcomes & binary label threshold\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Popular Datasets"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T20:42:06.330948Z",
     "start_time": "2021-06-10T20:42:06.325325Z"
    }
   },
   "outputs": [],
   "source": [
    "# each dataset is a dictionary where keys = short name, values = Dataset object\n",
    "\n",
    "datasets = dict()\n",
    "\n",
    "def add_dataset(dataset):\n",
    "    if not isinstance(dataset, Dataset):\n",
    "        print(\"Error! Please enter a valid Dataset object\")\n",
    "    else:\n",
    "        if dataset.short_name not in datasets.keys():\n",
    "            datasets[dataset.short_name] = dataset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T20:42:06.534123Z",
     "start_time": "2021-06-10T20:42:06.464059Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# example - adding a dataset\n",
    "path_adult_income = 'Datasets/adult.csv'\n",
    "cat_cols = ['workclass', 'education','marital-status', 'occupation', 'relationship', 'race',\n",
    "            'gender', 'native-country','income']\n",
    "num_cols = ['age', 'fnlwgt', 'educational-num', 'capital-gain', 'capital-loss', 'hours-per-week']\n",
    "adult_income = Dataset('adult_income', path_adult_income, cat_cols, num_cols)\n",
    "\n",
    "add_dataset(adult_income)\n",
    "\n",
    "# TODO - add more datasets"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T20:42:06.621748Z",
     "start_time": "2021-06-10T20:42:06.598777Z"
    }
   },
   "outputs": [],
   "source": [
    "cat = ['school', 'sex', 'address','famsize','Pstatus','Mjob','Fjob','reason',\n",
    "       'guardian','schoolsup','famsup','paid', 'activities','nursery','higher', 'internet','romantic']\n",
    "num = ['age', 'Medu', 'Fedu','traveltime','studytime','failures', 'famrel',\n",
    "       'freetime','goout','Dalc','Walc','health','absences','G1', 'G2', 'G3']\n",
    "\n",
    "#add_dataset(Dataset(\"student_mat\", path='Datasets/student-mat.csv', cat_cols=cat, num_cols=num))\n",
    "add_dataset(Dataset(\"student_por\", path='Datasets/student-por.csv', cat_cols=cat, num_cols=num))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## add synthetic data to dataset list\n",
    "\n",
    "# save to file\n",
    "df_gaussian_synthetic = pd.DataFrame(pd.DataFrame(joint))\n",
    "df_gaussian_synthetic.columns = ['num1','num2','num3','cat','outcome']\n",
    "path_gaussian_synthetic = 'Datasets/gaussian_synthetic.csv'\n",
    "df_gaussian_synthetic.to_csv(path_gaussian_synthetic)\n",
    "\n",
    "# define Dataset object\n",
    "cat = df_gaussian_synthetic.columns.values[-1]\n",
    "num = df_gaussian_synthetic.columns.values[:-2]\n",
    "gaussian_synthetic = Dataset('gaussian_synthetic', path_gaussian_synthetic, cat_cols=cat, num_cols=num)\n",
    "\n",
    "# add to dataset list\n",
    "add_dataset(gaussian_synthetic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to use any/all of the following EDA functions and/or add your own!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T20:42:07.151553Z",
     "start_time": "2021-06-10T20:42:07.125905Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# take a peek at the first few data points\n",
    "df_por = datasets['student_por'].df\n",
    "# df_por.head()"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T20:42:07.426895Z",
     "start_time": "2021-06-10T20:42:07.269718Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_counts(df, attr):\n",
    "    if attr in df.columns:\n",
    "        df[attr].value_counts(normalize=True).plot.barh()\n",
    "    else:\n",
    "        print(\"Error! Please enter a valid feature.\")\n",
    "\n",
    "# example\n",
    "# plot_counts(df_por, 'sex')"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T20:42:08.016753Z",
     "start_time": "2021-06-10T20:42:07.428807Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def group_by_plot(df, attr1, attr2):\n",
    "    for val in list(df[attr1].unique()):\n",
    "        print(val)\n",
    "        temp = df[df[attr1] == val]\n",
    "        sns.displot(temp[attr2])\n",
    "\n",
    "# example\n",
    "# group_by_plot(df_por, 'sex', 'G3')"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T20:42:08.021705Z",
     "start_time": "2021-06-10T20:42:08.019363Z"
    }
   },
   "outputs": [],
   "source": [
    "# add support for sns.pairplot, plt.scatter, sns.heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Formulation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T20:42:08.045526Z",
     "start_time": "2021-06-10T20:42:08.023712Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def threshold(df, g_1=0.3, g_2=0.3, g_3=0.4, threshold=11):\n",
    "    \"\"\"\n",
    "    Added \"pass/fail\" to make problem binary classification\n",
    "    \"\"\"\n",
    "    assert g_1 + g_2 + g_3 == 1, \"The sum of the percentages should be 1\"\n",
    "    assert 0 < threshold < 20, \"Threshold needs to be between 0 and 20\"\n",
    "    df['pass'] = df.apply(lambda row: 1\n",
    "                                 if g_1*row['G1'] + g_2*row['G2'] + g_3*row['G3'] >= threshold\n",
    "                                 else 0, axis=1)\n",
    "threshold(df_por, threshold=14)\n",
    "# df_por['pass'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now ask you to identify potential sensitive/protected attributes. Run the chunk below to see the different features in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T20:42:08.267282Z",
     "start_time": "2021-06-10T20:42:08.262276Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for reference\n",
    "print(datasets['student_por'].cat_cols)\n",
    "print(datasets['student_por'].num_cols)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T20:42:08.414127Z",
     "start_time": "2021-06-10T20:42:08.410308Z"
    }
   },
   "outputs": [],
   "source": [
    "sens_attrs = [df_por['sex'], df_por['address']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# format data\n",
    "X = df_por.iloc[:, :-2].values\n",
    "y = df_por.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T20:42:08.750322Z",
     "start_time": "2021-06-10T20:42:08.728346Z"
    }
   },
   "outputs": [],
   "source": [
    "# OHE categorical features (prompt for user's choice here?)\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# get indices of categorical columns\n",
    "def get_cat_cols(dataset):\n",
    "    df = dataset.df\n",
    "    res = []\n",
    "    for col in dataset.cat_cols:\n",
    "        res.append(df.columns.get_loc(col))\n",
    "    return res\n",
    "\n",
    "cat_cols = get_cat_cols(datasets['student_por'])\n",
    "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), cat_cols)], remainder='passthrough')\n",
    "X = np.array(ct.fit_transform(X))"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_true = X\n",
    "y_true = df_por['pass']\n",
    "#X_bias = copy.deepcopy(X_true)\n",
    "#y_bias = copy.deepcopy(y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias Injection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Distribution Visualization"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "#df_por.shape\n",
    "df_por['address'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "count = pd.value_counts(df_por['address'], sort = True)\n",
    "count.plot(kind = 'bar', rot = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias Selection"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - add different biases (over/under sampling, omitted variable, etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias Injection"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate based on protected attribute\n",
    "sens_values = sens_attrs[1].unique()\n",
    "\n",
    "# TODO - add prompt for user to specify which value is favored and which is unfavored\n",
    "\n",
    "df_favored = df_por[df_por['address'] == 'U']\n",
    "df_unfavored = df_por[df_por['address'] == 'R']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Under-Sampling Process"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# under-sampling process\n",
    "df_undersampled = df_unfavored.sample(n=190, random_state=42)\n",
    "\n",
    "#print(df_favored.shape, df_unfavored.shape, df_undersampled.shape)\n",
    "\n",
    "# combine undersampled and original favored class to create dataset\n",
    "df_concat = pd.concat([df_favored,df_undersampled])\n",
    "df_concat.shape\n",
    "\n",
    "# for fairness measures later\n",
    "df_sens = df_concat['address']\n",
    "\n",
    "# format data\n",
    "X_bias = df_concat.iloc[:, :-2].values\n",
    "#print(X_undersampled.shape)\n",
    "y_bias = df_concat.iloc[:, -1].values\n",
    "\n",
    "# OHE\n",
    "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), cat_cols)], remainder='passthrough')\n",
    "X_bias_true = np.array(ct.fit_transform(X_bias))\n",
    "y_bias_true = df_concat['pass']\n",
    "\n",
    "#print(X_true.shape, X_bias_true.shape)\n",
    "#print(y_true.value_counts(), \"\\n\\n\", y_bias_true.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Under-sampling w/ Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## inject under-sampling bias into synthetic data\n",
    "\n",
    "df_favored_gs = df_gaussian_synthetic[df_gaussian_synthetic['cat'] == 0]\n",
    "df_unfavored_gs = df_gaussian_synthetic[df_gaussian_synthetic['cat'] == 1]\n",
    "\n",
    "# under-sampling process\n",
    "pct_under = 0.9\n",
    "df_undersampled_gs = df_unfavored_gs.sample(n=int(pct_under*len(df_unfavored_gs)))\n",
    "\n",
    "df_concat_gs = pd.concat([df_favored_gs,df_undersampled_gs])\n",
    "\n",
    "# split X and y into arrays\n",
    "X_bias_gs = df_concat_gs.iloc[:, :-1].values\n",
    "y_bias_gs = df_concat_gs.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Post-Injection Visualization"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "favored = len(df_favored)\n",
    "true_unfavored = len(df_por[df_por['address'] == 'R'])\n",
    "bias_unfavored = len(df_undersampled)\n",
    "\n",
    "x_vals = ['Favored', \"Unfavored\"]\n",
    "y_vals_true = [favored, true_unfavored]\n",
    "y_vals_bias = [favored, bias_unfavored]\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.bar(x_vals, y_vals_true)\n",
    "plt.title(\"Ground Truth\")\n",
    "plt.ylabel(\"Count\")\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.bar(x_vals, y_vals_bias)\n",
    "plt.title(\"Under-Sampling\")\n",
    "plt.ylim([0,500])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model on Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## synthetic data: fit model on biased & ground truth, test on biased & ground truth\n",
    "\n",
    "# ground truth data (in array form)\n",
    "X_true_gs = df_gaussian_synthetic.iloc[:,:-1]\n",
    "y_true_gs = df_gaussian_synthetic.iloc[:,-1]\n",
    "\n",
    "classifier_gs = sk.linear_model.LogisticRegression()\n",
    "\n",
    "# fit & test on ground truth data\n",
    "classifier_true_gs = classifier_gs.fit(X_true_gs, y_true_gs)\n",
    "y_pred_truth_gs = classifier_true_gs.predict(X_true_gs)\n",
    "\n",
    "# fit on biased data, test on biased & ground truth\n",
    "classifier_bias_gs = classifier_gs.fit(X_bias_gs, y_bias_gs)\n",
    "y_pred_bias_gs = classifier_bias_gs.predict(X_bias_gs) # test on biased\n",
    "y_pred_bias_on_true_gs = classifier_bias_gs.predict(X_true_gs) # test on ground truth\n",
    "\n",
    "print(\"Accuracy of Ground Truth Model on Ground Truth Data: \", accuracy_score(y_pred_truth_gs, y_true_gs))\n",
    "print(\"Accuracy of Biased Model on Biased Data: \", accuracy_score(y_pred_bias_gs, y_bias_gs))\n",
    "print(\"Accuracy of Biased Model on Ground Truth Data: \", accuracy_score(y_pred_bias_on_true_gs, y_true_gs))\n",
    "\n",
    "'''The model trained on ground truth data doesn't achieve perfect test accuracy on ground truth data \n",
    "because of the binary label noise (flipped prop_noise proportion of binary labels). Set prop_noise=0 and\n",
    "it'll be ~99% accurate.'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Selection + Training (TODO: modularize)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T20:42:09.057618Z",
     "start_time": "2021-06-10T20:42:08.942574Z"
    }
   },
   "outputs": [],
   "source": [
    "# modularize and add data struct of different ml techniques\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "classifier = DecisionTreeClassifier(min_samples_leaf = 10, max_depth = 4)\n",
    "\n",
    "classifier_true = classifier.fit(X_true, y_true)\n",
    "y_pred_truth = classifier_true.predict(X_true)\n",
    "\n",
    "classifier_bias = classifier.fit(X_bias_true, y_bias_true)\n",
    "y_pred_bias = classifier_bias.predict(X_bias_true)\n",
    "y_pred_bias_on_true = classifier_bias.predict(X_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Performance (TODO: modularize)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T20:42:09.231352Z",
     "start_time": "2021-06-10T20:42:09.225108Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Accuracy of Ground Truth Model on Ground Truth Data: \", accuracy_score(y_pred_truth, y_true))\n",
    "print(\"Accuracy of Biased Model on Biased Data: \", accuracy_score(y_pred_bias, y_bias_true))\n",
    "print(\"Accuracy of Biased Model on Ground Truth Data: \", accuracy_score(y_pred_bias_on_true, y_true))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we can see that under-sampling decreases model performance on the ground truth data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T21:16:04.515563Z",
     "start_time": "2021-06-10T21:16:04.499948Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ground Truth Model on Ground Truth Data\n",
    "\n",
    "gm_true = MetricFrame(accuracy_score, y_true, y_pred_truth, sensitive_features = sens_attrs[1])\n",
    "print(\"Overall Accuracy: \", gm_true.overall)\n",
    "print(\"Group Accuracy : \", gm_true.by_group)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "sr_true = MetricFrame(selection_rate, y_true, y_pred_truth, sensitive_features = sens_attrs[1])\n",
    "print(\"Overall Selection Rate: \", sr_true.overall)\n",
    "print(\"Group Selection Rate : \", sr_true.by_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we can see that there is a **5% discrepancy** in the selection rate for Rural vs Urban students (with Urban students being preferred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Biased Model on Biased Data\n",
    "\n",
    "gm_bias = MetricFrame(accuracy_score, y_bias_true, y_pred_bias, sensitive_features = df_sens)\n",
    "print(\"Overall Accuracy: \", gm_bias.overall)\n",
    "print(\"Group Accuracy : \", gm_bias.by_group)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "sr_bias = MetricFrame(selection_rate, y_bias_true, y_pred_bias, sensitive_features = df_sens)\n",
    "print(\"Overall Selection Rate: \", sr_bias.overall)\n",
    "print(\"Group Selection Rate : \", sr_bias.by_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we can see that there is now a **7% discrepancy due to under-sampling** in the selection rate for Rural vs Urban students (with Urban students being preferred) on biased data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Biased Model on Ground Truth Data\n",
    "\n",
    "gm_bias_on_true = MetricFrame(accuracy_score, y_true,\n",
    "                           y_pred_bias_on_true, sensitive_features = sens_attrs[1])\n",
    "print(\"Overall Accuracy: \", gm_bias_on_true.overall)\n",
    "print(\"Group Accuracy : \", gm_bias_on_true.by_group)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "sr_bias_on_true = MetricFrame(selection_rate, y_true,\n",
    "                              y_pred_bias_on_true, sensitive_features = sens_attrs[1])\n",
    "print(\"Overall Selection Rate: \", sr_bias_on_true.overall)\n",
    "print(\"Group Selection Rate : \", sr_bias_on_true.by_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we see that the biased model has lower overall and subgroup accuracy on the ground truth data. \n",
    "Also, we can observe that the selection rate disparity increased ever so slightly with the biased model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T21:16:54.784535Z",
     "start_time": "2021-06-10T21:16:54.725029Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Ground Truth Model on Ground Truth Data\")\n",
    "\n",
    "print(\"EOD:\", equalized_odds_difference(y_true=y_true, y_pred = y_pred_truth, sensitive_features=sens_attrs[1]))\n",
    "print(\"DPD:\", demographic_parity_difference(y_true=y_true, y_pred = y_pred_truth, sensitive_features=sens_attrs[1]))\n",
    "\n",
    "print(\"EOR:\", equalized_odds_ratio(y_true=y_true, y_pred = y_pred_truth, sensitive_features=sens_attrs[1]))\n",
    "print(\"DPR:\", demographic_parity_ratio(y_true=y_true, y_pred = y_pred_truth, sensitive_features=sens_attrs[1]))\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Biased Model on Ground Truth Data\")\n",
    "\n",
    "print(\"EOD:\", equalized_odds_difference(y_true=y_true, y_pred = y_pred_bias_on_true, sensitive_features=sens_attrs[1]))\n",
    "print(\"DPD:\", demographic_parity_difference(y_true=y_true, y_pred = y_pred_bias_on_true, sensitive_features=sens_attrs[1]))\n",
    "\n",
    "print(\"EOR:\", equalized_odds_ratio(y_true=y_true, y_pred = y_pred_bias_on_true, sensitive_features=sens_attrs[1]))\n",
    "print(\"DPR:\", demographic_parity_ratio(y_true=y_true, y_pred = y_pred_bias_on_true, sensitive_features=sens_attrs[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fairness Visualization (Keep this?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-09T17:51:41.613558Z",
     "start_time": "2021-06-09T17:51:41.586931Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from fairlearn.widget import FairlearnDashboard\n",
    "FairlearnDashboard(sensitive_features = df_por['sex'],\n",
    "                   sensitive_feature_names = ['sex'],\n",
    "                   y_true = y_true,\n",
    "                   y_pred = {\"initial model\" : y_pred_truth})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-09T17:51:41.635383Z",
     "start_time": "2021-06-09T17:51:41.615653Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from fairlearn.widget import FairlearnDashboard\n",
    "FairlearnDashboard(sensitive_features = df_por['address'],\n",
    "                   sensitive_feature_names = ['address'],\n",
    "                   y_true = y_true,\n",
    "                   y_pred = {\"initial model\" : y_pred_bias_on_true})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fairness Intervention"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairlearn.reductions import ExponentiatedGradient, DemographicParity\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-09T17:51:42.957037Z",
     "start_time": "2021-06-09T17:51:41.643906Z"
    }
   },
   "outputs": [],
   "source": [
    "constraint = DemographicParity()\n",
    "mitigator_true = ExponentiatedGradient(classifier_true, constraint)\n",
    "mitigator_true.fit(X_true, y_true, sensitive_features = sens_attrs[1])\n",
    "y_pred_mitigated_true = mitigator_true.predict(X_true)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "constraint = DemographicParity()\n",
    "mitigator_bias = ExponentiatedGradient(classifier_bias, constraint)\n",
    "mitigator_bias.fit(X_bias_true, y_bias_true, sensitive_features = df_sens)\n",
    "y_pred_mitigated_bias = mitigator_bias.predict(X_bias_true)\n",
    "y_pred_mitigated_bias_on_true = mitigator_bias.predict(X_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: create data structure of different ml performance metrics\n",
    "#       get user input and modularize output shown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy of Ground Truth Model + Fairness Intervention on Ground Truth Data: \",\n",
    "      accuracy_score(y_pred_mitigated_true, y_true))\n",
    "\n",
    "print(\"Accuracy of Biased Model + Fairness Intervention on Ground Truth Data: \",\n",
    "      accuracy_score(y_pred_mitigated_bias_on_true, y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-09T17:51:42.986098Z",
     "start_time": "2021-06-09T17:51:42.973042Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ground Truth Model + Fairness Intervention on Ground Truth Data\n",
    "\n",
    "gm_mitigated = MetricFrame(accuracy_score, y_true, y_pred_mitigated_true, sensitive_features = sens_attrs[1])\n",
    "print(\"Overall Accuracy: \", gm_mitigated.overall)\n",
    "print(\"Group Accuracy : \", gm_mitigated.by_group)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "sr_mitigated = MetricFrame(selection_rate, y_true, y_pred_mitigated_true, sensitive_features = sens_attrs[1])\n",
    "print(\"Overall Selection Rate: \", sr_mitigated.overall)\n",
    "print(\"Group Selection Rate : \", sr_mitigated.by_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Biased Model + Fairness Intervention on Ground Truth Data\n",
    "\n",
    "gm_mitigated_bias_on_true = MetricFrame(accuracy_score, y_true, y_pred_mitigated_bias_on_true, sensitive_features = sens_attrs[1])\n",
    "print(\"Overall Accuracy: \", gm_mitigated_bias_on_true.overall)\n",
    "print(\"Group Accuracy : \", gm_mitigated_bias_on_true.by_group)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "sr_mitigated_bias_on_true = MetricFrame(selection_rate, y_true, y_pred_mitigated_bias_on_true, sensitive_features = sens_attrs[1])\n",
    "print(\"Overall Selection Rate: \", sr_mitigated_bias_on_true.overall)\n",
    "print(\"Group Selection Rate : \", sr_mitigated_bias_on_true.by_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, we observe an increase in overall and subgroup accuracy, but a decrease in disadvantaged subgroup selection rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-09T17:51:43.021839Z",
     "start_time": "2021-06-09T17:51:42.998287Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "FairlearnDashboard(sensitive_features = sens_attrs[1],\n",
    "                   sensitive_feature_names = ['address'],\n",
    "                   y_true = y_true,\n",
    "                   y_pred = {\"initial model\" : y_pred_truth, \"mitigated model\": y_pred_mitigated_true})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias vs Accuracy vs Fairness Trade-Off"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if verbose, shows \"Finished iteration: ... \"\n",
    "# if apply_fairness, uses fairness intervention\n",
    "def tradeoff_visualization(classifier, apply_fairness = False, verbose = False):\n",
    "    \n",
    "    bias_amts = list(range(0,200,10))\n",
    "    accuracy_on_true = []\n",
    "    accuracy_on_biased = []\n",
    "    accuracy_on_true_mitigated = []\n",
    "    accuracy_on_biased_mitigated = []\n",
    "    eod_on_true = []\n",
    "    eod_on_biased = []\n",
    "    dataset_size_true = np.full(shape=len(bias_amts), fill_value= X_true.shape[0]).tolist()\n",
    "    dataset_size_bias = []\n",
    "    table = []\n",
    "\n",
    "    classifier_true = classifier.fit(X_true, y_true)\n",
    "    y_pred_truth = classifier_true.predict(X_true)\n",
    "\n",
    "    df_undersampled = df_unfavored.sample(n=len(df_unfavored), random_state=42)\n",
    "\n",
    "    for i in range(20):\n",
    "        # under-sampling process\n",
    "        if i == 0:\n",
    "            df_undersampled = df_undersampled.sample(n=len(df_undersampled), random_state=42)\n",
    "        else:\n",
    "            df_undersampled = df_undersampled.sample(n=len(df_undersampled)-10, random_state=42)\n",
    "\n",
    "        # combine undersampled and original favored class to create dataset\n",
    "        df_concat = pd.concat([df_favored,df_undersampled])\n",
    "        df_concat.shape\n",
    "        df_sens = df_concat['address']\n",
    "\n",
    "        # format data\n",
    "        X_bias = df_concat.iloc[:, :-2].values\n",
    "        y_bias = df_concat.iloc[:, -1].values\n",
    "\n",
    "        # OHE\n",
    "        ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), cat_cols)], remainder='passthrough')\n",
    "        X_bias_true = np.array(ct.fit_transform(X_bias))\n",
    "        y_bias_true = df_concat['pass']\n",
    "\n",
    "        dataset_size_bias.append(X_bias_true.shape[0])\n",
    "        classifier_bias = classifier.fit(X_bias_true, y_bias_true)\n",
    "        \n",
    "        if apply_fairness:\n",
    "            constraint = DemographicParity()\n",
    "            classifier_mitigated_bias = ExponentiatedGradient(classifier_bias, constraint)\n",
    "            classifier_mitigated_bias.fit(X_bias_true, y_bias_true, sensitive_features = df_sens)\n",
    "            \n",
    "            # testing on biased data WITH fairness intervention\n",
    "            y_pred_mitigated_bias = classifier_mitigated_bias.predict(X_bias_true)\n",
    "            \n",
    "            # testing on GT data WITH fairness intervention\n",
    "            y_pred_mitigated_bias_on_true = classifier_mitigated_bias.predict(X_true)\n",
    "        \n",
    "        # testing on biased data withOUT fairness intervention\n",
    "        y_pred_bias = classifier_bias.predict(X_bias_true)\n",
    "        \n",
    "        # testing on GT data withOUT fairness intervention\n",
    "        y_pred_bias_on_true = classifier_bias.predict(X_true)\n",
    "\n",
    "        # model performance\n",
    "        \n",
    "        if apply_fairness:\n",
    "            # on biased data\n",
    "            acc_bias_mitigated = accuracy_score(y_pred=y_pred_mitigated_bias, y_true=y_bias_true)\n",
    "            accuracy_on_biased_mitigated.append(acc_bias_mitigated)\n",
    "            # on GT data\n",
    "            acc_bias_mitigated_on_true = accuracy_score(y_pred=y_pred_mitigated_bias_on_true, y_true=y_true)\n",
    "            accuracy_on_true_mitigated.append(acc_bias_mitigated_on_true)\n",
    "        \n",
    "        # on biased data\n",
    "        acc_bias = accuracy_score(y_pred=y_pred_bias, y_true=y_bias_true)\n",
    "        accuracy_on_biased.append(acc_bias)\n",
    "        # on GT data\n",
    "        acc_bias_on_true = accuracy_score(y_pred=y_pred_bias_on_true, y_true=y_true)\n",
    "        accuracy_on_true.append(acc_bias_on_true)\n",
    "\n",
    "        # fairness performance (TODO)\n",
    "        '''\n",
    "        eod_true = equalized_odds_difference(y_true=y_bias_true, y_pred = y_pred_bias, sensitive_features=df_sens)\n",
    "        eod_on_true.append(eod_true)\n",
    "\n",
    "        eod_bias_on_true = equalized_odds_difference(y_true=y_true, y_pred = y_pred_bias_on_true, sensitive_features=sens_attrs[1])\n",
    "        eod_on_biased.append(eod_bias_on_true)\n",
    "        '''\n",
    "\n",
    "        # table visualization \n",
    "        table_elem = [i*10, acc_bias, acc_bias_on_true]\n",
    "        table.append(table_elem)\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"Finished Iteration: \", len(df_concat))\n",
    "\n",
    "    return bias_amts, dataset_size_true, dataset_size_bias, accuracy_on_biased, accuracy_on_true, eod_on_biased, eod_on_true"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_visualizations(bias_amts, dataset_size_true, dataset_size_bias,\n",
    "                            accuracy_on_biased = [], accuracy_on_true = [],\n",
    "                            accuracy_on_biased_mitigated = [],\n",
    "                            accuracy_on_true_mitigated = [], fairness = False):\n",
    "    \n",
    "    if fairness:\n",
    "        plt.figure(figsize=(17,7))\n",
    "\n",
    "        plt.subplot(1,2,1)\n",
    "        plt.plot(bias_amts, accuracy_on_true_mitigated, label = 'Ground Truth')\n",
    "        plt.plot(bias_amts, accuracy_on_biased_mitigated, label = 'Biased Data')\n",
    "        plt.xlabel(\"Amount of Bias (number of minority samples removed)\")\n",
    "        plt.ylabel(\"Accuracy Score\")\n",
    "        plt.axhline(y=accuracy_score(y_pred_truth, y_true), color = \"green\", label = \"Ground Truth Model Accuracy\", alpha = 0.5)\n",
    "        plt.title(\"Biased Model Accuracy\")\n",
    "        plt.ylim(0.92, 0.99)\n",
    "        plt.legend()\n",
    "\n",
    "        plt.subplot(1,2,2)\n",
    "        plt.plot(bias_amts, dataset_size_true, label = 'Ground Truth')\n",
    "        plt.plot(bias_amts, dataset_size_bias, label = 'Biased Data')\n",
    "        plt.xlabel(\"Amount of Bias (number of minority samples removed)\")\n",
    "        plt.ylabel(\"Dataset Size\")\n",
    "        plt.legend()\n",
    "\n",
    "        plt.show()\n",
    "        \n",
    "    else:\n",
    "        plt.figure(figsize=(17,7))\n",
    "\n",
    "        plt.subplot(1,2,1)\n",
    "        plt.plot(bias_amts, accuracy_on_true, label = 'Ground Truth')\n",
    "        plt.plot(bias_amts, accuracy_on_biased, label = 'Biased Data')\n",
    "        plt.xlabel(\"Amount of Bias (number of minority samples removed)\")\n",
    "        plt.ylabel(\"Accuracy Score\")\n",
    "        plt.axhline(y=accuracy_score(y_pred_truth, y_true), color = \"green\", label = \"Ground Truth Model Accuracy\", alpha = 0.5)\n",
    "        plt.title(\"Biased Model Accuracy\")\n",
    "        plt.ylim(0.92, 0.99)\n",
    "        plt.legend()\n",
    "\n",
    "        plt.subplot(1,2,2)\n",
    "        plt.plot(bias_amts, dataset_size_true, label = 'Ground Truth')\n",
    "        plt.plot(bias_amts, dataset_size_bias, label = 'Biased Data')\n",
    "        plt.xlabel(\"Amount of Bias (number of minority samples removed)\")\n",
    "        plt.ylabel(\"Dataset Size\")\n",
    "        plt.legend()\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "52f79ddb-5166-423d-862f-72e9f33f9214",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fairness_visualizations(bias_amts, eod_on_true = [], eod_on_biased = [],\n",
    "                           eod_on_biased_mitigated = [], eod_on_true_mitigated = [],\n",
    "                           fairness = False):\n",
    "    if fairness:\n",
    "        plt.plot(bias_amts, eod_on_true_mitigated, label = 'Ground Truth')\n",
    "        plt.plot(bias_amts, eod_on_biased_mitigated, label = 'Biased Data')\n",
    "        plt.xlabel(\"Amount of Bias (number of minority samples removed)\")\n",
    "        plt.ylabel(\"Equalized Odds Difference\")\n",
    "        plt.legend()\n",
    "        plt.title(\"Biased Model Equalized Odds Difference\")\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.plot(bias_amts, eod_on_true, label = 'Ground Truth')\n",
    "        plt.plot(bias_amts, eod_on_biased, label = 'Biased Data')\n",
    "        plt.xlabel(\"Amount of Bias (number of minority samples removed)\")\n",
    "        plt.ylabel(\"Equalized Odds Difference\")\n",
    "        plt.legend()\n",
    "        plt.title(\"Biased Model Equalized Odds Difference\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_amts, dataset_size_true, dataset_size_bias, accuracy_on_biased, accuracy_on_true, eod_on_biased, eod_on_true = tradeoff_visualization(classifier, False, False)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_visualizations(bias_amts, dataset_size_true, dataset_size_bias, accuracy_on_biased, accuracy_on_true, eod_on_biased, eod_on_true, False)"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "fairness_visualizations(bias_amts, eod_on_true, eod_on_biased, False)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_amts, dataset_size_true, dataset_size_bias, accuracy_on_biased, accuracy_on_true, eod_on_biased, eod_on_true = tradeoff_visualization(classifier, True, False)"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "accuracy_visualizations(bias_amts, dataset_size_true, dataset_size_bias, accuracy_on_biased, accuracy_on_true, eod_on_biased, eod_on_true, True)"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "fairness_visualizations(bias_amts, eod_on_true, eod_on_biased, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Special Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_amts = list(range(0,200,10))\n",
    "accuracy_on_true = []\n",
    "accuracy_on_biased = []\n",
    "accuracy_on_true_mitigated = []\n",
    "accuracy_on_biased_mitigated = []\n",
    "eod_on_true = []\n",
    "eod_on_biased = []\n",
    "eod_on_true_mitigated = []\n",
    "eod_on_biased_mitigated = []\n",
    "dataset_size_true = np.full(shape=len(bias_amts), fill_value= X_true.shape[0]).tolist()\n",
    "dataset_size_bias = []\n",
    "table = []\n",
    "\n",
    "classifier_true = classifier.fit(X_true, y_true)\n",
    "y_pred_truth = classifier_true.predict(X_true)\n",
    "\n",
    "df_undersampled = df_unfavored.sample(n=len(df_unfavored), random_state=42)\n",
    "\n",
    "for i in range(20):\n",
    "    # under-sampling process\n",
    "    if i == 0:\n",
    "        df_undersampled = df_undersampled.sample(n=len(df_undersampled), random_state=42)\n",
    "    else:\n",
    "        df_undersampled = df_undersampled.sample(n=len(df_undersampled)-10, random_state=42)\n",
    "\n",
    "    # combine undersampled and original favored class to create dataset\n",
    "    df_concat = pd.concat([df_favored,df_undersampled])\n",
    "    df_concat.shape\n",
    "    df_sens = df_concat['address']\n",
    "\n",
    "    # format data\n",
    "    X_bias = df_concat.iloc[:, :-2].values\n",
    "    y_bias = df_concat.iloc[:, -1].values\n",
    "\n",
    "    # OHE\n",
    "    ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), cat_cols)], remainder='passthrough')\n",
    "    X_bias_true = np.array(ct.fit_transform(X_bias))\n",
    "    y_bias_true = df_concat['pass']\n",
    "\n",
    "    dataset_size_bias.append(X_bias_true.shape[0])\n",
    "    classifier_bias = classifier.fit(X_bias_true, y_bias_true)\n",
    "\n",
    "    constraint = DemographicParity()\n",
    "    classifier_mitigated_bias = ExponentiatedGradient(classifier_bias, constraint)\n",
    "    classifier_mitigated_bias.fit(X_bias_true, y_bias_true, sensitive_features = df_sens)\n",
    "    \n",
    "    # model testing\n",
    "    \n",
    "    # on biased data\n",
    "    \n",
    "    # without fairness intervention\n",
    "    y_pred_bias = classifier_bias.predict(X_bias_true)\n",
    "    # with fairness intervention\n",
    "    y_pred_mitigated_bias = classifier_mitigated_bias.predict(X_bias_true)\n",
    "    \n",
    "    # on GT data\n",
    "    \n",
    "    # without fairness intervention\n",
    "    y_pred_bias_on_true = classifier_bias.predict(X_true)\n",
    "    # with fairness intervention\n",
    "    y_pred_mitigated_bias_on_true = classifier_mitigated_bias.predict(X_true)\n",
    "    \n",
    "    \n",
    "    # model performance\n",
    "    \n",
    "    # on biased data\n",
    "    \n",
    "    # without fairness intervention\n",
    "    acc_bias = accuracy_score(y_pred=y_pred_bias, y_true=y_bias_true)\n",
    "    accuracy_on_biased.append(acc_bias)\n",
    "    # with fairness intervention\n",
    "    acc_bias_mitigated = accuracy_score(y_pred=y_pred_mitigated_bias, y_true=y_bias_true)\n",
    "    accuracy_on_biased_mitigated.append(acc_bias_mitigated)\n",
    "    \n",
    "    # on GT data\n",
    "    \n",
    "    # without fairness intervention\n",
    "    acc_bias_on_true = accuracy_score(y_pred=y_pred_bias_on_true, y_true=y_true)\n",
    "    accuracy_on_true.append(acc_bias_on_true)\n",
    "    # with fairness intervention\n",
    "    acc_bias_mitigated_on_true = accuracy_score(y_pred=y_pred_mitigated_bias_on_true, y_true=y_true)\n",
    "    accuracy_on_true_mitigated.append(acc_bias_mitigated_on_true)\n",
    "\n",
    "    # fairness performance\n",
    "    \n",
    "    eod_true = equalized_odds_difference(y_true=y_bias_true, y_pred = y_pred_bias, sensitive_features=df_sens)\n",
    "    eod_on_true.append(eod_true)\n",
    "\n",
    "    eod_bias_on_true = equalized_odds_difference(y_true=y_true, y_pred = y_pred_bias_on_true, sensitive_features=sens_attrs[1])\n",
    "    eod_on_biased.append(eod_bias_on_true)\n",
    "    \n",
    "    eod_bias_true_mitigated = equalized_odds_difference(y_true=y_true, y_pred = y_pred_mitigated_bias_on_true, sensitive_features=sens_attrs[1])\n",
    "    eod_on_true_mitigated.append(eod_bias_true_mitigated)\n",
    "    \n",
    "    # print(\"Finished Iteration: \", len(df_concat))\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(17,7))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(bias_amts, accuracy_on_biased, label = 'Tested On Biased Data + No Fairness Intervention', color = \"red\")\n",
    "plt.plot(bias_amts, accuracy_on_biased_mitigated, label = 'Tested On Biased Data + Fairness Intervention', color = \"green\")\n",
    "plt.plot(bias_amts, accuracy_on_true, label = 'Tested On Ground Truth + No Fairness Intervention', color = \"blue\")\n",
    "plt.plot(bias_amts, accuracy_on_true_mitigated, label = 'Tested On Ground Truth + Fairness Intervention', color = \"purple\")\n",
    "plt.xlabel(\"Amount of Bias (number of minority samples removed)\")\n",
    "plt.ylabel(\"Accuracy Score\")\n",
    "#plt.axhline(y=accuracy_score(y_pred_truth, y_true), color = \"green\", label = \"Ground Truth Model On Ground Truth Data\", alpha = 0.5)\n",
    "plt.title(\"Accuracy of Biased Model (trained on biased data)\")\n",
    "plt.ylim(0.93, 0.99)\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(bias_amts, dataset_size_true, label = 'Ground Truth')\n",
    "plt.plot(bias_amts, dataset_size_bias, label = 'Biased Data')\n",
    "plt.xlabel(\"Amount of Bias (number of minority samples removed)\")\n",
    "plt.ylabel(\"Dataset Size\")\n",
    "plt.title(\"Amount of Bias vs Dataset Size\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "plt.plot(bias_amts, eod_on_biased, label = 'On Biased Data + No Fairness Intervention', color = \"red\")\n",
    "plt.plot(bias_amts, eod_on_true, label = 'On Ground Truth + No Fairness Intervention', color = \"blue\")\n",
    "plt.plot(bias_amts, eod_on_true_mitigated, label = 'On Ground Truth + Fairness Intervention', color = \"purple\")\n",
    "plt.xlabel(\"Amount of Bias (number of minority samples removed)\")\n",
    "plt.ylabel(\"Equalized Odds Difference\")\n",
    "plt.axhline(y=equalized_odds_difference(y_true=y_true, y_pred = y_pred_truth, sensitive_features=sens_attrs[1]),\n",
    "            color = \"green\",\n",
    "            label = \"Ground Truth EOD\", alpha = 0.5)\n",
    "plt.legend()\n",
    "plt.title(\"Biased Model Equalized Odds Difference\")\n",
    "plt.show()\n",
    "#plt.savefig(\"bias1.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
