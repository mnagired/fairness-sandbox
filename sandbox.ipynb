{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-08T20:41:26.879421Z",
     "start_time": "2021-06-08T20:41:26.876140Z"
    }
   },
   "source": [
    "# Fairness Sandox Prototype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This rudimentary version of the framework will contain one dataset, one model, one bias mitigation algorithm, and a limited amount of validation metrics. Emphasis will be on modularity and abstraction so as to encourage more features in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "Please run the code block below to install the necessary packages (if needed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pip install aif360\n",
    "%pip install fairlearn\n",
    "%pip install imbalanced-learn\n",
    "%pip install matplotlib\n",
    "%pip install numpy\n",
    "%pip install pandas\n",
    "%pip install seaborn\n",
    "%pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T20:42:02.586362Z",
     "start_time": "2021-06-10T20:42:02.581026Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, roc_curve, auc\n",
    "from collections import Counter\n",
    "\n",
    "import fairlearn\n",
    "from fairlearn.metrics import *\n",
    "import aif360\n",
    "\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data + EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T20:42:06.040700Z",
     "start_time": "2021-06-10T20:42:06.035368Z"
    }
   },
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, short_name = '', path = '', cat_cols = [], num_cols = []):\n",
    "        self.short_name = short_name\n",
    "        self.path = path\n",
    "        self.cat_cols = cat_cols\n",
    "        self.num_cols = num_cols\n",
    "        self.df = pd.read_csv(path, sep = ';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Popular Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T20:42:06.330948Z",
     "start_time": "2021-06-10T20:42:06.325325Z"
    }
   },
   "outputs": [],
   "source": [
    "# each dataset is a dictionary where keys = short name, values = Dataset object\n",
    "\n",
    "datasets = dict()\n",
    "\n",
    "def add_dataset(dataset):\n",
    "    if not isinstance(dataset, Dataset):\n",
    "        print(\"Error! Please enter a valid Dataset object\")\n",
    "    else:\n",
    "        if dataset.short_name not in datasets.keys():\n",
    "            datasets[dataset.short_name] = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T20:42:06.534123Z",
     "start_time": "2021-06-10T20:42:06.464059Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# example - adding a dataset\n",
    "path_adult_income = 'Datasets/adult.csv'\n",
    "cat_cols = ['workclass', 'education','marital-status', 'occupation', 'relationship', 'race',\n",
    "            'gender', 'native-country','income']\n",
    "num_cols = ['age', 'fnlwgt', 'educational-num', 'capital-gain', 'capital-loss', 'hours-per-week']\n",
    "adult_income = Dataset('adult_income', path_adult_income, cat_cols, num_cols)\n",
    "\n",
    "add_dataset(adult_income)\n",
    "\n",
    "# TODO - add more datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T20:42:06.621748Z",
     "start_time": "2021-06-10T20:42:06.598777Z"
    }
   },
   "outputs": [],
   "source": [
    "cat = ['school', 'sex', 'address','famsize','Pstatus','Mjob','Fjob','reason',\n",
    "       'guardian','schoolsup','famsup','paid', 'activities','nursery','higher', 'internet','romantic']\n",
    "num = ['age', 'Medu', 'Fedu','traveltime','studytime','failures', 'famrel',\n",
    "       'freetime','goout','Dalc','Walc','health','absences','G1', 'G2', 'G3']\n",
    "\n",
    "#add_dataset(Dataset(\"student_mat\", path='Datasets/student-mat.csv', cat_cols=cat, num_cols=num))\n",
    "add_dataset(Dataset(\"student_por\", path='Datasets/student-por.csv', cat_cols=cat, num_cols=num))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to use any/all of the following EDA functions and/or add your own!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T20:42:07.151553Z",
     "start_time": "2021-06-10T20:42:07.125905Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# take a peek at the first few data points\n",
    "df_por = datasets['student_por'].df\n",
    "# df_por.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T20:42:07.426895Z",
     "start_time": "2021-06-10T20:42:07.269718Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_counts(df, attr):\n",
    "    if attr in df.columns:\n",
    "        df[attr].value_counts(normalize=True).plot.barh()\n",
    "    else:\n",
    "        print(\"Error! Please enter a valid feature.\")\n",
    "\n",
    "# example\n",
    "# plot_counts(df_por, 'sex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T20:42:08.016753Z",
     "start_time": "2021-06-10T20:42:07.428807Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def group_by_plot(df, attr1, attr2):\n",
    "    for val in list(df[attr1].unique()):\n",
    "        print(val)\n",
    "        temp = df[df[attr1] == val]\n",
    "        sns.displot(temp[attr2])\n",
    "\n",
    "# example\n",
    "# group_by_plot(df_por, 'sex', 'G3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T20:42:08.021705Z",
     "start_time": "2021-06-10T20:42:08.019363Z"
    }
   },
   "outputs": [],
   "source": [
    "# add support for sns.pairplot, plt.scatter, sns.heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Formulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T20:42:08.045526Z",
     "start_time": "2021-06-10T20:42:08.023712Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def threshold(df, g_1=0.3, g_2=0.3, g_3=0.4, threshold=11):\n",
    "    \"\"\"\n",
    "    Added \"pass/fail\" to make problem binary classification\n",
    "    \"\"\"\n",
    "    assert g_1 + g_2 + g_3 == 1, \"The sum of the percentages should be 1\"\n",
    "    assert 0 < threshold < 20, \"Threshold needs to be between 0 and 20\"\n",
    "    df['pass'] = df.apply(lambda row: 1\n",
    "                                 if g_1*row['G1'] + g_2*row['G2'] + g_3*row['G3'] >= threshold\n",
    "                                 else 0, axis=1)\n",
    "threshold(df_por, threshold=14)\n",
    "# df_por['pass'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now ask you to identify potential sensitive/protected attributes. Run the chunk below to see the different features in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T20:42:08.267282Z",
     "start_time": "2021-06-10T20:42:08.262276Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for reference\n",
    "print(datasets['student_por'].cat_cols)\n",
    "print(datasets['student_por'].num_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T20:42:08.414127Z",
     "start_time": "2021-06-10T20:42:08.410308Z"
    }
   },
   "outputs": [],
   "source": [
    "sens_attrs = [df_por['sex'], df_por['address']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# format data\n",
    "X = df_por.iloc[:, :-2].values\n",
    "y = df_por.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T20:42:08.750322Z",
     "start_time": "2021-06-10T20:42:08.728346Z"
    }
   },
   "outputs": [],
   "source": [
    "# OHE categorical features (prompt for user's choice here?)\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# get indices of categorical columns\n",
    "def get_cat_cols(dataset):\n",
    "    df = dataset.df\n",
    "    res = []\n",
    "    for col in dataset.cat_cols:\n",
    "        res.append(df.columns.get_loc(col))\n",
    "    return res\n",
    "\n",
    "cat_cols = get_cat_cols(datasets['student_por'])\n",
    "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), cat_cols)], remainder='passthrough')\n",
    "X = np.array(ct.fit_transform(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_true = X\n",
    "y_true = df_por['pass']\n",
    "#X_bias = copy.deepcopy(X_true)\n",
    "#y_bias = copy.deepcopy(y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias Injection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Distribution Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#df_por.shape\n",
    "df_por['address'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "count = pd.value_counts(df_por['address'], sort = True)\n",
    "count.plot(kind = 'bar', rot = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - add different biases (over/under sampling, omitted variable, etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias Injection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate based on protected attribute\n",
    "sens_values = sens_attrs[1].unique()\n",
    "\n",
    "# TODO - add prompt for user to specify which value is favored and which is unfavored\n",
    "\n",
    "df_favored = df_por[df_por['address'] == 'U']\n",
    "df_unfavored = df_por[df_por['address'] == 'R']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Under-Sampling Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# under-sampling process\n",
    "df_undersampled = df_unfavored.sample(n=190, random_state=42)\n",
    "\n",
    "#print(df_favored.shape, df_unfavored.shape, df_undersampled.shape)\n",
    "\n",
    "# combine undersampled and original favored class to create dataset\n",
    "df_concat = pd.concat([df_favored,df_undersampled])\n",
    "df_concat.shape\n",
    "\n",
    "# for fairness measures later\n",
    "df_sens = df_concat['address']\n",
    "\n",
    "# format data\n",
    "X_bias = df_concat.iloc[:, :-2].values\n",
    "#print(X_undersampled.shape)\n",
    "y_bias = df_concat.iloc[:, -1].values\n",
    "\n",
    "# OHE\n",
    "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), cat_cols)], remainder='passthrough')\n",
    "X_bias_true = np.array(ct.fit_transform(X_bias))\n",
    "y_bias_true = df_concat['pass']\n",
    "\n",
    "#print(X_true.shape, X_bias_true.shape)\n",
    "#print(y_true.value_counts(), \"\\n\\n\", y_bias_true.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Post-Injection Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "favored = len(df_favored)\n",
    "true_unfavored = len(df_por[df_por['address'] == 'R'])\n",
    "bias_unfavored = len(df_undersampled)\n",
    "\n",
    "x_vals = ['Favored', \"Unfavored\"]\n",
    "y_vals_true = [favored, true_unfavored]\n",
    "y_vals_bias = [favored, bias_unfavored]\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.bar(x_vals, y_vals_true)\n",
    "plt.title(\"Ground Truth\")\n",
    "plt.ylabel(\"Count\")\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.bar(x_vals, y_vals_bias)\n",
    "plt.title(\"Under-Sampling\")\n",
    "plt.ylim([0,500])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Selection + Training (TODO: modularize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T20:42:09.057618Z",
     "start_time": "2021-06-10T20:42:08.942574Z"
    }
   },
   "outputs": [],
   "source": [
    "# modularize and add data struct of different ml techniques\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "classifier = DecisionTreeClassifier(min_samples_leaf = 10, max_depth = 4)\n",
    "\n",
    "classifier_true = classifier.fit(X_true, y_true)\n",
    "y_pred_truth = classifier_true.predict(X_true)\n",
    "\n",
    "classifier_bias = classifier.fit(X_bias_true, y_bias_true)\n",
    "y_pred_bias = classifier_bias.predict(X_bias_true)\n",
    "y_pred_bias_on_true = classifier_bias.predict(X_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Performance (TODO: modularize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T20:42:09.231352Z",
     "start_time": "2021-06-10T20:42:09.225108Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Accuracy of Ground Truth Model on Ground Truth Data: \", accuracy_score(y_pred_truth, y_true))\n",
    "print(\"Accuracy of Biased Model on Biased Data: \", accuracy_score(y_pred_bias, y_bias_true))\n",
    "print(\"Accuracy of Biased Model on Ground Truth Data: \", accuracy_score(y_pred_bias_on_true, y_true))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we can see that under-sampling decreases model performance on the ground truth data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T21:16:04.515563Z",
     "start_time": "2021-06-10T21:16:04.499948Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ground Truth Model on Ground Truth Data\n",
    "\n",
    "gm_true = MetricFrame(accuracy_score, y_true, y_pred_truth, sensitive_features = sens_attrs[1])\n",
    "print(\"Overall Accuracy: \", gm_true.overall)\n",
    "print(\"Group Accuracy : \", gm_true.by_group)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "sr_true = MetricFrame(selection_rate, y_true, y_pred_truth, sensitive_features = sens_attrs[1])\n",
    "print(\"Overall Selection Rate: \", sr_true.overall)\n",
    "print(\"Group Selection Rate : \", sr_true.by_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we can see that there is a **5% discrepancy** in the selection rate for Rural vs Urban students (with Urban students being preferred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Biased Model on Biased Data\n",
    "\n",
    "gm_bias = MetricFrame(accuracy_score, y_bias_true, y_pred_bias, sensitive_features = df_sens)\n",
    "print(\"Overall Accuracy: \", gm_bias.overall)\n",
    "print(\"Group Accuracy : \", gm_bias.by_group)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "sr_bias = MetricFrame(selection_rate, y_bias_true, y_pred_bias, sensitive_features = df_sens)\n",
    "print(\"Overall Selection Rate: \", sr_bias.overall)\n",
    "print(\"Group Selection Rate : \", sr_bias.by_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we can see that there is now a **7% discrepancy due to under-sampling** in the selection rate for Rural vs Urban students (with Urban students being preferred) on biased data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Biased Model on Ground Truth Data\n",
    "\n",
    "gm_bias_on_true = MetricFrame(accuracy_score, y_true,\n",
    "                           y_pred_bias_on_true, sensitive_features = sens_attrs[1])\n",
    "print(\"Overall Accuracy: \", gm_bias_on_true.overall)\n",
    "print(\"Group Accuracy : \", gm_bias_on_true.by_group)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "sr_bias_on_true = MetricFrame(selection_rate, y_true,\n",
    "                              y_pred_bias_on_true, sensitive_features = sens_attrs[1])\n",
    "print(\"Overall Selection Rate: \", sr_bias_on_true.overall)\n",
    "print(\"Group Selection Rate : \", sr_bias_on_true.by_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we see that the biased model has lower overall and subgroup accuracy on the ground truth data. \n",
    "Also, we can observe that the selection rate disparity increased ever so slightly with the biased model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T21:16:54.784535Z",
     "start_time": "2021-06-10T21:16:54.725029Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Ground Truth Model on Ground Truth Data\")\n",
    "\n",
    "print(\"EOD:\", equalized_odds_difference(y_true=y_true, y_pred = y_pred_truth, sensitive_features=sens_attrs[1]))\n",
    "print(\"DPD:\", demographic_parity_difference(y_true=y_true, y_pred = y_pred_truth, sensitive_features=sens_attrs[1]))\n",
    "\n",
    "print(\"EOR:\", equalized_odds_ratio(y_true=y_true, y_pred = y_pred_truth, sensitive_features=sens_attrs[1]))\n",
    "print(\"DPR:\", demographic_parity_ratio(y_true=y_true, y_pred = y_pred_truth, sensitive_features=sens_attrs[1]))\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Biased Model on Ground Truth Data\")\n",
    "\n",
    "print(\"EOD:\", equalized_odds_difference(y_true=y_true, y_pred = y_pred_bias_on_true, sensitive_features=sens_attrs[1]))\n",
    "print(\"DPD:\", demographic_parity_difference(y_true=y_true, y_pred = y_pred_bias_on_true, sensitive_features=sens_attrs[1]))\n",
    "\n",
    "print(\"EOR:\", equalized_odds_ratio(y_true=y_true, y_pred = y_pred_bias_on_true, sensitive_features=sens_attrs[1]))\n",
    "print(\"DPR:\", demographic_parity_ratio(y_true=y_true, y_pred = y_pred_bias_on_true, sensitive_features=sens_attrs[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fairness Visualization (Keep this?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-09T17:51:41.613558Z",
     "start_time": "2021-06-09T17:51:41.586931Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from fairlearn.widget import FairlearnDashboard\n",
    "FairlearnDashboard(sensitive_features = df_por['sex'],\n",
    "                   sensitive_feature_names = ['sex'],\n",
    "                   y_true = y_true,\n",
    "                   y_pred = {\"initial model\" : y_pred_truth})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-09T17:51:41.635383Z",
     "start_time": "2021-06-09T17:51:41.615653Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from fairlearn.widget import FairlearnDashboard\n",
    "FairlearnDashboard(sensitive_features = df_por['address'],\n",
    "                   sensitive_feature_names = ['address'],\n",
    "                   y_true = y_true,\n",
    "                   y_pred = {\"initial model\" : y_pred_bias_on_true})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fairness Intervention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairlearn.reductions import ExponentiatedGradient, DemographicParity\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-09T17:51:42.957037Z",
     "start_time": "2021-06-09T17:51:41.643906Z"
    }
   },
   "outputs": [],
   "source": [
    "constraint = DemographicParity()\n",
    "mitigator_true = ExponentiatedGradient(classifier_true, constraint)\n",
    "mitigator_true.fit(X_true, y_true, sensitive_features = sens_attrs[1])\n",
    "y_pred_mitigated_true = mitigator_true.predict(X_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constraint = DemographicParity()\n",
    "mitigator_bias = ExponentiatedGradient(classifier_bias, constraint)\n",
    "mitigator_bias.fit(X_bias_true, y_bias_true, sensitive_features = df_sens)\n",
    "y_pred_mitigated_bias = mitigator_bias.predict(X_bias_true)\n",
    "y_pred_mitigated_bias_on_true = mitigator_bias.predict(X_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: create data structure of different ml performance metrics\n",
    "#       get user input and modularize output shown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy of Ground Truth Model + Fairness Intervention on Ground Truth Data: \",\n",
    "      accuracy_score(y_pred_mitigated_true, y_true))\n",
    "\n",
    "print(\"Accuracy of Biased Model + Fairness Intervention on Ground Truth Data: \",\n",
    "      accuracy_score(y_pred_mitigated_bias_on_true, y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-09T17:51:42.986098Z",
     "start_time": "2021-06-09T17:51:42.973042Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ground Truth Model + Fairness Intervention on Ground Truth Data\n",
    "\n",
    "gm_mitigated = MetricFrame(accuracy_score, y_true, y_pred_mitigated_true, sensitive_features = sens_attrs[1])\n",
    "print(\"Overall Accuracy: \", gm_mitigated.overall)\n",
    "print(\"Group Accuracy : \", gm_mitigated.by_group)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "sr_mitigated = MetricFrame(selection_rate, y_true, y_pred_mitigated_true, sensitive_features = sens_attrs[1])\n",
    "print(\"Overall Selection Rate: \", sr_mitigated.overall)\n",
    "print(\"Group Selection Rate : \", sr_mitigated.by_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Biased Model + Fairness Intervention on Ground Truth Data\n",
    "\n",
    "gm_mitigated_bias_on_true = MetricFrame(accuracy_score, y_true, y_pred_mitigated_bias_on_true, sensitive_features = sens_attrs[1])\n",
    "print(\"Overall Accuracy: \", gm_mitigated_bias_on_true.overall)\n",
    "print(\"Group Accuracy : \", gm_mitigated_bias_on_true.by_group)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "sr_mitigated_bias_on_true = MetricFrame(selection_rate, y_true, y_pred_mitigated_bias_on_true, sensitive_features = sens_attrs[1])\n",
    "print(\"Overall Selection Rate: \", sr_mitigated_bias_on_true.overall)\n",
    "print(\"Group Selection Rate : \", sr_mitigated_bias_on_true.by_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, we observe an increase in overall and subgroup accuracy, but a decrease in disadvantaged subgroup selection rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-09T17:51:43.021839Z",
     "start_time": "2021-06-09T17:51:42.998287Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "FairlearnDashboard(sensitive_features = sens_attrs[1],\n",
    "                   sensitive_feature_names = ['address'],\n",
    "                   y_true = y_true,\n",
    "                   y_pred = {\"initial model\" : y_pred_truth, \"mitigated model\": y_pred_mitigated_true})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias vs Accuracy vs Fairness Trade-Off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if verbose, shows \"Finished iteration: ... \"\n",
    "# if apply_fairness, uses fairness intervention\n",
    "def tradeoff_visualization(classifier, apply_fairness = False, verbose = False):\n",
    "    \n",
    "    bias_amts = list(range(0,200,10))\n",
    "    accuracy_on_true = []\n",
    "    accuracy_on_biased = []\n",
    "    eod_on_true = []\n",
    "    eod_on_biased = []\n",
    "    dataset_size_true = np.full(shape=len(bias_amts), fill_value= X_true.shape[0]).tolist()\n",
    "    dataset_size_bias = []\n",
    "    table = []\n",
    "\n",
    "    classifier_true = classifier.fit(X_true, y_true)\n",
    "    y_pred_truth = classifier_true.predict(X_true)\n",
    "\n",
    "    df_undersampled = df_unfavored.sample(n=len(df_unfavored), random_state=42)\n",
    "\n",
    "    for i in range(20):\n",
    "        # under-sampling process\n",
    "        if i == 0:\n",
    "            df_undersampled = df_undersampled.sample(n=len(df_undersampled), random_state=42)\n",
    "        else:\n",
    "            df_undersampled = df_undersampled.sample(n=len(df_undersampled)-10, random_state=42)\n",
    "\n",
    "        # combine undersampled and original favored class to create dataset\n",
    "        df_concat = pd.concat([df_favored,df_undersampled])\n",
    "        df_concat.shape\n",
    "        df_sens = df_concat['address']\n",
    "\n",
    "        # format data\n",
    "        X_bias = df_concat.iloc[:, :-2].values\n",
    "        y_bias = df_concat.iloc[:, -1].values\n",
    "\n",
    "        # OHE\n",
    "        ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), cat_cols)], remainder='passthrough')\n",
    "        X_bias_true = np.array(ct.fit_transform(X_bias))\n",
    "        y_bias_true = df_concat['pass']\n",
    "\n",
    "        dataset_size_bias.append(X_bias_true.shape[0])\n",
    "        classifier_bias = classifier.fit(X_bias_true, y_bias_true)\n",
    "        \n",
    "        if apply_fairness:\n",
    "            constraint = DemographicParity()\n",
    "            mitigator_bias = ExponentiatedGradient(classifier_bias, constraint)\n",
    "            mitigator_bias.fit(X_bias_true, y_bias_true, sensitive_features = df_sens)\n",
    "            y_pred_bias = classifier_bias.predict(X_bias_true)\n",
    "            y_pred_bias_on_true = mitigator_bias.predict(X_true)\n",
    "        \n",
    "        else:\n",
    "            y_pred_bias = classifier_bias.predict(X_bias_true)\n",
    "            y_pred_bias_on_true = classifier_bias.predict(X_true)\n",
    "\n",
    "        # model performance\n",
    "        acc_bias = accuracy_score(y_pred=y_pred_bias, y_true=y_bias_true)\n",
    "        accuracy_on_biased.append(acc_bias)\n",
    "\n",
    "        acc_bias_on_true = accuracy_score(y_pred=y_pred_bias_on_true, y_true=y_true)\n",
    "        accuracy_on_true.append(acc_bias_on_true)\n",
    "\n",
    "        # fairness performance\n",
    "        eod_true = equalized_odds_difference(y_true=y_bias_true, y_pred = y_pred_bias, sensitive_features=df_sens)\n",
    "        eod_on_true.append(eod_true)\n",
    "\n",
    "        eod_bias_on_true = equalized_odds_difference(y_true=y_true, y_pred = y_pred_bias_on_true, sensitive_features=sens_attrs[1])\n",
    "        eod_on_biased.append(eod_bias_on_true)\n",
    "\n",
    "        # table visualization \n",
    "        table_elem = [i*10, acc_bias, acc_bias_on_true]\n",
    "        table.append(table_elem)\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"Finished Iteration: \", len(df_concat))\n",
    "\n",
    "    return bias_amts, dataset_size_true, dataset_size_bias, accuracy_on_biased, accuracy_on_true, eod_on_biased, eod_on_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualizations(bias_amts, dataset_size_true, dataset_size_bias, accuracy_on_biased, accuracy_on_true, eod_on_biased, eod_on_true, fairness = False):\n",
    "    \n",
    "    if not fairness:\n",
    "        plt.figure(figsize=(17,7))\n",
    "\n",
    "        plt.subplot(1,2,1)\n",
    "        plt.plot(bias_amts, accuracy_on_true, label = 'Ground Truth')\n",
    "        plt.plot(bias_amts, accuracy_on_biased, label = 'Biased Data')\n",
    "        plt.xlabel(\"Amount of Bias (number of minority samples removed)\")\n",
    "        plt.ylabel(\"Accuracy Score\")\n",
    "        plt.axhline(y=accuracy_score(y_pred_truth, y_true), color = \"green\", label = \"Ground Truth Model Accuracy\", alpha = 0.5)\n",
    "        plt.title(\"Biased Model Accuracy\")\n",
    "        plt.ylim(0.92, 0.99)\n",
    "        plt.legend()\n",
    "\n",
    "        plt.subplot(1,2,2)\n",
    "        plt.plot(bias_amts, dataset_size_true, label = 'Ground Truth')\n",
    "        plt.plot(bias_amts, dataset_size_bias, label = 'Biased Data')\n",
    "        plt.xlabel(\"Amount of Bias (number of minority samples removed)\")\n",
    "        plt.ylabel(\"Dataset Size\")\n",
    "        plt.legend()\n",
    "\n",
    "        plt.show()\n",
    "    else:\n",
    "        \n",
    "        plt.plot(bias_amts, eod_on_true, label = 'Ground Truth')\n",
    "        plt.plot(bias_amts, eod_on_biased, label = 'Biased Data')\n",
    "        plt.xlabel(\"Amount of Bias (number of minority samples removed)\")\n",
    "        plt.ylabel(\"Equalized Odds Difference\")\n",
    "        plt.axhline(y=equalized_odds_difference(y_true=y_true, y_pred = y_pred_truth, sensitive_features=sens_attrs[1]), color = \"green\",\n",
    "                    label = \"Ground Truth EOD\", alpha = 0.5)\n",
    "        plt.legend()\n",
    "        plt.title(\"Biased Model Equalized Odds Difference\")\n",
    "        plt.show()\n",
    "        #plt.savefig(\"bias1.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_amts, dataset_size_true, dataset_size_bias, accuracy_on_biased, accuracy_on_true, eod_on_biased, eod_on_true = tradeoff_visualization(classifier, False, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizations(bias_amts, dataset_size_true, dataset_size_bias, accuracy_on_biased, accuracy_on_true, eod_on_biased, eod_on_true, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizations(bias_amts, dataset_size_true, dataset_size_bias, accuracy_on_biased, accuracy_on_true, eod_on_biased, eod_on_true, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_amts, dataset_size_true, dataset_size_bias, accuracy_on_biased, accuracy_on_true, eod_on_biased, eod_on_true = tradeoff_visualization(classifier, True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizations(bias_amts, dataset_size_true, dataset_size_bias, accuracy_on_biased, accuracy_on_true, eod_on_biased, eod_on_true, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizations(bias_amts, dataset_size_true, dataset_size_bias, accuracy_on_biased, accuracy_on_true, eod_on_biased, eod_on_true, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Special Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_amts = list(range(0,200,10))\n",
    "accuracy_on_true = []\n",
    "accuracy_on_biased = []\n",
    "accuracy_on_true_mitigated = []\n",
    "eod_on_true = []\n",
    "eod_on_biased = []\n",
    "eod_on_true_mitigated = []\n",
    "dataset_size_true = np.full(shape=len(bias_amts), fill_value= X_true.shape[0]).tolist()\n",
    "dataset_size_bias = []\n",
    "table = []\n",
    "\n",
    "classifier_true = classifier.fit(X_true, y_true)\n",
    "y_pred_truth = classifier_true.predict(X_true)\n",
    "\n",
    "df_undersampled = df_unfavored.sample(n=len(df_unfavored), random_state=42)\n",
    "\n",
    "for i in range(20):\n",
    "    # under-sampling process\n",
    "    if i == 0:\n",
    "        df_undersampled = df_undersampled.sample(n=len(df_undersampled), random_state=42)\n",
    "    else:\n",
    "        df_undersampled = df_undersampled.sample(n=len(df_undersampled)-10, random_state=42)\n",
    "\n",
    "    # combine undersampled and original favored class to create dataset\n",
    "    df_concat = pd.concat([df_favored,df_undersampled])\n",
    "    df_concat.shape\n",
    "    df_sens = df_concat['address']\n",
    "\n",
    "    # format data\n",
    "    X_bias = df_concat.iloc[:, :-2].values\n",
    "    y_bias = df_concat.iloc[:, -1].values\n",
    "\n",
    "    # OHE\n",
    "    ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), cat_cols)], remainder='passthrough')\n",
    "    X_bias_true = np.array(ct.fit_transform(X_bias))\n",
    "    y_bias_true = df_concat['pass']\n",
    "\n",
    "    dataset_size_bias.append(X_bias_true.shape[0])\n",
    "    classifier_bias = classifier.fit(X_bias_true, y_bias_true)\n",
    "\n",
    "    constraint = DemographicParity()\n",
    "    mitigator_bias = ExponentiatedGradient(classifier_bias, constraint)\n",
    "    mitigator_bias.fit(X_bias_true, y_bias_true, sensitive_features = df_sens)\n",
    "    y_pred_mitigated_bias = classifier_bias.predict(X_bias_true)\n",
    "    y_pred_mitigated_bias_on_true = mitigator_bias.predict(X_true)\n",
    "\n",
    "    y_pred_bias = classifier_bias.predict(X_bias_true)\n",
    "    y_pred_bias_on_true = classifier_bias.predict(X_true)\n",
    "\n",
    "    # model performance\n",
    "    acc_bias = accuracy_score(y_pred=y_pred_bias, y_true=y_bias_true)\n",
    "    accuracy_on_biased.append(acc_bias)\n",
    "\n",
    "    acc_bias_on_true = accuracy_score(y_pred=y_pred_bias_on_true, y_true=y_true)\n",
    "    accuracy_on_true.append(acc_bias_on_true)\n",
    "    \n",
    "    acc_bias_mitigated_on_true = accuracy_score(y_pred=y_pred_mitigated_bias_on_true, y_true=y_true)\n",
    "    accuracy_on_true_mitigated.append(acc_bias_mitigated_on_true)\n",
    "\n",
    "    # fairness performance\n",
    "    eod_true = equalized_odds_difference(y_true=y_bias_true, y_pred = y_pred_bias, sensitive_features=df_sens)\n",
    "    eod_on_true.append(eod_true)\n",
    "\n",
    "    eod_bias_on_true = equalized_odds_difference(y_true=y_true, y_pred = y_pred_bias_on_true, sensitive_features=sens_attrs[1])\n",
    "    eod_on_biased.append(eod_bias_on_true)\n",
    "    \n",
    "    eod_bias_true_mitigated = equalized_odds_difference(y_true=y_true, y_pred = y_pred_mitigated_bias_on_true, sensitive_features=sens_attrs[1])\n",
    "    eod_on_true_mitigated.append(eod_bias_true_mitigated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(17,7))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(bias_amts, accuracy_on_biased, label = 'On Biased Data + No Fairness Intervention', color = \"red\")\n",
    "plt.plot(bias_amts, accuracy_on_true, label = 'On Ground Truth + No Fairness Intervention', color = \"blue\")\n",
    "plt.plot(bias_amts, accuracy_on_true_mitigated, label = 'On Ground Truth + Fairness Intervention', color = \"purple\")\n",
    "plt.xlabel(\"Amount of Bias (number of minority samples removed)\")\n",
    "plt.ylabel(\"Accuracy Score\")\n",
    "plt.axhline(y=accuracy_score(y_pred_truth, y_true), color = \"green\", label = \"Ground Truth Model On Ground Truth Data\", alpha = 0.5)\n",
    "plt.title(\"Accuracy of Biased Model (trained on biased data) \\n\\nNote: ground truth model trained on ground truth data means \\na barebones DT classifier trained on unbiased data\")\n",
    "plt.ylim(0.92, 0.99)\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(bias_amts, dataset_size_true, label = 'Ground Truth')\n",
    "plt.plot(bias_amts, dataset_size_bias, label = 'Biased Data')\n",
    "plt.xlabel(\"Amount of Bias (number of minority samples removed)\")\n",
    "plt.ylabel(\"Dataset Size\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(bias_amts, eod_on_biased, label = 'On Biased Data + No Fairness Intervention', color = \"red\")\n",
    "plt.plot(bias_amts, eod_on_true, label = 'On Ground Truth + No Fairness Intervention', color = \"blue\")\n",
    "plt.plot(bias_amts, eod_on_true_mitigated, label = 'On Ground Truth + Fairness Intervention', color = \"purple\")\n",
    "plt.xlabel(\"Amount of Bias (number of minority samples removed)\")\n",
    "plt.ylabel(\"Equalized Odds Difference\")\n",
    "plt.axhline(y=equalized_odds_difference(y_true=y_true, y_pred = y_pred_truth, sensitive_features=sens_attrs[1]), color = \"green\",\n",
    "            label = \"Ground Truth EOD\", alpha = 0.5)\n",
    "plt.legend()\n",
    "plt.title(\"Biased Model Equalized Odds Difference\")\n",
    "plt.show()\n",
    "#plt.savefig(\"bias1.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
