{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5e91243",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-08T20:41:26.879421Z",
     "start_time": "2021-06-08T20:41:26.876140Z"
    }
   },
   "source": [
    "# Fairness Sandox"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a42ced1",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "Please run the code block below to install the necessary packages (if needed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3234f6d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# uncomment as needed\n",
    "\n",
    "# %pip install aif360\n",
    "# %pip install fairlearn\n",
    "# %pip install imbalanced-learn\n",
    "# %pip install matplotlib\n",
    "# %pip install numpy\n",
    "# %pip install pandas\n",
    "# %pip install seaborn\n",
    "# %pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a692724",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T20:42:02.586362Z",
     "start_time": "2021-06-10T20:42:02.581026Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, roc_curve, auc\n",
    "from numpy import percentile\n",
    "\n",
    "import fairlearn\n",
    "from fairlearn.metrics import *\n",
    "from fairlearn.reductions import *\n",
    "import aif360\n",
    "\n",
    "import copy, random\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da59e303",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a51666",
   "metadata": {},
   "source": [
    "### Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "996906d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T20:42:06.040700Z",
     "start_time": "2021-06-10T20:42:06.035368Z"
    }
   },
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, short_name = '', path = '', cat_cols = [], num_cols = [],\n",
    "                 sens_attr = '', has_sens_attr = True,\n",
    "                 sep = '', synthetic = False):\n",
    "        self.short_name = short_name\n",
    "        self.path = path\n",
    "        self.cat_cols = cat_cols\n",
    "        self.num_cols = num_cols\n",
    "        self.has_sens_attr = has_sens_attr\n",
    "        if has_sens_attr:\n",
    "            self.sens_attr = sens_attr\n",
    "        if not synthetic:\n",
    "            self.df = pd.read_csv(path, sep = sep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21b5fd8a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T20:42:06.330948Z",
     "start_time": "2021-06-10T20:42:06.325325Z"
    }
   },
   "outputs": [],
   "source": [
    "# collection of datasets is a dictionary where keys = short name, values = Dataset object\n",
    "\n",
    "datasets = dict()\n",
    "\n",
    "def add_dataset(dataset):\n",
    "    if not isinstance(dataset, Dataset):\n",
    "        raise TypeError(\"Please enter a valid Dataset object\")\n",
    "    else:\n",
    "        if dataset.short_name not in datasets.keys():\n",
    "            datasets[dataset.short_name] = dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f38f1ae",
   "metadata": {},
   "source": [
    "### Popular Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e570253c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T20:42:06.534123Z",
     "start_time": "2021-06-10T20:42:06.464059Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# example - adding a dataset\n",
    "path_adult_income = 'Datasets/adult.csv'\n",
    "cat_cols_adult = ['workclass', 'education','marital-status', 'occupation', 'relationship', 'race',\n",
    "            'gender', 'native-country','income']\n",
    "num_cols_adult = ['age', 'fnlwgt', 'educational-num', 'capital-gain', 'capital-loss', 'hours-per-week']\n",
    "adult_income = Dataset('adult_income', path_adult_income, cat_cols_adult, num_cols_adult, sep = \",\", sens_attr = 'race')\n",
    "\n",
    "add_dataset(adult_income)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38eddaac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T20:42:06.621748Z",
     "start_time": "2021-06-10T20:42:06.598777Z"
    }
   },
   "outputs": [],
   "source": [
    "cat_cols_por = ['school', 'sex', 'address','famsize','Pstatus','Mjob','Fjob','reason',\n",
    "       'guardian','schoolsup','famsup','paid', 'activities','nursery','higher', 'internet','romantic']\n",
    "num_cols_por = ['age', 'Medu', 'Fedu','traveltime','studytime','failures', 'famrel',\n",
    "       'freetime','goout','Dalc','Walc','health','absences','G1', 'G2', 'G3']\n",
    "\n",
    "add_dataset(Dataset(\"student_por\", path='Datasets/student-por.csv',\n",
    "                    cat_cols=cat_cols_por, num_cols=num_cols_por, sep = \";\", sens_attr = 'sex'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d15bc1-8e18-40cb-8f71-89006d1a9fbb",
   "metadata": {},
   "source": [
    "### Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f6bd2ff-f213-4722-8eee-5554f624b627",
   "metadata": {},
   "outputs": [],
   "source": [
    "from synthetic_data import get_synthetic_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b818c2fd-4b7e-45b3-8bb7-f0a7370e64f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Parameters:\n",
    "\n",
    "    n is the total number of examples in the dataset\n",
    "\n",
    "    num_numerical_feats is number of numerical features\n",
    "        each numerical feature is drawn from a\n",
    "        multivariate normal distribution\n",
    "\n",
    "    means = mean for each numerical features, default is 0 mean\n",
    "        NOTE: len(means) == num_numerical_feats\n",
    "\n",
    "    cov_matrix = covariance matrix for numerical features, \n",
    "        default is identity matrix\n",
    "        NOTE: cov_matrix.shape == np.identity(num_numerical_feats).shape\n",
    "\n",
    "    num_cat_feats is number of categorical features\n",
    "\n",
    "    cat_levels is an array where each element is the number\n",
    "        of levels for each categorical feature\n",
    "        len(cat_levels) = num_cat_feats\n",
    "\n",
    "    r is the proportion of examples in the minority group\n",
    "        (1-r) is proportion of examples in majority group\n",
    "\n",
    "    label_noise is in [0,1]\n",
    "\n",
    "    diff_dist is true if minority and majority have different\n",
    "        underlying sampling distributions\n",
    "\n",
    "    show_vis displays the distribution of outcomes\n",
    "\n",
    "'''\n",
    "\n",
    "df_synthetic = get_synthetic_data(n=1000, r = 0.25, num_numerical_feats=3, num_cat_feats=2, \n",
    "                                  cat_levels=[2,3], diff_dist=True, label_noise = 0.1)\n",
    "\n",
    "# add to dictionary of datasets\n",
    "path_synthetic = 'Datasets/synthetic_data.csv'\n",
    "df_synthetic.to_csv(path_synthetic)\n",
    "add_dataset(Dataset('synthetic', path_synthetic, cat_cols=[], num_cols=[], synthetic=True, sens_attr = \"sens_feat\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba55ee6",
   "metadata": {},
   "source": [
    "# EDA + Data Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad765b98",
   "metadata": {},
   "source": [
    "Feel free to use any/all of the following EDA functions and/or add your own!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b887484",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T20:42:07.151553Z",
     "start_time": "2021-06-10T20:42:07.125905Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num1</th>\n",
       "      <th>num2</th>\n",
       "      <th>num3</th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>sens_feat</th>\n",
       "      <th>outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.184999</td>\n",
       "      <td>0.827643</td>\n",
       "      <td>-0.528369</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.252946</td>\n",
       "      <td>-0.303078</td>\n",
       "      <td>-0.042762</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.205863</td>\n",
       "      <td>0.210320</td>\n",
       "      <td>0.270890</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-2.100500</td>\n",
       "      <td>0.102263</td>\n",
       "      <td>-0.099867</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.144574</td>\n",
       "      <td>-2.067131</td>\n",
       "      <td>-0.297975</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       num1      num2      num3  cat1  cat2  sens_feat  outcome\n",
       "0 -0.184999  0.827643 -0.528369   1.0   0.0        1.0      0.0\n",
       "1  1.252946 -0.303078 -0.042762   1.0   1.0        1.0      0.0\n",
       "2 -0.205863  0.210320  0.270890   0.0   1.0        0.0      1.0\n",
       "3 -2.100500  0.102263 -0.099867   1.0   0.0        1.0      0.0\n",
       "4 -1.144574 -2.067131 -0.297975   1.0   0.0        1.0      0.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take a peek at the first few data points\n",
    "df_synthetic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5f3bce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T20:42:07.426895Z",
     "start_time": "2021-06-10T20:42:07.269718Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_counts(df, attr):\n",
    "    if attr in df.columns:\n",
    "        df[attr].value_counts(normalize=True).plot.barh()\n",
    "    else:\n",
    "        print(\"Error! Please enter a valid feature.\")\n",
    "\n",
    "# example\n",
    "plot_counts(df_synthetic, 'sens_feat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243c0f8c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T20:42:08.016753Z",
     "start_time": "2021-06-10T20:42:07.428807Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def group_by_plot(df, attr1, attr2):\n",
    "    for val in list(df[attr1].unique()):\n",
    "        print(val)\n",
    "        temp = df[df[attr1] == val]\n",
    "        sns.displot(temp[attr2])\n",
    "\n",
    "# example\n",
    "group_by_plot(df_synthetic, 'sens_feat', 'outcome')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a7fd87-924b-41b0-a505-25cac190e8cd",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360cd4bf-693e-4ebb-a1c4-4d5051dea153",
   "metadata": {},
   "source": [
    "### Train/Test Split\n",
    "\n",
    "NOTES: \n",
    "1. Whether you input your own data, choose one of our provided datasets, or generate synthetic data, we will consider this to be the UNBIASED GROUND TRUTH.\n",
    "2. Then, we will split the data, with the first part being the data which we will inject bias into and the second part being the unbiased ground truth testing data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3140d33-1003-43b3-baad-49203e16f68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    train_ratio: is the proportion of data examples in the training set\n",
    "        (1-train_ratio is proportion in unbiased testing set)\n",
    "'''\n",
    "def train_test_split(df, train_ratio = 0.5):\n",
    "    \n",
    "    df_train = df.loc[range(0,int(len(df_synthetic)*train_ratio)), :]\n",
    "    df_test = df_synthetic.loc[range(int(len(df_synthetic)*train_ratio)+1, len(df_synthetic)), :]\n",
    "    \n",
    "    return df_train, df_test\n",
    "\n",
    "df_train, df_test = train_test_split(df_synthetic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d385bc3-625f-4726-a2b3-7dac659fda04",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8977856-1125-4a45-bd5d-eac6cb5158b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "This function separates the minority and majority classes\n",
    "\n",
    "Parameters:\n",
    "    \n",
    "    sens_attr: sensitive attribute\n",
    "    maj_val: value of sens_attr which indicates majority class\n",
    "    min_val: value of sens_attr which indicates minority class\n",
    "\n",
    "'''\n",
    "def get_maj_min(df, sens_attr, maj_val, min_val):\n",
    "    assert sens_attr in list(df.columns), \"Sensitive attribute must be a column in the dataframe!\"\n",
    "    df_majority = df_train[df_train[sens_attr] == maj_val]\n",
    "    df_minority = df_train[df_train[sens_attr] == min_val]\n",
    "    \n",
    "    return df_majority, df_minority\n",
    "\n",
    "df_majority, df_minority = get_maj_min(df_train, 'sens_feat', 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773aa0d5-dcda-4c49-93f1-8d8d116972c9",
   "metadata": {},
   "source": [
    "#### One-Hot Encoding (ONLY for non-synthetic data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13da5c4e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T20:42:08.750322Z",
     "start_time": "2021-06-10T20:42:08.728346Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# OHE categorical features (prompt for user's choice here?)\n",
    "\n",
    "# get indices of categorical columns\n",
    "def get_cat_cols(dataset):\n",
    "    df = dataset.df\n",
    "    res = []\n",
    "    for col in dataset.cat_cols:\n",
    "        res.append(df.columns.get_loc(col))\n",
    "    return res\n",
    "\n",
    "# df_train = pd.get_dummies(df_train, get_cat_cols(datasets['short_name']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dab8c31-4082-4019-8a71-7042bd83b755",
   "metadata": {},
   "source": [
    "#### Formatting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547edc7c-4c9b-4f47-8a61-9398fc20f79f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train = df_train.iloc[:, :-1].values\n",
    "y_train = df_train.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec781149-5558-4461-94d2-fc6db7f35513",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_true = df_test.iloc[:, :-1].values\n",
    "y_true = df_test.iloc[:, -1].values\n",
    "\n",
    "sens_attrs_true = [df_test[datasets['synthetic'].sens_attr]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950918b3",
   "metadata": {},
   "source": [
    "# Bias Injection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae1aacf-4096-440e-ab87-c83cd1b1ffef",
   "metadata": {},
   "source": [
    "### List of Biases\n",
    "1. Representation\n",
    "2. Measurement\n",
    "3. Omitted Variable\n",
    "4. Label Noise\n",
    "5. Over-Sampling (of Majority)\n",
    "6. Under-Sampling (of Minority)\n",
    "\n",
    "NOTE: if data is NOT synthetic, print out the dataframe to check the dummy variables, e.g. if you want to apply a bias to a categorical feature cat_feat with value x, then use `df[cat_feat_x] == 1`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700bcab2-ee1d-49de-928a-fdbd1992514e",
   "metadata": {},
   "source": [
    "#### Representation Bias\n",
    "\n",
    "Under-sample an attribute conditioned on subgroups for that feature and/or subgroups for other feature(s)\n",
    "\n",
    "Note: you will need to input $\\beta$, which is the probability of deleting an example from the desired group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a963c8dd-4369-4e3b-90d2-8c9db586e6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from biases import representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393b7547-56fc-45eb-9042-9e9406d55bd8",
   "metadata": {},
   "source": [
    "#### Measurement Bias\n",
    "\n",
    "Add noise to an attribute, either entirely or on various subgroups\n",
    "\n",
    "Refer to [numpy.random documentation](https://numpy.org/doc/1.16/reference/routines.random.html) for types of sampling distributions for the noise.\n",
    "\n",
    "Helper Functions:\n",
    "1. Get unique values for categorical features\n",
    "2. Get 5 number summary for numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36291c8d-d273-4b1b-9aeb-a3f66bcc1077",
   "metadata": {},
   "outputs": [],
   "source": [
    "from biases import measurement, get_unique_cat, get_summary_num"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12aa7c2-9c3a-4c9e-89f1-c36e245ab75f",
   "metadata": {},
   "source": [
    "#### Omitted Variable\n",
    "\n",
    "Note: if you choose to remove the sensitive feature, you will no longer be able to impose a fairness intervention! Resulting comparisons will simply be between regular ml models trained with and without the sensitive attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229ce87e-721f-44b1-92e7-b65d5a4e90f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from biases import omitted_variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076c7ee5-3a92-4cde-aaed-71d762206461",
   "metadata": {},
   "source": [
    "#### Label Noise Bias\n",
    "\n",
    "add noise to labels for a specific subset of the data (conditioned on another feature or subgroup of another feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ef1c5f-5518-4b33-94fe-343a2855de0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from biases import label_noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f094e8c-ea43-4b4f-a14f-8de61c1d42e9",
   "metadata": {},
   "source": [
    "#### Over-Sampling Majority Class\n",
    "\n",
    "Note: you can either choose to randomly over-sample existing examples or generate new samples by interpolation using SMOTE and ADASYN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81862d1-20ce-4d9a-9a7b-a330a0023192",
   "metadata": {},
   "outputs": [],
   "source": [
    "from biases import random_over_sampling, over_sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a35ca10-2100-4c43-94b6-732f3c581244",
   "metadata": {},
   "source": [
    "#### Under-Sampling Minority Class\n",
    "\n",
    "Note 1: you will need to input $\\beta$, which is the probability of deleting an example from the minority class. For example, if $\\beta = 0.25$ then each example in the training data will be deleted with probability $0.25$, which will result in approximately $25\\%$ of the total minority class examples being deleted.\n",
    "\n",
    "Note 2: this method is equivalent to using representation bias on the minority"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6f07f3-842c-4df1-8cfa-accb806a5c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from biases import under_sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c4d2e3",
   "metadata": {},
   "source": [
    "### Bias Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b65ae5e-949a-49a4-ae9c-4b79dd62c99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "biases = dict()\n",
    "\n",
    "def add_bias(bias_func, short_name):\n",
    "    biases[short_name] = bias_func\n",
    "\n",
    "# example usage\n",
    "add_bias(under_sampling, 'under_sampling')\n",
    "add_bias(omitted_variable, 'omitted_variable')\n",
    "add_bias(random_over_sampling, 'random_over_sampling')\n",
    "add_bias(over_sampling, 'over_sampling')\n",
    "add_bias(label_noise, 'label_noise')\n",
    "add_bias(measurement, 'measurement')\n",
    "add_bias(representation, 'representation')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd676d70",
   "metadata": {},
   "source": [
    "### Bias Injection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d39470a-30e9-4690-b225-77fefad47b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_bias = biases['under_sampling'](df_train, 0.5, 'sens_feat', 1, 0)\n",
    "#df_bias = biases['omitted_variable'](datasets, df_train, 'synthetic', 'num1', is_sens_attr=False)\n",
    "#df_bias = biases['random_over_sampling'](df_train, 'sens_feat', 1, 0, 2)\n",
    "#df_bias = biases['over_sampling'](df_train, df_minority, 'sens_feat', 1, 0, 2, type=2)\n",
    "#df_bias = biases['label_noise'](df_train, 'sens_feat', 'categorical', 1, 0.2)\n",
    "#df_bias = biases['measurement'](df_train, 'cat2', 'categorical', noise_prob=1, noise_type=1, subgroups=[2])\n",
    "df_bias = biases['representation'](df_train, (df_train['num1'] > 0) & (df_train['cat1'] == 0), 0.5)\n",
    "\n",
    "# for fairness measures later\n",
    "if datasets['synthetic'].has_sens_attr:\n",
    "    df_sens = df_bias[datasets['synthetic'].sens_attr]\n",
    "\n",
    "# format data\n",
    "X_bias = df_bias.iloc[:, :-1].values\n",
    "y_bias = df_bias.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84403b44",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f6827f",
   "metadata": {},
   "source": [
    "### Model Selection + Training (TODO: modularize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d79618",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T20:42:09.057618Z",
     "start_time": "2021-06-10T20:42:08.942574Z"
    }
   },
   "outputs": [],
   "source": [
    "# modularize and add data struct of different ml techniques\n",
    "\n",
    "classifier = DecisionTreeClassifier(min_samples_leaf = 10, max_depth = 4)\n",
    "\n",
    "classifier_true = classifier.fit(X_true, y_true)\n",
    "y_pred_truth = classifier_true.predict(X_true)\n",
    "\n",
    "classifier_bias = classifier.fit(X_bias, y_bias)\n",
    "y_pred_bias = classifier_bias.predict(X_bias)\n",
    "y_pred_bias_on_true = classifier_bias.predict(X_true)\n",
    "\n",
    "sens_feat_true = df_test['sens_feat']\n",
    "sens_feat_bias = df_sens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff477ccd",
   "metadata": {},
   "source": [
    "### Model Performance (TODO: modularize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf0a2b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T20:42:09.231352Z",
     "start_time": "2021-06-10T20:42:09.225108Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Accuracy of Ground Truth Model on Ground Truth Data: \", accuracy_score(y_pred_truth, y_true))\n",
    "print(\"Accuracy of Biased Model on Biased Data: \", accuracy_score(y_pred_bias, y_bias))\n",
    "print(\"Accuracy of Biased Model on Ground Truth Data: \", accuracy_score(y_pred_bias_on_true, y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d049d3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T21:16:04.515563Z",
     "start_time": "2021-06-10T21:16:04.499948Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ground Truth Model on Ground Truth Data\n",
    "\n",
    "gm_true = MetricFrame(metrics=accuracy_score,y_true=y_true, y_pred=y_pred_truth, sensitive_features = sens_feat_true)\n",
    "print(\"Overall Accuracy: \", gm_true.overall)\n",
    "print(\"Group Accuracy : \", gm_true.by_group)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "sr_true = MetricFrame(metrics=selection_rate, y_true=y_true, y_pred=y_pred_truth, sensitive_features = sens_feat_true)\n",
    "print(\"Overall Selection Rate: \", sr_true.overall)\n",
    "print(\"Group Selection Rate : \", sr_true.by_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c85b935",
   "metadata": {},
   "source": [
    "# Fairness Intervention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba33585-22db-4fcd-bea8-ca60828b9f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "\n",
    "from fairlearn.preprocessing import CorrelationRemover\n",
    "\n",
    "remover = CorrelationRemover(sensitive_feature_ids=[5])\n",
    "remover_df = CorrelationRemover(sensitive_feature_ids=['sens_feat'])\n",
    "\n",
    "X_train_corr = remover.fit_transform(X_train)\n",
    "df_corr = pd.DataFrame(remover_df.fit_transform(df_train))\n",
    "\n",
    "df_temp = df_train.drop('sens_feat', axis=1)\n",
    "df_corr.columns = df_temp.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9352481e-0335-44c0-a4a9-288d4d8af531",
   "metadata": {},
   "outputs": [],
   "source": [
    "constraints = [EqualizedOdds(), DemographicParity(), ErrorRateParity(),\n",
    "               FalsePositiveRateParity(), TruePositiveRateParity()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8aec404-870a-4506-9224-4c0d8e07d081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exponentiated Gradient\n",
    "constraint = DemographicParity()\n",
    "mitigator_bias = ExponentiatedGradient(classifier_bias, constraint)\n",
    "mitigator_bias.fit(X_bias, y_bias, sensitive_features = sens_feat_bias)\n",
    "y_pred_mitigated_bias_on_true = mitigator_bias.predict(X_true)\n",
    "\n",
    "print(\"Accuracy of Biased Model + Fairness Intervention on Ground Truth Data: \",\n",
    "      accuracy_score(y_pred_mitigated_bias_on_true, y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166433e1-3fca-4ff3-b5df-fdd0c57b5752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Search\n",
    "constraint = DemographicParity()\n",
    "mitigator_grid_bias = GridSearch(classifier_bias, constraint)\n",
    "mitigator_grid_bias.fit(X_bias, y_bias, sensitive_features = sens_feat_bias)\n",
    "y_pred_mitigated_grid_bias_on_true = mitigator_grid_bias.predict(X_true)\n",
    "\n",
    "print(\"Accuracy of Biased Model + Fairness Intervention on Ground Truth Data: \",\n",
    "      accuracy_score(y_pred_mitigated_grid_bias_on_true, y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516b8835-ccdd-4008-abf9-1985180f41ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# postprocessing\n",
    "\n",
    "from fairlearn.postprocessing import ThresholdOptimizer\n",
    "\n",
    "thresh_optim = ThresholdOptimizer(estimator=classifier_bias, constraints= 'equalized_odds', predict_method='auto')\n",
    "thresh_optim.fit(X_bias, y_bias, sensitive_features=sens_feat_bias)\n",
    "y_pred_thresh_on_true = thresh_optim.predict(X_true, sensitive_features=sens_feat_true)\n",
    "\n",
    "print(\"Accuracy of Biased Model + Fairness Intervention on Ground Truth Data: \",\n",
    "      accuracy_score(y_pred_thresh_on_true, y_true))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7ea20d",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d85690-7232-405f-9256-d987abd01b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy of Biased Model + Fairness Intervention on Ground Truth Data: \",\n",
    "      accuracy_score(y_pred_mitigated_bias_on_true, y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d5b09e-f879-4a03-ba44-9a1a66999204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Biased Model + Fairness Intervention on Ground Truth Data\n",
    "\n",
    "gm_mitigated_bias_on_true = MetricFrame(metrics=accuracy_score, y_true=y_true, y_pred=y_pred_mitigated_bias_on_true, sensitive_features = sens_feat_true)\n",
    "print(\"Overall Accuracy: \", gm_mitigated_bias_on_true.overall)\n",
    "print(\"Group Accuracy : \", gm_mitigated_bias_on_true.by_group)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "sr_mitigated_bias_on_true = MetricFrame(metrics=selection_rate, y_true=y_true, y_pred=y_pred_mitigated_bias_on_true, sensitive_features = sens_feat_true)\n",
    "print(\"Overall Selection Rate: \", sr_mitigated_bias_on_true.overall)\n",
    "print(\"Group Selection Rate : \", sr_mitigated_bias_on_true.by_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e393d0-149b-4da6-91ba-e74e5225b0e2",
   "metadata": {},
   "source": [
    "# Trade-Off Visualization (Standalone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48687919-16ae-446a-8db3-7120e07fc5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if verbose, shows \"Finished iteration: ... \"\n",
    "# if apply_fairness, uses fairness intervention\n",
    "def tradeoff_visualization(bias_amts, classifier, X_true, y_true, \n",
    "                           df_train, sensitive_feature = \"cat\",\n",
    "                           is_synthetic = False,\n",
    "                           apply_fairness = False, verbose = False):\n",
    "    \n",
    "    accuracy_on_true = []\n",
    "    accuracy_on_biased = []\n",
    "    accuracy_on_true_mitigated = []\n",
    "    accuracy_on_biased_mitigated = []\n",
    "    \n",
    "    count = 0\n",
    "\n",
    "    classifier_true = classifier.fit(X_true, y_true)\n",
    "    y_pred_truth = classifier_true.predict(X_true)\n",
    "\n",
    "    for bias in bias_amts:\n",
    "        \n",
    "        df_train_copy = df_train.copy()\n",
    "        \n",
    "        df_bias = biases['over_sampling'](df_train, df_minority, 'sens_feat', 1, 0, 2, type=2)\n",
    "        df_sens = df_bias[sensitive_feature]\n",
    "\n",
    "        # format data\n",
    "        X_bias = df_bias.iloc[:, :-1].values\n",
    "        y_bias = df_bias.iloc[:, -1].values\n",
    "        \n",
    "        if not is_synthetic:\n",
    "            # OHE\n",
    "            ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), cat_cols)], remainder='passthrough')\n",
    "            X_bias_true = np.array(ct.fit_transform(X_bias))\n",
    "        else:\n",
    "            X_bias_true = X_bias\n",
    "        \n",
    "        y_bias_true = df_bias.iloc[:, -1].values\n",
    "        \n",
    "        classifier_bias = classifier.fit(X_bias_true, y_bias_true)\n",
    "        \n",
    "        if apply_fairness:\n",
    "            constraint = DemographicParity()\n",
    "            classifier_mitigated_bias = ExponentiatedGradient(classifier_bias, constraint)\n",
    "            classifier_mitigated_bias.fit(X_bias_true, y_bias_true, sensitive_features = df_sens)\n",
    "            \n",
    "            # testing on biased data WITH fairness intervention\n",
    "            y_pred_mitigated_bias = classifier_mitigated_bias.predict(X_bias_true)\n",
    "            \n",
    "            # testing on GT data WITH fairness intervention\n",
    "            y_pred_mitigated_bias_on_true = classifier_mitigated_bias.predict(X_true)\n",
    "        \n",
    "        # testing on biased data withOUT fairness intervention\n",
    "        y_pred_bias = classifier_bias.predict(X_bias_true)\n",
    "        \n",
    "        # testing on GT data withOUT fairness intervention\n",
    "        y_pred_bias_on_true = classifier_bias.predict(X_true)\n",
    "\n",
    "        # model performance\n",
    "        \n",
    "        if apply_fairness:\n",
    "            # on biased data\n",
    "            acc_bias_mitigated = accuracy_score(y_pred=y_pred_mitigated_bias, y_true=y_bias_true)\n",
    "            accuracy_on_biased_mitigated.append(acc_bias_mitigated)\n",
    "            # on GT data\n",
    "            acc_bias_mitigated_on_true = accuracy_score(y_pred=y_pred_mitigated_bias_on_true, y_true=y_true)\n",
    "            accuracy_on_true_mitigated.append(acc_bias_mitigated_on_true)\n",
    "        \n",
    "        # on biased data\n",
    "        acc_bias = accuracy_score(y_pred=y_pred_bias, y_true=y_bias_true)\n",
    "        accuracy_on_biased.append(acc_bias)\n",
    "        # on GT data\n",
    "        acc_bias_on_true = accuracy_score(y_pred=y_pred_bias_on_true, y_true=y_true)\n",
    "        accuracy_on_true.append(acc_bias_on_true)\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"Finished Iteration: \", count)\n",
    "            count +=1\n",
    "\n",
    "    return bias_amts, accuracy_on_biased, accuracy_on_true, \\\n",
    "           accuracy_on_biased_mitigated, accuracy_on_true_mitigated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bfd23f-6f40-45d1-a77f-e93d9b460459",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_visualizations(bias_amts,\n",
    "                            accuracy_on_biased = [], accuracy_on_true = [],\n",
    "                            accuracy_on_biased_mitigated = [],\n",
    "                            accuracy_on_true_mitigated = [], fairness = False):\n",
    "    \n",
    "    if fairness:\n",
    "        plt.figure(figsize=(17,7))\n",
    "\n",
    "        plt.plot(bias_amts, accuracy_on_true_mitigated, label = 'Ground Truth')\n",
    "        plt.plot(bias_amts, accuracy_on_biased_mitigated, label = 'Biased Data')\n",
    "        plt.xlabel(\"Amount of Bias (number of minority samples removed)\")\n",
    "        plt.ylabel(\"Accuracy Score\")\n",
    "        plt.axhline(y=accuracy_score(y_pred_truth, y_true), color = \"green\", label = \"Ground Truth Model Accuracy\", alpha = 0.5)\n",
    "        plt.title(\"Biased Model Accuracy\")\n",
    "        plt.ylim(0.92, 0.99)\n",
    "        plt.legend()\n",
    "\n",
    "        plt.show()\n",
    "        \n",
    "    else:\n",
    "        plt.figure(figsize=(17,7))\n",
    "\n",
    "        plt.plot(bias_amts, accuracy_on_true, label = 'Ground Truth')\n",
    "        plt.plot(bias_amts, accuracy_on_biased, label = 'Biased Data')\n",
    "        plt.xlabel(\"Amount of Bias (number of minority samples removed)\")\n",
    "        plt.ylabel(\"Accuracy Score\")\n",
    "        plt.axhline(y=accuracy_score(y_pred_truth, y_true), color = \"green\", label = \"Ground Truth Model Accuracy\", alpha = 0.5)\n",
    "        plt.title(\"Biased Model Accuracy\")\n",
    "        plt.ylim(0.92, 0.99)\n",
    "        plt.legend()\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f79ddb-5166-423d-862f-72e9f33f9214",
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_visualizations(bias_amts, accuracy_on_biased, accuracy_on_true,\n",
    "                        accuracy_on_biased_mitigated, accuracy_on_true_mitigated):\n",
    "    plt.figure(figsize=(17,7))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(bias_amts, accuracy_on_biased, label = 'Tested On Biased Data + No Fairness Intervention', color = \"red\")\n",
    "    plt.plot(bias_amts, accuracy_on_biased_mitigated, label = 'Tested On Biased Data + Fairness Intervention', color = \"green\")\n",
    "    plt.plot(bias_amts, accuracy_on_true, label = 'Tested On Ground Truth + No Fairness Intervention', color = \"blue\")\n",
    "    plt.plot(bias_amts, accuracy_on_true_mitigated, label = 'Tested On Ground Truth + Fairness Intervention', color = \"purple\")\n",
    "    plt.xlabel(\"Amount of Bias (number of minority samples removed)\")\n",
    "    plt.ylabel(\"Accuracy Score\")\n",
    "    #plt.axhline(y=accuracy_score(y_pred_truth, y_true), color = \"green\", label = \"Ground Truth Model On Ground Truth Data\", alpha = 0.5)\n",
    "    plt.title(\"Accuracy of Biased Model (trained on biased data)\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0c372b-e5f1-4ed0-824d-80f8ba5f0a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = LogisticRegression()\n",
    "\n",
    "bias_amts = np.divide(list(range(10,-1,-1)),10)\n",
    "\n",
    "bias_amts, accuracy_on_biased, accuracy_on_true, \\\n",
    "           accuracy_on_biased_mitigated, accuracy_on_true_mitigated = \\\n",
    "tradeoff_visualization(bias_amts, classifier, X_true, y_true,\n",
    "                       df_train, \"sens_feat\", is_synthetic=True,\n",
    "                       apply_fairness=True, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58396013-0031-4828-8796-c2b12e7b59af",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_visualizations(bias_amts, accuracy_on_biased, accuracy_on_true,\n",
    "                     accuracy_on_biased_mitigated, accuracy_on_true_mitigated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ead60c7-3a77-4b77-aeee-d08e1dcd6603",
   "metadata": {},
   "source": [
    "# Trade-Off Visualization (Error Bars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800f6a8c-2145-4a1c-9a73-3526c58eb1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ERROR BAR\n",
    "def tradeoff_visualization_error(bias_amts, classifier, X_true, y_true, \n",
    "                           df_train, sensitive_feature = \"cat\",\n",
    "                           is_synthetic = False,\n",
    "                           apply_fairness = False, verbose = False, num_iters = 1):\n",
    "    total_accuracy_on_true = []\n",
    "    total_accuracy_on_biased = []\n",
    "    total_accuracy_on_true_mitigated = []\n",
    "    total_accuracy_on_biased_mitigated = []\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "        _, acc_bias, acc_true, acc_bias_mit, acc_true_mit = \\\n",
    "        tradeoff_visualization(bias_amts, classifier, X_true, y_true,\n",
    "                               df_train, sensitive_feature,is_synthetic,\n",
    "                               apply_fairness, verbose)\n",
    "                \n",
    "        total_accuracy_on_biased.append(acc_bias)\n",
    "        total_accuracy_on_biased_mitigated.append(acc_bias_mit)\n",
    "        total_accuracy_on_true.append(acc_true)\n",
    "        total_accuracy_on_true_mitigated.append(acc_true_mit)\n",
    "        \n",
    "        if verbose:\n",
    "                print(\"Finished Total Iteration: \", i+1)\n",
    "\n",
    "    mean_biased = np.mean(total_accuracy_on_biased, axis = 0)\n",
    "    mean_biased_mitigated = np.mean(total_accuracy_on_biased_mitigated, axis = 0)\n",
    "    mean_true = np.mean(total_accuracy_on_true, axis = 0)\n",
    "    mean_true_mitigated = np.mean(total_accuracy_on_true_mitigated, axis = 0)\n",
    "\n",
    "    y_err_biased = np.std(total_accuracy_on_biased, axis = 0)\n",
    "    y_err_biased_mitigated = np.std(total_accuracy_on_biased_mitigated, axis = 0)\n",
    "    y_err_true = np.std(total_accuracy_on_true, axis = 0)\n",
    "    y_err_true_mitigated = np.std(total_accuracy_on_true_mitigated, axis = 0)\n",
    "\n",
    "    return bias_amts, mean_biased, mean_true, \\\n",
    "           mean_biased_mitigated, mean_true_mitigated, y_err_biased, \\\n",
    "           y_err_true, y_err_biased_mitigated, y_err_true_mitigated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8716345b-63f8-4310-964b-3b0f903fe4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_visualizations_error(bias_amts, mean_biased, mean_true,\n",
    "                        mean_biased_mitigated, mean_true_mitigated):\n",
    "    plt.figure(figsize=(10,7))\n",
    "    plt.errorbar(bias_amts, mean_biased, yerr= y_err_biased, label = 'Tested On Biased Data + No Fairness Intervention', color = \"red\")\n",
    "    plt.errorbar(bias_amts, mean_biased_mitigated, yerr= y_err_biased_mitigated, label = 'Tested On Biased Data + Fairness Intervention', color = \"green\")\n",
    "    plt.errorbar(bias_amts, mean_true, yerr= y_err_true, label = 'Tested On Ground Truth + No Fairness Intervention', color = \"blue\")\n",
    "    plt.errorbar(bias_amts, mean_true_mitigated, yerr= y_err_true_mitigated, label = 'Tested On Ground Truth + Fairness Intervention', color = \"purple\")\n",
    "    #plt.plot(bias_amts, bayes_accuracy_on_biased, label = 'Bayes Optimal Model On Biased Data', color = \"black\")\n",
    "    #plt.axhline(y = bayes_optimal_accuracy(df_test), label = \"Bayes Optimal Model On Ground Truth Data\", color = \"pink\")\n",
    "    plt.xlim(1.05, -0.05)\n",
    "    plt.xlabel(\"Beta Value\")\n",
    "    plt.ylabel(\"Accuracy Score\")\n",
    "    plt.title(\"Accuracy of Biased Model (trained on biased data)\")\n",
    "    #plt.legend(loc = 1)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df57692-d631-4d9c-a5c2-6bf6a3b43325",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = LogisticRegression()\n",
    "\n",
    "bias_amts = np.divide(list(range(10,-1,-1)),10)\n",
    "\n",
    "bias_amts, mean_biased, mean_true, \\\n",
    "           mean_biased_mitigated, mean_true_mitigated, y_err_biased, \\\n",
    "           y_err_true, y_err_biased_mitigated, y_err_true_mitigated = \\\n",
    "tradeoff_visualization_error(bias_amts, classifier, X_true, y_true,\n",
    "                       df_train, \"sens_feat\", is_synthetic=True,\n",
    "                       apply_fairness=True, verbose=True, num_iters=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdee3628-d7a2-4600-9909-ee744e00713a",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_visualizations_error(bias_amts, mean_biased, mean_true,\n",
    "                    mean_biased_mitigated, mean_true_mitigated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e803e8-c6a1-4026-9598-335d0faaeb22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
