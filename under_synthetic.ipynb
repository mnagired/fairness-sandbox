{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5e91243",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-08T20:41:26.879421Z",
     "start_time": "2021-06-08T20:41:26.876140Z"
    }
   },
   "source": [
    "# Under-Representation Bias (w/ Synthetic Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00909876",
   "metadata": {},
   "source": [
    "This notebook recreates the finding that Equalized Odds constrained model can recover from under-representation bias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a42ced1",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "Please run the code block below to install the necessary packages (if needed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a692724",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T20:42:02.586362Z",
     "start_time": "2021-06-10T20:42:02.581026Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, roc_curve, auc\n",
    "from collections import Counter\n",
    "\n",
    "import fairlearn\n",
    "from fairlearn.metrics import *\n",
    "from fairlearn.reductions import *\n",
    "import aif360\n",
    "\n",
    "import copy, random\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da59e303",
   "metadata": {},
   "source": [
    "# Synthetic Dataset Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d46dc53-d064-4c1f-8845-891216d77cde",
   "metadata": {},
   "source": [
    "## Parameters (User Input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e4c1bcf-a112-4a4f-9d02-a6724a2268b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "r is the proportion of training examples in the minority group, \n",
    "\n",
    "which means 1-r is proportion of examples in the majority group\n",
    "\n",
    "eta is the probability of flipping the label\n",
    "\n",
    "n is the number of training examples\n",
    "\n",
    "beta is the probability of keeping a positively labeled example\n",
    "from the minority class\n",
    "\n",
    "NOTE: results can be replicated if and only if the following condition holds:\n",
    "\n",
    "(1-r)(1-2*eta) + r((1-eta)*beta - eta) > 0\n",
    "\n",
    "'''\n",
    "def get_params(r = 1/3, eta = 1/4, n = 2000, beta = 0.5):\n",
    "    return r, eta, n, beta\n",
    "\n",
    "r, eta, n, beta = get_params(n = 20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e7d3c98-54c3-4198-9f03-528bebb016cc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constraint:  0.5\n",
      "yes! 0.3333333333333333 0.25 1.0\n",
      "constraint:  0.47500000000000003\n",
      "yes! 0.3333333333333333 0.25 0.9\n",
      "constraint:  0.45000000000000007\n",
      "yes! 0.3333333333333333 0.25 0.8\n",
      "constraint:  0.425\n",
      "yes! 0.3333333333333333 0.25 0.7\n",
      "constraint:  0.4\n",
      "yes! 0.3333333333333333 0.25 0.6\n",
      "constraint:  0.37500000000000006\n",
      "yes! 0.3333333333333333 0.25 0.5\n",
      "constraint:  0.35000000000000003\n",
      "yes! 0.3333333333333333 0.25 0.4\n",
      "constraint:  0.325\n",
      "yes! 0.3333333333333333 0.25 0.3\n",
      "constraint:  0.30000000000000004\n",
      "yes! 0.3333333333333333 0.25 0.2\n",
      "constraint:  0.275\n",
      "yes! 0.3333333333333333 0.25 0.1\n",
      "constraint:  0.25000000000000006\n",
      "yes! 0.3333333333333333 0.25 0.0\n"
     ]
    }
   ],
   "source": [
    "# check if above constraint holds\n",
    "def check_constraints(r, eta, beta):\n",
    "    first = (1-r)*(1-2*eta)\n",
    "    second = r * ((1-eta)*beta - eta)\n",
    "    res = first + second\n",
    "    print(\"constraint: \", res)\n",
    "    print(\"yes!\", r, eta, beta) if res > 0 else print(\"no\", r, eta, beta)\n",
    "    \n",
    "bias_amts = np.divide(list(range(10, -1, -1)),10)\n",
    "\n",
    "for beta in bias_amts:\n",
    "    check_constraints(r, eta, beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270ebbd2-719c-42d5-8de4-dce388fc0a51",
   "metadata": {},
   "source": [
    "## True Label Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74f9b8a4-c293-4c8a-9c8c-72d28f624312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create minority and majority groups\n",
    "def get_cat_features(n, r):\n",
    "    num_minority = int(r * n)\n",
    "    num_majority = n - num_minority\n",
    "    \n",
    "    minority = np.zeros((num_minority, 1))\n",
    "    majority = np.ones((num_majority, 1))\n",
    "    \n",
    "    cat_features = np.vstack((minority, majority))\n",
    "    \n",
    "    # shuffle so as to ensure randomness\n",
    "    np.random.shuffle(cat_features)\n",
    "    \n",
    "    return cat_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "acba1061-c071-4a58-ae21-706122184dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# return labels from Bayes Optimal Classifier\n",
    "def get_bayes_optimal_labels(features, effect_param):\n",
    "    outcome_continuous = 1/(1+np.exp(-np.matmul(features, effect_param)))\n",
    "    return np.where(outcome_continuous >= 0.5, 1, 0)\n",
    "\n",
    "# flip labels with probability eta\n",
    "def flip_labels(df_synthetic, eta):\n",
    "    labels = df_synthetic['outcome']\n",
    "    \n",
    "    for i in range(len(labels)):\n",
    "        if random.uniform(0,1) <= eta:\n",
    "            labels[i] = 1 if labels[i] == 0 else 0\n",
    "    df_synthetic['outcome'] = labels\n",
    "    \n",
    "    return df_synthetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a643d5b1-14da-4b8d-a9ae-075fa85610b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure equal proportion of positive examples across minority and majority\n",
    "def equal_base_rates(df_majority, df_minority):\n",
    "    base_rate_maj = df_majority['outcome'].value_counts()[0] / len(df_majority)\n",
    "    base_rate_min = df_minority['outcome'].value_counts()[0] / len(df_minority)\n",
    "    \n",
    "    X_maj_pos = df_majority[df_majority['outcome'] == 1].iloc[:, :].values\n",
    "    X_maj_neg = df_majority[df_majority['outcome'] == 0].iloc[:, :].values\n",
    "    \n",
    "    diff = round(base_rate_maj,4) - round(base_rate_min,4)\n",
    "    \n",
    "    print(diff*100)\n",
    "    \n",
    "    count = 0\n",
    "    \n",
    "    if diff > 0:\n",
    "        while(diff > 0.01):\n",
    "            X_maj_neg = np.delete(X_maj_neg, 0, axis = 0)\n",
    "\n",
    "            df_majority = pd.DataFrame(pd.DataFrame(np.vstack((X_maj_pos, X_maj_neg))))\n",
    "            df_majority.columns = ['num1','num2','num3','cat','outcome']\n",
    "\n",
    "            base_rate_maj = df_majority['outcome'].value_counts()[0] / len(df_majority)\n",
    "            diff = round(base_rate_maj,4) - round(base_rate_min,4)\n",
    "            count+=1\n",
    "\n",
    "            # fail-safe\n",
    "            if count > int(len(df_majority)/3): break\n",
    "    else:\n",
    "        diff = round(base_rate_min,4) - round(base_rate_maj,4) \n",
    "        while(diff > 0.01):\n",
    "            X_maj_pos = np.delete(X_maj_pos, 0, axis = 0)\n",
    "\n",
    "            df_majority = pd.DataFrame(pd.DataFrame(np.vstack((X_maj_pos, X_maj_neg))))\n",
    "            df_majority.columns = ['num1','num2','num3','cat','outcome']\n",
    "\n",
    "            base_rate_maj = df_majority['outcome'].value_counts()[0] / len(df_majority)\n",
    "            diff = round(base_rate_min,4) - round(base_rate_maj,4) \n",
    "            count+=1\n",
    "\n",
    "            # fail-safe\n",
    "            if count > int(len(df_majority)/3): break\n",
    "                \n",
    "    total = np.vstack((df_majority, df_minority))\n",
    "    \n",
    "    # shuffle so as to ensure randomness\n",
    "    np.random.shuffle(total)\n",
    "                \n",
    "    df_true = pd.DataFrame(pd.DataFrame(total))\n",
    "    df_true.columns = ['num1','num2','num3','cat','outcome']\n",
    "    \n",
    "    print(diff*100)\n",
    "    \n",
    "    return df_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9faebb17-ed24-453b-a179-16933179c612",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "create synthetic data with:\n",
    "    3 numerical features (Gaussian), 1 categorical (sensitive attribute) \n",
    "    logistic outcome model s.t. outcome = Indicator[logit(effect_param*features) >= 0.5]\n",
    "    \n",
    "create minority/majority groups according to r param\n",
    "\n",
    "simulate Bayes Optimal Classifiers for minority and majority\n",
    "\n",
    "flip labels according to eta param\n",
    "\n",
    "ensure equal base rates (proportion of positive examples) across both groups\n",
    "\n",
    "'''\n",
    "\n",
    "def true_label_generation(r, eta, n):\n",
    "\n",
    "    ''' \n",
    "    delete this variable to allow user to control percentage of positively labeled examples\n",
    "    eg: let outcome_continuous >= 0.2 implies 80% positively labeled samples\n",
    "    '''\n",
    "    # causal effect params\n",
    "    effect_param_min = [0.5, -0.2, 0.1] \n",
    "    effect_param_maj = [-0.7, 0.5, 1.5]\n",
    "    \n",
    "    num_min = int(n*r)\n",
    "    num_maj = n - num_min\n",
    "\n",
    "    # required: len(cat_probabilities) = n_cat_features\n",
    "    n_cat_features = 2\n",
    "    cat_probabilities = [0.5, 0.5] \n",
    "\n",
    "    # numerical feature params\n",
    "    means = [0, 0, 0]\n",
    "    cov_matrix = [[1, 0, 0], [0, 1, 0], [0, 0, 1]]\n",
    "\n",
    "    # features\n",
    "    cat_features = get_cat_features(r=r, n=n)\n",
    "    num_features_min = np.random.multivariate_normal(means, cov_matrix, num_min)\n",
    "    num_features_maj = np.random.multivariate_normal(means, cov_matrix, num_maj)\n",
    "\n",
    "    num_features = np.concatenate((num_features_min, num_features_maj))\n",
    "\n",
    "    # outcomes\n",
    "    outcome_continuous_min = 1/(1+np.exp(-np.matmul(num_features_min,effect_param_min))) # logit model + no added noise\n",
    "    outcome_continuous_maj = 1/(1+np.exp(-np.matmul(num_features_maj,effect_param_maj))) # logit model + no added noise\n",
    "    outcome_binary_min = get_bayes_optimal_labels(features=num_features_min, effect_param=effect_param_min)\n",
    "    outcome_binary_maj = get_bayes_optimal_labels(features=num_features_maj, effect_param=effect_param_maj)\n",
    "    \n",
    "    outcome = np.hstack((outcome_binary_min,outcome_binary_maj)).reshape(n,1)\n",
    "    temp_data = np.hstack((num_features,cat_features, outcome))\n",
    "    np.random.shuffle(temp_data) # randomly shuffle the data\n",
    "    \n",
    "    df_synthetic = pd.DataFrame(temp_data)\n",
    "    df_synthetic.columns = ['num1','num2','num3','cat','outcome']\n",
    "    \n",
    "    df_majority = df_synthetic[df_synthetic['cat'] == 1]\n",
    "    df_minority = df_synthetic[df_synthetic['cat'] == 0]\n",
    "    \n",
    "    df_synthetic = flip_labels(df_synthetic, eta)\n",
    "    \n",
    "    df_majority = df_synthetic[df_synthetic['cat'] == 1]\n",
    "    df_minority = df_synthetic[df_synthetic['cat'] == 0]\n",
    "    \n",
    "    # df_synthetic = equal_base_rates(df_majority, df_minority)\n",
    "    \n",
    "    return df_synthetic \n",
    "\n",
    "df_synthetic = true_label_generation(r=r, eta=eta, n=n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a7fd87-924b-41b0-a505-25cac190e8cd",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d385bc3-625f-4726-a2b3-7dac659fda04",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "547edc7c-4c9b-4f47-8a61-9398fc20f79f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# split into train and test\n",
    "df_train = df_synthetic.loc[range(0,int(len(df_synthetic)/2)), :]\n",
    "# if original dataset has odd number of samples, remove 1 sample to be even\n",
    "if (len(df_synthetic) % 2 == 1):\n",
    "    df_test = df_synthetic.loc[range(int(len(df_synthetic)/2)+1, len(df_synthetic)), :]\n",
    "else:\n",
    "    df_test = df_synthetic.loc[range(int(len(df_synthetic)/2), len(df_synthetic)), :]\n",
    "\n",
    "# format data\n",
    "X_true = df_test.iloc[:, :-1].values\n",
    "y_true = df_test.iloc[:, -1].values\n",
    "\n",
    "sens_attrs_true = [df_test['cat']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950918b3",
   "metadata": {},
   "source": [
    "# Bias Injection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a788ace-562a-4256-a853-40db73a61e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def under_sample(df_minority_positive, beta):\n",
    "    X_min = df_minority_positive.iloc[:, :].values\n",
    "    \n",
    "    # keep each example with probability beta\n",
    "    for i in range(len(X_min)):\n",
    "        if random.uniform(0,1) > beta:\n",
    "            X_min = np.delete(X_min, 0, axis=0)\n",
    "    \n",
    "    df_minority_positive = pd.DataFrame(pd.DataFrame(X_min))\n",
    "    df_minority_positive.columns = ['num1','num2','num3','cat','outcome']\n",
    "    return df_minority_positive\n",
    "\n",
    "def get_biased_data(df_train, beta):\n",
    "    df_majority = df_train[df_train['cat'] == 1]\n",
    "    df_minority = df_train[df_train['cat'] == 0]\n",
    "    \n",
    "    # unfavored group with negative label\n",
    "    df_minority_negative = df_minority[df_minority['outcome'] == 0.0]\n",
    "\n",
    "    # unfavored group with positive label (preferred)\n",
    "    df_minority_positive = df_minority[df_minority['outcome'] == 1.0]\n",
    "    \n",
    "    # data frame without positively labeled examples from minority class\n",
    "    df_total = pd.concat([df_majority, df_minority_negative])\n",
    "    \n",
    "    # under-sampling process\n",
    "    df_undersampled = under_sample(df_minority_positive, beta)\n",
    "\n",
    "    # combine undersampled and original favored class to create dataset\n",
    "    df_concat = pd.concat([df_total,df_undersampled])\n",
    "    \n",
    "    return df_concat.sample(frac=1)\n",
    "\n",
    "df_concat = get_biased_data(df_train, 0.5)\n",
    "\n",
    "# for fairness measures later\n",
    "df_sens = df_concat['cat']\n",
    "\n",
    "# format data\n",
    "X_bias = df_concat.iloc[:, :-1].values\n",
    "y_bias = df_concat.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84403b44",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f6827f",
   "metadata": {},
   "source": [
    "### Model Selection + Training (TODO: modularize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72d79618",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T20:42:09.057618Z",
     "start_time": "2021-06-10T20:42:08.942574Z"
    }
   },
   "outputs": [],
   "source": [
    "classifier = LogisticRegression(random_state=42)\n",
    "\n",
    "classifier_bias = classifier.fit(X_bias, y_bias)\n",
    "y_pred_bias = classifier_bias.predict(X_bias)\n",
    "y_pred_bias_on_true = classifier_bias.predict(X_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff477ccd",
   "metadata": {},
   "source": [
    "### Model Performance (TODO: modularize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0854c348-c45b-4cda-b1f4-5dd771baee53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# return accuracy of bayes optimal model on a given dataframe\n",
    "def bayes_optimal_accuracy(df):\n",
    "    \n",
    "    # need to exclude outcome and categorical variables\n",
    "    X_maj = df[df['cat'] == 1].iloc[:, :-2].values\n",
    "    y_maj_true = df[df['cat'] == 1].iloc[:, -1].values\n",
    "    effect_param_maj = [-0.7, 0.5, 1.5]\n",
    "\n",
    "    X_min = df[df['cat'] == 0].iloc[:, :-2].values\n",
    "    y_min_true = df[df['cat'] == 0].iloc[:, -1].values\n",
    "    effect_param_min = [0.5, -0.2, 0.1] \n",
    "\n",
    "    pred_maj_labels = get_bayes_optimal_labels(X_maj, effect_param_maj).reshape(len(X_maj), 1)\n",
    "    pred_min_labels = get_bayes_optimal_labels(X_min, effect_param_min).reshape(len(X_min), 1)\n",
    "\n",
    "    accuracy_maj = accuracy_score(y_pred=pred_maj_labels, y_true=y_maj_true)\n",
    "    accuracy_min = accuracy_score(y_pred=pred_min_labels, y_true=y_min_true)\n",
    "    \n",
    "    accuracy_bayes = np.mean([accuracy_maj, accuracy_min])\n",
    "    \n",
    "    return accuracy_bayes, accuracy_maj, accuracy_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "adf0a2b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T20:42:09.231352Z",
     "start_time": "2021-06-10T20:42:09.225108Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synthetic Data\n",
      "\n",
      "Accuracy of Bayes Optimal Model on Ground Truth Data (Avg):  0.5993588355654584\n",
      "Accuracy of Bayes Optimal Model on Ground Truth Data (Maj):  0.6482172161718633\n",
      "Accuracy of Bayes Optimal Model on Ground Truth Data (Min):  0.5505004549590536\n",
      "Accuracy of Biased Model on Biased Data:  0.6326486251494403\n",
      "Accuracy of Biased Model on Ground Truth Data:  0.5901\n"
     ]
    }
   ],
   "source": [
    "print(\"Synthetic Data\\n\")\n",
    "\n",
    "bayes, maj, minor = bayes_optimal_accuracy(df_test)\n",
    "print(\"Accuracy of Bayes Optimal Model on Ground Truth Data (Avg): \", bayes)\n",
    "print(\"Accuracy of Bayes Optimal Model on Ground Truth Data (Maj): \", maj)\n",
    "print(\"Accuracy of Bayes Optimal Model on Ground Truth Data (Min): \", minor)\n",
    "print(\"Accuracy of Biased Model on Biased Data: \", accuracy_score(y_pred_bias, y_bias))\n",
    "print(\"Accuracy of Biased Model on Ground Truth Data: \", accuracy_score(y_pred_bias_on_true, y_true))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7ea20d",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e393d0-149b-4da6-91ba-e74e5225b0e2",
   "metadata": {},
   "source": [
    "### Bias vs Accuracy vs Fairness Trade-Off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3bc63ab-f34e-4f6f-8867-d13e420d6f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if verbose, shows \"Finished iteration: ... \"\n",
    "def tradeoff_visualization(classifier, X_true, y_true, df_train,\n",
    "                           sensitive_feature = \"cat\",\n",
    "                           apply_fairness = False, verbose = False):\n",
    "    \n",
    "    # 0 to 1 in increments of 0.1\n",
    "    #bias_amts = np.divide(list(range(0,11)),10)\n",
    "    bias_amts = np.divide(list(range(10,-1,-1)),10)\n",
    "    accuracy_on_true = []\n",
    "    accuracy_on_biased = []\n",
    "    bayes_accuracy_on_biased = []\n",
    "    accuracy_on_true_mitigated = []\n",
    "    accuracy_on_biased_mitigated = []\n",
    "    \n",
    "    count = 0\n",
    "\n",
    "    for beta in bias_amts:\n",
    "        \n",
    "        df_train_copy = df_train.copy()\n",
    "        \n",
    "        df_majority = df_train_copy[df_train_copy['cat'] == 1]\n",
    "        df_minority = df_train_copy[df_train_copy['cat'] == 0]\n",
    "\n",
    "        # unfavored group with negative label\n",
    "        df_minority_negative = df_minority[df_minority['outcome'] == 0.0]\n",
    "\n",
    "        # unfavored group with positive label (preferred)\n",
    "        df_minority_positive = df_minority[df_minority['outcome'] == 1.0]\n",
    "\n",
    "        # data frame without positively labeled examples from minority class\n",
    "        df_total = pd.concat([df_majority, df_minority_negative])\n",
    "\n",
    "        # under-sampling process\n",
    "        df_undersampled = under_sample(df_minority_positive, beta)\n",
    "        print(\"Num Minority Pos: \", len(df_undersampled))\n",
    "\n",
    "        # combine undersampled and original favored class to create dataset\n",
    "        df_concat = pd.concat([df_total,df_undersampled])\n",
    "        \n",
    "        bayes_accuracy_on_biased.append(bayes_optimal_accuracy(df_concat))\n",
    "        \n",
    "        df_sens = df_concat[sensitive_feature]\n",
    "\n",
    "        # format data\n",
    "        X_bias_true = df_concat.iloc[:, :-1].values\n",
    "        y_bias_true = df_concat.iloc[:, -1].values\n",
    "\n",
    "        classifier_bias = classifier.fit(X_bias_true, y_bias_true)\n",
    "        \n",
    "        if apply_fairness:\n",
    "            constraint = EqualizedOdds()\n",
    "            classifier_mitigated_bias = ExponentiatedGradient(classifier_bias, constraint)\n",
    "            classifier_mitigated_bias.fit(X_bias_true, y_bias_true, sensitive_features = df_sens)\n",
    "            \n",
    "            # testing on biased data WITH fairness intervention\n",
    "            y_pred_mitigated_bias = classifier_mitigated_bias.predict(X_bias_true)\n",
    "            \n",
    "            # testing on GT data WITH fairness intervention\n",
    "            y_pred_mitigated_bias_on_true = classifier_mitigated_bias.predict(X_true)\n",
    "        \n",
    "        # testing on biased data withOUT fairness intervention\n",
    "        y_pred_bias = classifier_bias.predict(X_bias_true)\n",
    "        \n",
    "        # testing on GT data withOUT fairness intervention\n",
    "        y_pred_bias_on_true = classifier_bias.predict(X_true)\n",
    "\n",
    "        # model performance\n",
    "        \n",
    "        if apply_fairness:\n",
    "            # on biased data\n",
    "            acc_bias_mitigated = accuracy_score(y_pred=y_pred_mitigated_bias, y_true=y_bias_true)\n",
    "            accuracy_on_biased_mitigated.append(acc_bias_mitigated)\n",
    "            \n",
    "            # on GT data\n",
    "            acc_bias_mitigated_on_true = accuracy_score(y_pred=y_pred_mitigated_bias_on_true, y_true=y_true)\n",
    "            accuracy_on_true_mitigated.append(acc_bias_mitigated_on_true)\n",
    "        \n",
    "        # on biased data\n",
    "        acc_bias = accuracy_score(y_pred=y_pred_bias, y_true=y_bias_true)\n",
    "        accuracy_on_biased.append(acc_bias)\n",
    "        # on GT data\n",
    "        acc_bias_on_true = accuracy_score(y_pred=y_pred_bias_on_true, y_true=y_true)\n",
    "        accuracy_on_true.append(acc_bias_on_true)\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"Finished Iteration: \", count)\n",
    "            count +=1\n",
    "        \n",
    "        # fail-safe\n",
    "        if count > 30:\n",
    "            break\n",
    "\n",
    "    return bias_amts, accuracy_on_biased, accuracy_on_true, \\\n",
    "           accuracy_on_biased_mitigated, accuracy_on_true_mitigated, bayes_accuracy_on_biased"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f17d7a-7138-479b-87f7-ff45e561eb97",
   "metadata": {},
   "source": [
    "### Error Bars with Multiple Iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "7cdc3dd6-a00b-4e1d-8177-86130db9a557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if verbose, shows \"Finished iteration: ... \"\n",
    "def tradeoff_visualization_error(classifier, X_true, y_true, df_train,\n",
    "                           sensitive_feature = \"cat\",\n",
    "                           apply_fairness = False, verbose = False, num_iters = 10):\n",
    "    \n",
    "    total_accuracy_on_true = []\n",
    "    total_accuracy_on_biased = []\n",
    "    total_accuracy_on_true_mitigated = []\n",
    "    total_accuracy_on_biased_mitigated = []\n",
    "    total_bayes_accuracy_on_true = []\n",
    "    total_bayes_accuracy_on_true_maj = []\n",
    "    total_bayes_accuracy_on_true_min = []\n",
    "    bayes_accuracy_on_true = []\n",
    "    bayes_accuracy_on_true_maj = []\n",
    "    bayes_accuracy_on_true_min = []\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "    \n",
    "        # 0 to 1 in increments of 0.1\n",
    "        bias_amts = np.divide(list(range(10,-1,-1)),10)\n",
    "        accuracy_on_true = []\n",
    "        accuracy_on_biased = []\n",
    "        accuracy_on_true_mitigated = []\n",
    "        accuracy_on_biased_mitigated = []\n",
    "        bayes_temp = []\n",
    "        bayes_temp_maj = []\n",
    "        bayes_temp_min = []\n",
    "\n",
    "        count = 0\n",
    "        \n",
    "        df_synthetic = true_label_generation(r=r, eta=eta, n=n)\n",
    "        \n",
    "        # split into train and test\n",
    "        df_train = df_synthetic.loc[range(0,int(len(df_synthetic)/2)), :]\n",
    "        # if original dataset has odd number of samples, remove 1 sample to be even\n",
    "        if (len(df_synthetic) % 2 == 1):\n",
    "            df_test = df_synthetic.loc[range(int(len(df_synthetic)/2)+1, len(df_synthetic)), :]\n",
    "        else:\n",
    "            df_test = df_synthetic.loc[range(int(len(df_synthetic)/2), len(df_synthetic)), :]\n",
    "\n",
    "        # format data\n",
    "        X_true = df_test.iloc[:, :-1].values\n",
    "        y_true = df_test.iloc[:, -1].values\n",
    "        \n",
    "        # bayes optimal classifier accuracy on ground truth data\n",
    "        bayes, bayes_maj, bayes_min = bayes_optimal_accuracy(df_test)\n",
    "        bayes_accuracy_on_true_maj.append(bayes_maj)\n",
    "        bayes_accuracy_on_true_min.append(bayes_min)\n",
    "        bayes_accuracy_on_true.append(bayes)\n",
    "\n",
    "        sens_attrs_true = [df_test['cat']]\n",
    "\n",
    "        for beta in bias_amts:\n",
    "\n",
    "            df_train_copy = df_train.copy()\n",
    "\n",
    "            df_majority = df_train_copy[df_train_copy['cat'] == 1]\n",
    "            df_minority = df_train_copy[df_train_copy['cat'] == 0]\n",
    "\n",
    "            # unfavored group with negative label\n",
    "            df_minority_negative = df_minority[df_minority['outcome'] == 0.0]\n",
    "\n",
    "            # unfavored group with positive label (preferred)\n",
    "            df_minority_positive = df_minority[df_minority['outcome'] == 1.0]\n",
    "\n",
    "            # data frame without positively labeled examples from minority class\n",
    "            df_total = pd.concat([df_majority, df_minority_negative])\n",
    "\n",
    "            # under-sampling process\n",
    "            df_undersampled = under_sample(df_minority_positive, beta)\n",
    "            #print(\"Num Minority Pos: \", len(df_undersampled))\n",
    "\n",
    "            # combine undersampled and original favored class to create dataset\n",
    "            df_concat = pd.concat([df_total,df_undersampled])\n",
    "\n",
    "            df_sens = df_concat[sensitive_feature]\n",
    "\n",
    "            # format data\n",
    "            X_bias_true = df_concat.iloc[:, :-1].values\n",
    "            y_bias_true = df_concat.iloc[:, -1].values\n",
    "\n",
    "            classifier_bias = classifier.fit(X_bias_true, y_bias_true)\n",
    "\n",
    "            if apply_fairness:\n",
    "                constraint = EqualizedOdds()\n",
    "                classifier_mitigated_bias = ExponentiatedGradient(classifier_bias, constraint)\n",
    "                classifier_mitigated_bias.fit(X_bias_true, y_bias_true, sensitive_features = df_sens)\n",
    "\n",
    "                # testing on biased data WITH fairness intervention\n",
    "                y_pred_mitigated_bias = classifier_mitigated_bias.predict(X_bias_true)\n",
    "\n",
    "                # testing on GT data WITH fairness intervention\n",
    "                y_pred_mitigated_bias_on_true = classifier_mitigated_bias.predict(X_true)\n",
    "\n",
    "            # testing on biased data withOUT fairness intervention\n",
    "            y_pred_bias = classifier_bias.predict(X_bias_true)\n",
    "\n",
    "            # testing on GT data withOUT fairness intervention\n",
    "            y_pred_bias_on_true = classifier_bias.predict(X_true)\n",
    "\n",
    "            # model performance\n",
    "\n",
    "            if apply_fairness:\n",
    "                # on biased data\n",
    "                acc_bias_mitigated = accuracy_score(y_pred=y_pred_mitigated_bias, y_true=y_bias_true)\n",
    "                accuracy_on_biased_mitigated.append(acc_bias_mitigated)\n",
    "\n",
    "                # on GT data\n",
    "                acc_bias_mitigated_on_true = accuracy_score(y_pred=y_pred_mitigated_bias_on_true, y_true=y_true)\n",
    "                accuracy_on_true_mitigated.append(acc_bias_mitigated_on_true)\n",
    "\n",
    "            # on biased data\n",
    "            acc_bias = accuracy_score(y_pred=y_pred_bias, y_true=y_bias_true)\n",
    "            accuracy_on_biased.append(acc_bias)\n",
    "            # on GT data\n",
    "            acc_bias_on_true = accuracy_score(y_pred=y_pred_bias_on_true, y_true=y_true)\n",
    "            accuracy_on_true.append(acc_bias_on_true)\n",
    "            \n",
    "            bayes_temp.append(bayes)\n",
    "            bayes_temp_maj.append(bayes_maj)\n",
    "            bayes_temp_min.append(bayes_min)\n",
    "            \n",
    "\n",
    "            if verbose:\n",
    "                print(\"Finished Iteration: \", count)\n",
    "                count +=1\n",
    "\n",
    "            # fail-safe\n",
    "            if count > 30:\n",
    "                break\n",
    "        \n",
    "        total_accuracy_on_biased.append(accuracy_on_biased)\n",
    "        total_accuracy_on_biased_mitigated.append(accuracy_on_biased_mitigated)\n",
    "        total_accuracy_on_true.append(accuracy_on_true)\n",
    "        total_accuracy_on_true_mitigated.append(accuracy_on_true_mitigated)\n",
    "        total_bayes_accuracy_on_true.append(bayes_temp)\n",
    "        total_bayes_accuracy_on_true_maj.append(bayes_temp_maj)\n",
    "        total_bayes_accuracy_on_true_min.append(bayes_temp_min)\n",
    "        \n",
    "        if verbose:\n",
    "                print(\"Finished Total Iteration: \", i+1)\n",
    "    \n",
    "        \n",
    "    mean_biased = np.mean(total_accuracy_on_biased, axis = 0)\n",
    "    mean_biased_mitigated = np.mean(total_accuracy_on_biased_mitigated, axis = 0)\n",
    "    mean_true = np.mean(total_accuracy_on_true, axis = 0)\n",
    "    mean_true_mitigated = np.mean(total_accuracy_on_true_mitigated, axis = 0)\n",
    "    mean_bayes_accuracy_on_true = np.mean(total_bayes_accuracy_on_true, axis = 0)\n",
    "    mean_bayes_accuracy_on_true_maj = np.mean(total_bayes_accuracy_on_true_maj, axis = 0)\n",
    "    mean_bayes_accuracy_on_true_min = np.mean(total_bayes_accuracy_on_true_min, axis = 0)\n",
    "    \n",
    "    y_err_biased = np.std(total_accuracy_on_biased, axis = 0)\n",
    "    y_err_biased_mitigated = np.std(total_accuracy_on_biased_mitigated, axis = 0)\n",
    "    y_err_true = np.std(total_accuracy_on_true, axis = 0)\n",
    "    y_err_true_mitigated = np.std(total_accuracy_on_true_mitigated, axis = 0)\n",
    "\n",
    "\n",
    "    return bias_amts, mean_biased, mean_true, \\\n",
    "           mean_biased_mitigated, mean_true_mitigated, y_err_biased, \\\n",
    "           y_err_true, y_err_biased_mitigated, y_err_true_mitigated, \\\n",
    "           mean_bayes_accuracy_on_true, mean_bayes_accuracy_on_true_maj, \\\n",
    "           mean_bayes_accuracy_on_true_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "99bfd23f-6f40-45d1-a77f-e93d9b460459",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_visualizations(bias_amts, accuracy_on_biased = [], accuracy_on_true = [],\n",
    "                            accuracy_on_biased_mitigated = [],\n",
    "                            accuracy_on_true_mitigated = [], fairness = False):\n",
    "    plt.figure(figsize=(10,7))\n",
    "    if fairness:\n",
    "        plt.plot(bias_amts, accuracy_on_true_mitigated, label = 'Ground Truth')\n",
    "        plt.plot(bias_amts, accuracy_on_biased_mitigated, label = 'Biased Data')\n",
    "        plt.xlim(1.05, -0.05)\n",
    "        plt.xlabel(\"Beta Value\")\n",
    "        plt.ylabel(\"Accuracy Score\")\n",
    "        plt.title(\"Biased Model Accuracy\")\n",
    "        plt.legend()\n",
    "\n",
    "        plt.show()\n",
    "        \n",
    "    else:\n",
    "        plt.plot(bias_amts, accuracy_on_true, label = 'Ground Truth')\n",
    "        plt.plot(bias_amts, accuracy_on_biased, label = 'Biased Data')\n",
    "        plt.xlim(1.05, -0.05)\n",
    "        plt.xlabel(\"Beta Value\")\n",
    "        plt.ylabel(\"Accuracy Score\")\n",
    "        plt.title(\"Biased Model Accuracy\")\n",
    "        plt.legend()\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "4d884d65-9fac-4e54-aafa-d3fc41b627b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_visualizations(bias_amts, mean_biased, mean_true,\n",
    "                        mean_biased_mitigated, mean_true_mitigated):\n",
    "    plt.figure(figsize=(10,7))\n",
    "    plt.errorbar(bias_amts, mean_biased, yerr= y_err_biased, label = 'Tested On Biased Data + No Fairness Intervention', color = \"red\")\n",
    "    plt.errorbar(bias_amts, mean_biased_mitigated, yerr= y_err_biased_mitigated, label = 'Tested On Biased Data + Fairness Intervention', color = \"green\")\n",
    "    plt.errorbar(bias_amts, mean_true, yerr= y_err_true, label = 'Tested On Ground Truth + No Fairness Intervention', color = \"blue\")\n",
    "    plt.errorbar(bias_amts, mean_true_mitigated, yerr= y_err_true_mitigated, label = 'Tested On Ground Truth + Fairness Intervention', color = \"purple\")\n",
    "    #plt.plot(bias_amts, bayes_accuracy_on_biased, label = 'Bayes Optimal Model On Biased Data', color = \"black\")\n",
    "    #plt.axhline(y = bayes_optimal_accuracy(df_test), label = \"Bayes Optimal Model On Ground Truth Data\", color = \"pink\")\n",
    "    plt.xlim(1.05, -0.05)\n",
    "    plt.xlabel(\"Beta Value\")\n",
    "    plt.ylabel(\"Accuracy Score\")\n",
    "    plt.title(\"Accuracy of Biased Model (trained on biased data)\")\n",
    "    #plt.legend(loc = 1)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41379f6f-9cc4-4ab8-bf1c-5fd3dd0164ae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Iteration:  0\n"
     ]
    }
   ],
   "source": [
    "classifier = LogisticRegression()\n",
    "\n",
    "bias_amts, mean_biased, mean_true, \\\n",
    "           mean_biased_mitigated, mean_true_mitigated, y_err_biased, \\\n",
    "           y_err_true, y_err_biased_mitigated, y_err_true_mitigated, \\\n",
    "           mean_bayes_accuracy_on_true, mean_bayes_accuracy_on_true_maj, \\\n",
    "           mean_bayes_accuracy_on_true_min = \\\n",
    "tradeoff_visualization_error(classifier, X_true=X_true, y_true=y_true, \\\n",
    "                       df_train=df_train, sensitive_feature=\"cat\", \\\n",
    "                       apply_fairness=True,verbose=True, num_iters=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0491708e-1d7c-43ef-a567-5e69861a9eb0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.errorbar(bias_amts, mean_true_mitigated, yerr= y_err_true_mitigated, label = 'Tested On Ground Truth + Fairness Intervention', color = \"purple\")\n",
    "plt.errorbar(bias_amts, mean_bayes_accuracy_on_true, label = 'Bayes Optimal (Avg)', color = 'blue')\n",
    "plt.errorbar(bias_amts, mean_bayes_accuracy_on_true_maj, label = 'Bayes Optimal (Maj)', color = 'red')\n",
    "plt.errorbar(bias_amts, mean_bayes_accuracy_on_true_min, label = 'Bayes Optimal (Min)', color = 'green')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45a365a-fefe-4be6-9a10-7e465ea93d06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# without fairness intervention\n",
    "#accuracy_visualizations(bias_amts, accuracy_on_biased, accuracy_on_true,\n",
    "#                        accuracy_on_biased_mitigated, accuracy_on_true_mitigated, False, beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ec3103-ef16-4507-b188-75202083401a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# with fairness intervention\n",
    "#accuracy_visualizations(bias_amts, accuracy_on_biased, accuracy_on_true,\n",
    "#                        accuracy_on_biased_mitigated, accuracy_on_true_mitigated, True, beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f95dca7-da5e-45fe-be62-0edc4e3164ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_visualizations(bias_amts, mean_biased, mean_true,\n",
    "                    mean_biased_mitigated, mean_true_mitigated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718b42df-933b-4fd6-a6fe-8d3f5e0a519b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
