{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5e91243",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-08T20:41:26.879421Z",
     "start_time": "2021-06-08T20:41:26.876140Z"
    }
   },
   "source": [
    "# Under-Representation Bias (w/ Synthetic Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00909876",
   "metadata": {},
   "source": [
    "This notebook recreates the finding that Equalized Odds constrained model can recover from under-representation bias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a42ced1",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "Please run the code block below to install the necessary packages (if needed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a692724",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T20:42:02.586362Z",
     "start_time": "2021-06-10T20:42:02.581026Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, roc_curve, auc\n",
    "from collections import Counter\n",
    "\n",
    "import fairlearn\n",
    "from fairlearn.metrics import *\n",
    "from fairlearn.reductions import *\n",
    "import aif360\n",
    "\n",
    "import copy, random\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da59e303",
   "metadata": {},
   "source": [
    "# Synthetic Dataset Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d46dc53-d064-4c1f-8845-891216d77cde",
   "metadata": {},
   "source": [
    "## Parameters (User Input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e4c1bcf-a112-4a4f-9d02-a6724a2268b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "r is the proportion of training examples in the minority group, \n",
    "\n",
    "which means 1-r is proportion of examples in the majority group\n",
    "\n",
    "eta is the probability of flipping the label\n",
    "\n",
    "n is the number of training examples\n",
    "\n",
    "beta is the probability of keeping a positively labeled example\n",
    "from the minority class\n",
    "\n",
    "'''\n",
    "def get_params(r = 1/3, eta = 1/4, n = 2000, beta = 0.5):\n",
    "    return r, eta, n, beta\n",
    "\n",
    "r, eta, n, beta = get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270ebbd2-719c-42d5-8de4-dce388fc0a51",
   "metadata": {},
   "source": [
    "## True Label Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74f9b8a4-c293-4c8a-9c8c-72d28f624312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create minority and majority groups\n",
    "def get_cat_features(n, r):\n",
    "    num_minority = int(r * n)\n",
    "    num_majority = n - num_minority\n",
    "    \n",
    "    minority = np.zeros((num_minority, 1))\n",
    "    majority = np.ones((num_majority, 1))\n",
    "    \n",
    "    cat_features = np.vstack((minority, majority))\n",
    "    \n",
    "    # shuffle so as to ensure randomness\n",
    "    np.random.shuffle(cat_features)\n",
    "    \n",
    "    return cat_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46126312-7441-413f-a425-78649fe33c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate Bayes Optimal Classifiers for majority and minority groups\n",
    "def get_bayes_optimal_labels(df_majority, df_minority):\n",
    "    # format data\n",
    "    X_maj = df_majority.iloc[:, :-1].values\n",
    "    y_maj = df_majority.iloc[:, -1].values\n",
    "    \n",
    "    X_min = df_minority.iloc[:, :-1].values\n",
    "    y_min = df_minority.iloc[:, -1].values\n",
    "\n",
    "    classifier = LogisticRegression(random_state=42)\n",
    "    \n",
    "    # true labels\n",
    "    \n",
    "    classifier_majority = classifier.fit(X_maj, y_maj)\n",
    "    y_pred_maj = classifier_majority.predict(X_maj).reshape(len(X_maj), 1)\n",
    "    # print(\"Accuracy on Majority: \", accuracy_score(y_pred=y_pred_maj, y_true= y_maj))\n",
    "    \n",
    "    classifier_minority = classifier.fit(X_min, y_min)\n",
    "    y_pred_min = classifier_minority.predict(X_min).reshape(len(X_min), 1)\n",
    "    # print(\"Accuracy on Minority: \", accuracy_score(y_pred=y_pred_min, y_true= y_min))\n",
    "    \n",
    "    majority = np.hstack((X_maj, y_pred_maj))\n",
    "    minority = np.hstack((X_min, y_pred_min))\n",
    "    \n",
    "    total = np.vstack((majority, minority))\n",
    "    \n",
    "    # shuffle so as to ensure randomness\n",
    "    np.random.shuffle(total)\n",
    "    \n",
    "    df_true = pd.DataFrame(pd.DataFrame(total))\n",
    "    df_true.columns = ['num1','num2','num3','cat','outcome']\n",
    "    \n",
    "    return df_true\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "acba1061-c071-4a58-ae21-706122184dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flip labels with probability eta\n",
    "def flip_labels(df_synthetic, eta):\n",
    "    labels = df_synthetic['outcome']\n",
    "    \n",
    "    for i in range(len(labels)):\n",
    "        if random.uniform(0,1) <= eta:\n",
    "            labels[i] = 1 if labels[i] == 0 else 1\n",
    "    df_synthetic['outcome'] = labels\n",
    "    \n",
    "    return df_synthetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a643d5b1-14da-4b8d-a9ae-075fa85610b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure equal proportion of positive examples across minority and majority\n",
    "def equal_base_rates(df_majority, df_minority):\n",
    "    base_rate_maj = df_majority['outcome'].value_counts()[0] / len(df_majority)\n",
    "    base_rate_min = df_minority['outcome'].value_counts()[0] / len(df_minority)\n",
    "    \n",
    "    X_maj_pos = df_majority[df_majority['outcome'] == 1].iloc[:, :].values\n",
    "    X_maj_neg = df_majority[df_majority['outcome'] == 0].iloc[:, :].values\n",
    "    \n",
    "    diff = round(base_rate_maj,4) - round(base_rate_min,4)\n",
    "    \n",
    "    count = 0\n",
    "    \n",
    "    if diff > 0:\n",
    "        while(diff > 0.01):\n",
    "            X_maj_neg = np.delete(X_maj_neg, 0, axis = 0)\n",
    "\n",
    "            df_majority = pd.DataFrame(pd.DataFrame(np.vstack((X_maj_pos, X_maj_neg))))\n",
    "            df_majority.columns = ['num1','num2','num3','cat','outcome']\n",
    "\n",
    "            base_rate_maj = df_majority['outcome'].value_counts()[0] / len(df_majority)\n",
    "            # print(len(df_majority))\n",
    "            diff = round(base_rate_maj,4) - round(base_rate_min,4)\n",
    "            count+=1\n",
    "\n",
    "            # fail-safe\n",
    "            if count > int(len(df_majority)/3): break\n",
    "    else:\n",
    "        diff = round(base_rate_min,4) - round(base_rate_maj,4) \n",
    "        while(diff > 0.01):\n",
    "            X_maj_pos = np.delete(X_maj_pos, 0, axis = 0)\n",
    "\n",
    "            df_majority = pd.DataFrame(pd.DataFrame(np.vstack((X_maj_pos, X_maj_neg))))\n",
    "            df_majority.columns = ['num1','num2','num3','cat','outcome']\n",
    "\n",
    "            base_rate_maj = df_majority['outcome'].value_counts()[0] / len(df_majority)\n",
    "            diff = round(base_rate_min,4) - round(base_rate_maj,4) \n",
    "            count+=1\n",
    "\n",
    "            # fail-safe\n",
    "            if count > int(len(df_majority)/3): break\n",
    "                \n",
    "    total = np.vstack((df_majority, df_minority))\n",
    "    \n",
    "    # shuffle so as to ensure randomness\n",
    "    np.random.shuffle(total)\n",
    "                \n",
    "    df_true = pd.DataFrame(pd.DataFrame(total))\n",
    "    df_true.columns = ['num1','num2','num3','cat','outcome']\n",
    "    \n",
    "    return df_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9faebb17-ed24-453b-a179-16933179c612",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "create synthetic data with:\n",
    "    3 numerical features (Gaussian), 1 categorical (sensitive attribute) \n",
    "    logistic outcome model s.t. outcome = Indicator[logit(effect_param*features) >= 0.5]\n",
    "    \n",
    "create minority/majority groups according to r param\n",
    "\n",
    "simulate Bayes Optimal Classifiers for minority and majority\n",
    "\n",
    "flip labels according to eta param\n",
    "\n",
    "ensure equal base rates (proportion of positive examples) across both groups\n",
    "\n",
    "'''\n",
    "\n",
    "def true_label_generation(r, eta, n):\n",
    "\n",
    "    ''' \n",
    "    delete this variable to allow user to control percentage of positively labeled examples\n",
    "    eg: let outcome_continuous >= 0.2 implies 80% positively labeled samples\n",
    "    '''\n",
    "    effect_param = [0.5, -0.2, 0.1] # causal effect parameter (to create outcomes)\n",
    "\n",
    "    # required: len(cat_probabilities) = n_cat_features\n",
    "    n_cat_features = 2\n",
    "    cat_probabilities = [0.5, 0.5] \n",
    "\n",
    "    # numerical feature params\n",
    "    num_feature_mean = [0, 0, 0]\n",
    "    num_feature_cov = [[1, 0, 0], [0, 1, 0], [0, 0, 1]]\n",
    "\n",
    "    # features\n",
    "    num_features = np.random.multivariate_normal(num_feature_mean, num_feature_cov, n)\n",
    "    cat_features = get_cat_features(r=r, n=n)\n",
    "\n",
    "    # outcomes\n",
    "    outcome_continuous = 1/(1+np.exp(-np.matmul(num_features,effect_param))) # logit model + no added noise\n",
    "    outcome_binary = np.where(outcome_continuous >= 0.5 , 1, 0).reshape(n,1)\n",
    "    \n",
    "    df_synthetic = pd.DataFrame(pd.DataFrame(np.hstack((num_features,\n",
    "                                                        cat_features, outcome_binary))))\n",
    "    df_synthetic.columns = ['num1','num2','num3','cat','outcome']\n",
    "    \n",
    "    df_majority = df_synthetic[df_synthetic['cat'] == 1]\n",
    "    df_minority = df_synthetic[df_synthetic['cat'] == 0]\n",
    "    \n",
    "    df_synthetic = get_bayes_optimal_labels(df_majority, df_minority)\n",
    "    \n",
    "    df_synthetic = flip_labels(df_synthetic, eta)\n",
    "    \n",
    "    df_majority = df_synthetic[df_synthetic['cat'] == 1]\n",
    "    df_minority = df_synthetic[df_synthetic['cat'] == 0]\n",
    "    \n",
    "    df_synthetic = equal_base_rates(df_majority, df_minority)\n",
    "    \n",
    "    return df_synthetic \n",
    "\n",
    "df_synthetic = true_label_generation(r=1/3, eta=0.25, n=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a7fd87-924b-41b0-a505-25cac190e8cd",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d385bc3-625f-4726-a2b3-7dac659fda04",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "547edc7c-4c9b-4f47-8a61-9398fc20f79f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# split into train and test\n",
    "df_train = df_synthetic.loc[range(0,int(len(df_synthetic)/2)), :]\n",
    "df_test = df_synthetic.loc[range(int(len(df_synthetic)/2), len(df_synthetic)), :]\n",
    "\n",
    "# format data\n",
    "X_true = df_test.iloc[:, :-1].values\n",
    "y_true = df_test.iloc[:, -1].values\n",
    "\n",
    "sens_attrs_true = [df_train['cat']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950918b3",
   "metadata": {},
   "source": [
    "# Bias Injection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1a788ace-562a-4256-a853-40db73a61e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def under_sample(df_minority_positive, beta):\n",
    "    X_min = df_minority_positive.iloc[:, :].values\n",
    "    \n",
    "    # keep each example with probability beta\n",
    "    for i in range(len(X_min)):\n",
    "        if random.uniform(0,1) > beta:\n",
    "            X_min = np.delete(X_min, 0, axis=0)\n",
    "    \n",
    "    df_minority_positive = pd.DataFrame(pd.DataFrame(X_min))\n",
    "    df_minority_positive.columns = ['num1','num2','num3','cat','outcome']\n",
    "    return df_minority_positive\n",
    "\n",
    "def get_biased_data(df_train, beta):\n",
    "    df_majority = df_train[df_train['cat'] == 1]\n",
    "    df_minority = df_train[df_train['cat'] == 0]\n",
    "    \n",
    "    # unfavored group with negative label\n",
    "    df_minority_negative = df_minority[df_minority['outcome'] == 0.0]\n",
    "\n",
    "    # unfavored group with positive label (preferred)\n",
    "    df_minority_positive = df_minority[df_minority['outcome'] == 1.0]\n",
    "    \n",
    "    # data frame without positively labeled examples from minority class\n",
    "    df_total = pd.concat([df_majority, df_minority_negative])\n",
    "    \n",
    "    # under-sampling process\n",
    "    df_undersampled = under_sample(df_minority_positive, beta)\n",
    "\n",
    "    # combine undersampled and original favored class to create dataset\n",
    "    df_concat = pd.concat([df_total,df_undersampled])\n",
    "    \n",
    "    return df_concat\n",
    "\n",
    "df_concat = get_biased_data(df_train, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "144c9d4b-2b36-4f86-b2be-d0c8b5c442cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for fairness measures later\n",
    "df_sens = df_concat['cat']\n",
    "\n",
    "# format data\n",
    "X_bias = df_concat.iloc[:, :-1].values\n",
    "y_bias = df_concat.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84403b44",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f6827f",
   "metadata": {},
   "source": [
    "### Model Selection + Training (TODO: modularize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "72d79618",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T20:42:09.057618Z",
     "start_time": "2021-06-10T20:42:08.942574Z"
    }
   },
   "outputs": [],
   "source": [
    "# modularize and add data struct of different ml techniques\n",
    "classifier = LogisticRegression(random_state=42)\n",
    "\n",
    "# Synthetic Data\n",
    "classifier_true = classifier.fit(X_true, y_true)\n",
    "y_pred_truth = classifier_true.predict(X_true)\n",
    "\n",
    "classifier_bias = classifier.fit(X_bias, y_bias)\n",
    "y_pred_bias = classifier_bias.predict(X_bias)\n",
    "y_pred_bias_on_true = classifier_bias.predict(X_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff477ccd",
   "metadata": {},
   "source": [
    "### Model Performance (TODO: modularize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "adf0a2b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T20:42:09.231352Z",
     "start_time": "2021-06-10T20:42:09.225108Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synthetic Data\n",
      "\n",
      "Accuracy of Ground Truth Model on Ground Truth Data:  0.802\n",
      "Accuracy of Biased Model on Biased Data:  0.8358038768529077\n",
      "Accuracy of Biased Model on Ground Truth Data:  0.817\n"
     ]
    }
   ],
   "source": [
    "print(\"Synthetic Data\\n\")\n",
    "\n",
    "print(\"Accuracy of Ground Truth Model on Ground Truth Data: \", accuracy_score(y_pred_truth, y_true))\n",
    "print(\"Accuracy of Biased Model on Biased Data: \", accuracy_score(y_pred_bias, y_bias))\n",
    "print(\"Accuracy of Biased Model on Ground Truth Data: \", accuracy_score(y_pred_bias_on_true, y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a9c4cdf1-b8b4-4674-bce4-9104ba87ae46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy:  0.30851063829787234\n",
      "Group Accuracy :  cat\n",
      "0.0    0.279661\n",
      "1.0    0.321705\n",
      "Name: false_positive_rate, dtype: object\n",
      "\n",
      "\n",
      "Overall True Positive Rate:  0.8685897435897436\n",
      "Group True Positive Rate :  cat\n",
      "0.0    0.846154\n",
      "1.0    0.882051\n",
      "Name: true_positive_rate, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Ground Truth Model on Ground Truth Data\n",
    "\n",
    "fpr_true = MetricFrame(false_positive_rate, y_true, y_pred_truth, sensitive_features = sens_attrs_true[0])\n",
    "print(\"Overall Accuracy: \", fpr_true.overall)\n",
    "print(\"Group Accuracy : \", fpr_true.by_group)\n",
    "\n",
    "print(\"\\n\")\n",
    "fnr_true = MetricFrame(true_positive_rate, y_true, y_pred_truth, sensitive_features = sens_attrs_true[0])\n",
    "print(\"Overall True Positive Rate: \", fnr_true.overall)\n",
    "print(\"Group True Positive Rate : \", fnr_true.by_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c85b935",
   "metadata": {},
   "source": [
    "# Fairness Intervention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3db7ace8-9024-47f7-9215-88463035949a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairlearn.reductions import ExponentiatedGradient, DemographicParity, EqualizedOdds\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e05fce6-7d80-41c1-b57b-0475438601af",
   "metadata": {},
   "source": [
    "#### Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "eb52a316-c5be-4c96-a72c-a88ba6b0d5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "constraint = EqualizedOdds()\n",
    "mitigator_true = ExponentiatedGradient(classifier_true, constraint)\n",
    "mitigator_true.fit(X_true, y_true, sensitive_features = sens_attrs_true[0])\n",
    "y_pred_mitigated_true = mitigator_true.predict(X_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "cb4407ad-9b58-4b9d-8c49-f4b4158e6ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "constraint = EqualizedOdds()\n",
    "mitigator_bias = ExponentiatedGradient(classifier_bias, constraint)\n",
    "mitigator_bias.fit(X_bias, y_bias, sensitive_features = df_sens)\n",
    "y_pred_mitigated_bias = mitigator_bias.predict(X_bias)\n",
    "y_pred_mitigated_bias_on_true = mitigator_bias.predict(X_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7ea20d",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "40d85690-7232-405f-9256-d987abd01b22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synthetic Data\n",
      "\n",
      "Accuracy of Ground Truth Model + Fairness Intervention on Ground Truth Data:  0.757\n",
      "Accuracy of Biased Model + Fairness Intervention on Ground Truth Data:  0.812\n"
     ]
    }
   ],
   "source": [
    "print(\"Synthetic Data\\n\")\n",
    "\n",
    "print(\"Accuracy of Ground Truth Model + Fairness Intervention on Ground Truth Data: \",\n",
    "      accuracy_score(y_pred_mitigated_true, y_true))\n",
    "\n",
    "print(\"Accuracy of Biased Model + Fairness Intervention on Ground Truth Data: \",\n",
    "      accuracy_score(y_pred_mitigated_bias_on_true, y_true))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e393d0-149b-4da6-91ba-e74e5225b0e2",
   "metadata": {},
   "source": [
    "### Bias vs Accuracy vs Fairness Trade-Off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9021a6d5-19ad-4c05-b165-497c84360069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if verbose, shows \"Finished iteration: ... \"\n",
    "# if apply_fairness, uses fairness intervention\n",
    "def tradeoff_visualization(classifier, X_true, y_true, df_favored,\n",
    "                           df_unfavored_positive, df_unfavored_negative,\n",
    "                           sensitive_feature = \"cat\",\n",
    "                           label = \"outcome\", cat_cols = [],\n",
    "                           apply_fairness = False, verbose = False):\n",
    "    \n",
    "    bias_amts = list(range(0,30))\n",
    "    accuracy_on_true = []\n",
    "    accuracy_on_biased = []\n",
    "    accuracy_on_true_mitigated = []\n",
    "    accuracy_on_biased_mitigated = []\n",
    "    \n",
    "    dataset_size_true = np.full(shape=len(bias_amts), fill_value= X_true.shape[0]).tolist()\n",
    "    dataset_size_bias = []\n",
    "\n",
    "    df_undersampled = df_unfavored_positive.sample(n=len(df_unfavored_positive), random_state=42)\n",
    "\n",
    "    for i in range(30):\n",
    "        # under-sampling process\n",
    "        if i != 0:\n",
    "            df_undersampled = df_undersampled.sample(n=int(len(df_undersampled)*0.9), random_state=42)\n",
    "\n",
    "        # combine undersampled and original favored class to create dataset\n",
    "        df_concat = pd.concat([df_favored, df_unfavored_negative, df_undersampled])\n",
    "        df_sens = df_concat[sensitive_feature]\n",
    "\n",
    "        # format data\n",
    "        X_bias = df_concat.iloc[:, :-2].values\n",
    "        y_bias = df_concat.iloc[:, -1].values\n",
    "\n",
    "        X_bias_true = df_concat.iloc[:, :-1].values\n",
    "        y_bias_true = df_concat.iloc[:, -1].values\n",
    "\n",
    "        dataset_size_bias.append(X_bias_true.shape[0])\n",
    "        classifier_bias = classifier.fit(X_bias_true, y_bias_true)\n",
    "        \n",
    "        if apply_fairness:\n",
    "            constraint = EqualizedOdds()\n",
    "            classifier_mitigated_bias = ExponentiatedGradient(classifier_bias, constraint)\n",
    "            classifier_mitigated_bias.fit(X_bias_true, y_bias_true, sensitive_features = df_sens)\n",
    "            \n",
    "            # testing on biased data WITH fairness intervention\n",
    "            y_pred_mitigated_bias = classifier_mitigated_bias.predict(X_bias_true)\n",
    "            \n",
    "            # testing on GT data WITH fairness intervention\n",
    "            y_pred_mitigated_bias_on_true = classifier_mitigated_bias.predict(X_true)\n",
    "        \n",
    "        # testing on biased data withOUT fairness intervention\n",
    "        y_pred_bias = classifier_bias.predict(X_bias_true)\n",
    "        \n",
    "        # testing on GT data withOUT fairness intervention\n",
    "        y_pred_bias_on_true = classifier_bias.predict(X_true)\n",
    "\n",
    "        # model performance\n",
    "        \n",
    "        if apply_fairness:\n",
    "            # on biased data\n",
    "            acc_bias_mitigated = accuracy_score(y_pred=y_pred_mitigated_bias, y_true=y_bias_true)\n",
    "            accuracy_on_biased_mitigated.append(acc_bias_mitigated)\n",
    "            \n",
    "            # on GT data\n",
    "            acc_bias_mitigated_on_true = accuracy_score(y_pred=y_pred_mitigated_bias_on_true, y_true=y_true)\n",
    "            accuracy_on_true_mitigated.append(acc_bias_mitigated_on_true)\n",
    "        \n",
    "        # on biased data\n",
    "        acc_bias = accuracy_score(y_pred=y_pred_bias, y_true=y_bias_true)\n",
    "        accuracy_on_biased.append(acc_bias)\n",
    "        # on GT data\n",
    "        acc_bias_on_true = accuracy_score(y_pred=y_pred_bias_on_true, y_true=y_true)\n",
    "        accuracy_on_true.append(acc_bias_on_true)\n",
    "        \n",
    "        '''\n",
    "        # fairness performance (TODO)\n",
    "        eod_true = equalized_odds_difference(y_true=y_bias_true, y_pred = y_pred_bias, sensitive_features=df_sens)\n",
    "        eod_on_true.append(eod_true)\n",
    "\n",
    "        eod_bias_on_true = equalized_odds_difference(y_true=y_true,\\\n",
    "        y_pred = y_pred_bias_on_true, sensitive_features=sens_attrs[1])\n",
    "        eod_on_biased.append(eod_bias_on_true)\n",
    "\n",
    "        # table visualization \n",
    "        table_elem = [i*10, acc_bias, acc_bias_on_true]\n",
    "        table.append(table_elem)\n",
    "        '''\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"Finished Iteration: \", i)\n",
    "\n",
    "    return bias_amts, dataset_size_true, dataset_size_bias, accuracy_on_biased,\\\n",
    "           accuracy_on_true, accuracy_on_biased_mitigated, accuracy_on_true_mitigated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bfd23f-6f40-45d1-a77f-e93d9b460459",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_visualizations(bias_amts, dataset_size_true, dataset_size_bias,\n",
    "                            accuracy_on_biased = [], accuracy_on_true = [],\n",
    "                            accuracy_on_biased_mitigated = [],\n",
    "                            accuracy_on_true_mitigated = [], fairness = False):\n",
    "    \n",
    "    if fairness:\n",
    "        plt.figure(figsize=(17,7))\n",
    "\n",
    "        plt.subplot(1,2,1)\n",
    "        plt.plot(bias_amts, accuracy_on_true_mitigated, label = 'Ground Truth')\n",
    "        plt.plot(bias_amts, accuracy_on_biased_mitigated, label = 'Biased Data')\n",
    "        plt.xlabel(\"Number of iterations of removing 10% of positively labeled minority samples\")\n",
    "        plt.ylabel(\"Accuracy Score\")\n",
    "        plt.title(\"Biased Model Accuracy\")\n",
    "        #plt.ylim(0.97, 0.99)\n",
    "        plt.legend()\n",
    "\n",
    "        plt.subplot(1,2,2)\n",
    "        plt.plot(bias_amts, dataset_size_true, label = 'Ground Truth')\n",
    "        plt.plot(bias_amts, dataset_size_bias, label = 'Biased Data')\n",
    "        plt.xlabel(\"Number of iterations of removing 10% of positively labeled minority samples\")\n",
    "        plt.ylabel(\"Dataset Size\")\n",
    "        plt.legend()\n",
    "\n",
    "        plt.show()\n",
    "        \n",
    "    else:\n",
    "        plt.figure(figsize=(17,7))\n",
    "\n",
    "        plt.subplot(1,2,1)\n",
    "        plt.plot(bias_amts, accuracy_on_true, label = 'Ground Truth')\n",
    "        plt.plot(bias_amts, accuracy_on_biased, label = 'Biased Data')\n",
    "        plt.xlabel(\"Number of iterations of removing 10% of positively labeled minority samples\")\n",
    "        plt.ylabel(\"Accuracy Score\")\n",
    "        plt.title(\"Biased Model Accuracy\")\n",
    "        #plt.ylim(0.97, 0.99)\n",
    "        plt.legend()\n",
    "\n",
    "        plt.subplot(1,2,2)\n",
    "        plt.plot(bias_amts, dataset_size_true, label = 'Ground Truth')\n",
    "        plt.plot(bias_amts, dataset_size_bias, label = 'Biased Data')\n",
    "        plt.xlabel(\"Number of iterations of removing 10% of positively labeled minority samples\")\n",
    "        plt.ylabel(\"Dataset Size\")\n",
    "        plt.legend()\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d884d65-9fac-4e54-aafa-d3fc41b627b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_visualizations(bias_amts, accuracy_on_biased, accuracy_on_true,\n",
    "                        accuracy_on_biased_mitigated, accuracy_on_true_mitigated,\n",
    "                        dataset_size_true, dataset_size_bias):\n",
    "    plt.figure(figsize=(17,7))\n",
    "\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(bias_amts, accuracy_on_biased, label = 'Tested On Biased Data + No Fairness Intervention', color = \"red\")\n",
    "    plt.plot(bias_amts, accuracy_on_biased_mitigated, label = 'Tested On Biased Data + Fairness Intervention', color = \"green\")\n",
    "    plt.plot(bias_amts, accuracy_on_true, label = 'Tested On Ground Truth + No Fairness Intervention', color = \"blue\")\n",
    "    plt.plot(bias_amts, accuracy_on_true_mitigated, label = 'Tested On Ground Truth + Fairness Intervention', color = \"purple\")\n",
    "    plt.xlabel(\"Number of iterations of removing 10% of positively labeled minority samples\")\n",
    "    plt.ylabel(\"Accuracy Score\")\n",
    "    #plt.axhline(y=accuracy_score(y_pred_truth, y_true), color = \"green\", label = \"Ground Truth Model On Ground Truth Data\", alpha = 0.5)\n",
    "    plt.title(\"Accuracy of Biased Model (trained on biased data)\")\n",
    "    plt.legend(loc = 3)\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(bias_amts, dataset_size_true, label = 'Ground Truth')\n",
    "    plt.plot(bias_amts, dataset_size_bias, label = 'Biased Data')\n",
    "    plt.xlabel(\"Number of iterations of removing 10% of positively labeled minority samples\")\n",
    "    plt.ylabel(\"Dataset Size\")\n",
    "    plt.title(\"Amount of Minority Samples Removed vs Dataset Size\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583bb578-ffa4-4ff5-bfe1-1ab4496c1e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = LogisticRegression()\n",
    "\n",
    "bias_amts, dataset_size_true, dataset_size_bias, \\\n",
    "accuracy_on_biased, accuracy_on_true, accuracy_on_biased_mitigated, accuracy_on_true_mitigated = \\\n",
    "tradeoff_visualization(classifier, X_true=X_syn_true, y_true=y_syn_true, \\\n",
    "                       df_favored=df_favored_syn, df_unfavored_positive= df_unfavored_syn_positive,\\\n",
    "                       df_unfavored_negative= df_unfavored_syn_negative, sensitive_feature=\"cat\",\n",
    "                       cat_cols=[3], label = \"outcome\",\n",
    "                       apply_fairness=True,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45a365a-fefe-4be6-9a10-7e465ea93d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# without fairness intervention\n",
    "accuracy_visualizations(bias_amts, dataset_size_true, dataset_size_bias, accuracy_on_biased, accuracy_on_true, accuracy_on_biased_mitigated, accuracy_on_true_mitigated, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ec3103-ef16-4507-b188-75202083401a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with fairness intervention\n",
    "accuracy_visualizations(bias_amts, dataset_size_true, dataset_size_bias, accuracy_on_biased, accuracy_on_true, accuracy_on_biased_mitigated, accuracy_on_true_mitigated, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f95dca7-da5e-45fe-be62-0edc4e3164ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_visualizations(bias_amts, accuracy_on_biased, accuracy_on_true,\n",
    "                    accuracy_on_biased_mitigated, accuracy_on_true_mitigated,\n",
    "                    dataset_size_true, dataset_size_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75149e2-7bd5-4a3d-982d-984c4841bcfb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
