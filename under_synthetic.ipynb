{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5e91243",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-08T20:41:26.879421Z",
     "start_time": "2021-06-08T20:41:26.876140Z"
    }
   },
   "source": [
    "# Under-Representation Bias (w/ Synthetic Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00909876",
   "metadata": {},
   "source": [
    "This notebook recreates the finding that Equalized Odds constrained model can recover from under-representation bias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a42ced1",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "Please run the code block below to install the necessary packages (if needed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4a692724",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T20:42:02.586362Z",
     "start_time": "2021-06-10T20:42:02.581026Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "from sklearn.base import clone\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, roc_curve, auc\n",
    "from collections import Counter\n",
    "\n",
    "import fairlearn\n",
    "from fairlearn.metrics import *\n",
    "from fairlearn.reductions import *\n",
    "import aif360\n",
    "\n",
    "import copy, random\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da59e303",
   "metadata": {},
   "source": [
    "# Synthetic Dataset Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d46dc53-d064-4c1f-8845-891216d77cde",
   "metadata": {},
   "source": [
    "## Parameters (User Input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0e4c1bcf-a112-4a4f-9d02-a6724a2268b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "r is the proportion of training examples in the minority group, \n",
    "\n",
    "which means 1-r is proportion of examples in the majority group\n",
    "\n",
    "eta is the probability of flipping the label\n",
    "\n",
    "n is the number of training examples\n",
    "\n",
    "beta is the probability of keeping a positively labeled example\n",
    "from the minority class\n",
    "\n",
    "NOTE: results can be replicated if and only if the following condition holds:\n",
    "\n",
    "(1-r)(1-2*eta) + r((1-eta)*beta - eta) > 0\n",
    "\n",
    "'''\n",
    "def get_params(r = 1/3, eta = 1/4, n = 2000, beta = 0.5):\n",
    "    return r, eta, n, beta\n",
    "\n",
    "r, eta, n, beta = get_params(r = 1/3, eta = 0, n = 30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "8e7d3c98-54c3-4198-9f03-528bebb016cc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constraint:  0.19999999999999996\n",
      "yes! 0.2 0.4 1.0\n",
      "constraint:  0.18799999999999997\n",
      "yes! 0.2 0.4 0.9\n",
      "constraint:  0.17599999999999996\n",
      "yes! 0.2 0.4 0.8\n",
      "constraint:  0.16399999999999998\n",
      "yes! 0.2 0.4 0.7\n",
      "constraint:  0.15199999999999997\n",
      "yes! 0.2 0.4 0.6\n",
      "constraint:  0.13999999999999996\n",
      "yes! 0.2 0.4 0.5\n",
      "constraint:  0.12799999999999997\n",
      "yes! 0.2 0.4 0.4\n",
      "constraint:  0.11599999999999996\n",
      "yes! 0.2 0.4 0.3\n",
      "constraint:  0.10399999999999997\n",
      "yes! 0.2 0.4 0.2\n",
      "constraint:  0.09199999999999997\n",
      "yes! 0.2 0.4 0.1\n",
      "constraint:  0.07999999999999996\n",
      "yes! 0.2 0.4 0.0\n"
     ]
    }
   ],
   "source": [
    "# check if above constraint holds\n",
    "def check_constraints(r, eta, beta):\n",
    "    first = (1-r)*(1-2*eta)\n",
    "    second = r * ((1-eta)*beta - eta)\n",
    "    res = first + second\n",
    "    print(\"constraint: \", res)\n",
    "    print(\"yes!\", r, eta, beta) if res > 0 else print(\"no\", r, eta, beta)\n",
    "    \n",
    "bias_amts = np.divide(list(range(10, -1, -1)),10)\n",
    "\n",
    "for beta in bias_amts:\n",
    "    check_constraints(r=0.2, eta=0.4, beta=beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270ebbd2-719c-42d5-8de4-dce388fc0a51",
   "metadata": {},
   "source": [
    "## True Label Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "74f9b8a4-c293-4c8a-9c8c-72d28f624312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create minority and majority groups\n",
    "def get_cat_features(n, r):\n",
    "    num_minority = int(r * n)\n",
    "    num_majority = n - num_minority\n",
    "    \n",
    "    minority = np.zeros((num_minority, 1))\n",
    "    majority = np.ones((num_majority, 1))\n",
    "    \n",
    "    cat_features = np.vstack((minority, majority))\n",
    "    #np.random.shuffle(cat_features) # this is what causes us to not recover coeffs\n",
    "    \n",
    "    return cat_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "acba1061-c071-4a58-ae21-706122184dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# return labels from Bayes Optimal Classifier\n",
    "def get_bayes_optimal_labels(features, effect_param, threshold = 0.5):\n",
    "    outcome_continuous = 1/(1+np.exp(-np.matmul(features, effect_param)))\n",
    "    #return outcome_continuous, np.random.binomial(1,outcome_continuous) # bernoulli to simulate LR's probabilistic nature\n",
    "    return outcome_continuous, np.where(outcome_continuous < threshold, 0, 1)\n",
    "\n",
    "# flip labels with probability eta\n",
    "def flip_labels(df_synthetic, eta):\n",
    "    labels = df_synthetic['outcome']\n",
    "    #print('Before:', df_synthetic['outcome'].value_counts())\n",
    "    num_flipped = 0\n",
    "    for i in range(len(labels)):\n",
    "        if random.uniform(0,1) <= eta:\n",
    "            labels[i] = 1 if labels[i] == 0 else 0\n",
    "            num_flipped += 1\n",
    "    df_synthetic['outcome'] = labels\n",
    "    #print('After:', df_synthetic['outcome'].value_counts())\n",
    "    #print('Num Flipped: ', num_flipped, \"\\tRate: \", num_flipped / len(df_synthetic))\n",
    "    return df_synthetic\n",
    "\n",
    "def flip_labels2(df_majority, eta_maj, df_minority, eta_min):\n",
    "    labels_maj = df_majority['outcome'].values\n",
    "    labels_min = df_minority['outcome'].values\n",
    "    \n",
    "    num_flipped_maj = 0\n",
    "    for i in range(len(labels_maj)):\n",
    "        if random.uniform(0,1) <= eta_maj:\n",
    "            labels_maj[i] = 1 if labels_maj[i] == 0 else 0\n",
    "            num_flipped_maj += 1\n",
    "    df_majority['outcome'] = labels_maj\n",
    "    print('Num Flipped Maj: ', num_flipped_maj, \"\\tRate: \", num_flipped_maj / len(df_majority))\n",
    "    \n",
    "    num_flipped_min = 0\n",
    "    for i in range(len(labels_min)):\n",
    "        if random.uniform(0,1) <= eta_min:\n",
    "            labels_min[i] = 1 if labels_min[i] == 0 else 0\n",
    "            num_flipped_min += 1\n",
    "    df_minority['outcome'] = labels_min\n",
    "    print('Num Flipped Min: ', num_flipped_min, \"\\tRate: \", num_flipped_min / len(df_minority))\n",
    "    \n",
    "    df_concat = pd.concat([df_majority, df_minority])\n",
    "    return df_concat.sample(frac=1, random_state = 42) # permute data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9faebb17-ed24-453b-a179-16933179c612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diff:  -9.999999999998899e-05\n"
     ]
    }
   ],
   "source": [
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "'''\n",
    "\n",
    "create synthetic data with:\n",
    "    3 numerical features (Gaussian), 1 categorical (sensitive attribute) \n",
    "    logistic outcome model s.t. outcome = Indicator[logit(effect_param*features) >= 0.5]\n",
    "    \n",
    "create minority/majority groups according to r param\n",
    "\n",
    "simulate Bayes Optimal Classifiers for minority and majority\n",
    "\n",
    "flip labels according to eta param\n",
    "\n",
    "ensure equal base rates (proportion of positive examples) across both groups\n",
    "\n",
    "'''\n",
    "\n",
    "def true_label_generation(r, eta, n, maj_means = [0,0,0]):\n",
    "\n",
    "    ''' \n",
    "    delete this variable to allow user to control percentage of positively labeled examples\n",
    "    eg: let outcome_continuous >= 0.2 implies 80% positively labeled samples\n",
    "    '''\n",
    "    # causal effect params\n",
    "    maj_params = [-0.7, 0.5, 1.5]\n",
    "    effect_param_min = [0.5, -0.2, 0.1]\n",
    "    #effect_param_maj = [i + np.random.uniform(low = -1, high = 1) for i in maj_params]\n",
    "    effect_param_maj = maj_params\n",
    "    \n",
    "    num_min = int(n*r)\n",
    "    num_maj = n - num_min\n",
    "\n",
    "    # required: len(cat_probabilities) = n_cat_features\n",
    "    n_cat_features = 2\n",
    "    cat_probabilities = [0.5, 0.5] \n",
    "\n",
    "    # numerical feature params\n",
    "    means = [0, 0, 0]\n",
    "    cov_matrix = [[1, 0, 0], [0, 1, 0], [0, 0, 1]]\n",
    "\n",
    "    # features\n",
    "    cat_features = get_cat_features(r=r, n=n)\n",
    "    \n",
    "    num_features_min = np.random.multivariate_normal(means, cov_matrix, num_min)\n",
    "    #num_features_min = np.random.normal(0, 1, num_min)\n",
    "    num_features_maj = np.random.multivariate_normal(maj_means, cov_matrix, num_maj)\n",
    "    # num_features_maj = np.array([i + np.random.uniform(low = -15, high = 10) for i in num_features_maj])\n",
    "    #num_features_min = np.random.normal(0, 1, num_min)\n",
    "\n",
    "    num_features = np.concatenate((num_features_min, num_features_maj))\n",
    "\n",
    "    # outcomes\n",
    "    outcome_continuous_min, outcome_binary_min = get_bayes_optimal_labels(features=num_features_min, effect_param=effect_param_min, threshold = 0.5)\n",
    "    #outcome_binary_min = np.where(np.matmul(num_features_min, effect_param_min) > 0.5, 1, 0)\n",
    "    outcome_continuous_maj, outcome_binary_maj = get_bayes_optimal_labels(features=num_features_maj, effect_param=effect_param_maj, threshold = 0.5)\n",
    "    #outcome_binary_maj = np.where(np.matmul(num_features_maj, effect_param_maj) > 0.5, 1, 0)\n",
    "    \n",
    "    outcome = np.hstack((outcome_binary_min,outcome_binary_maj)).reshape(n,1)\n",
    "    outcome_continuous = np.hstack((outcome_continuous_min,outcome_continuous_maj)).reshape(n,1)\n",
    "    temp_data = np.hstack((num_features,cat_features, outcome, outcome_continuous))\n",
    "    #print(outcome_continuous)\n",
    "    #print(np.where(outcome_continuous < 0.5, 0, 1))\n",
    "    np.random.shuffle(temp_data) # randomly shuffle the data\n",
    "    \n",
    "    df_synthetic = pd.DataFrame(temp_data)\n",
    "    df_synthetic.columns = ['num1','num2','num3','cat','outcome','outcome_cont']\n",
    "    \n",
    "    outcome_continuous = df_synthetic.outcome_cont\n",
    "    df_synthetic = df_synthetic[['num1','num2','num3','cat','outcome']]\n",
    "    \n",
    "    df_majority = df_synthetic[df_synthetic['cat'] == 1]\n",
    "    df_minority = df_synthetic[df_synthetic['cat'] == 0]\n",
    "    \n",
    "    #print('% Positive Majority: ', df_majority['outcome'].value_counts()[1] / len(df_majority))\n",
    "    #print('\\n% Positive Minority: ', df_minority['outcome'].value_counts()[1] / len(df_minority))\n",
    "    print('Diff: ', df_majority['outcome'].value_counts()[1] / len(df_majority)- df_minority['outcome'].value_counts()[1] / len(df_minority))\n",
    "    #print('\\nTotal: ', df_majority['outcome'].value_counts())\n",
    "    \n",
    "    df_synthetic = flip_labels(df_synthetic, eta)\n",
    "    \n",
    "    # uncomment below to add diff eta for majority/minority\n",
    "    #df_synthetic = flip_labels2(df_majority, 0.2, df_minority, eta) # group dependent label noise\n",
    "    \n",
    "    return outcome_continuous, df_synthetic \n",
    "\n",
    "outcome_continuous, df_synthetic = true_label_generation(r=r, eta=eta, n=2*n, maj_means = [0,0,0])\n",
    "#xx = np.where(outcome_continuous < 0.5, 0, 1)\n",
    "#print(eta)\n",
    "#print(accuracy_score(xx,df_synthetic.outcome.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a7fd87-924b-41b0-a505-25cac190e8cd",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d385bc3-625f-4726-a2b3-7dac659fda04",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "240d919e-9d87-4429-9f42-e4f2a938fac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and test\n",
    "df_train = df_synthetic.loc[range(0,int(n/2)), :]\n",
    "# if original dataset has odd number of samples, remove 1 sample to be even\n",
    "if (n % 2 == 1):\n",
    "    df_test = df_synthetic.loc[range(int(n/2)+1, n), :]\n",
    "else:\n",
    "    df_test = df_synthetic.loc[range(int(n/2), n), :]\n",
    "\n",
    "df_fidel = df_synthetic.loc[range(n, len(df_synthetic)),:]\n",
    "outcome_cts = outcome_continuous[n:len(df_synthetic)]\n",
    "    \n",
    "df_test_maj = df_test[df_test['cat'] == 1]\n",
    "df_test_min = df_test[df_test['cat'] == 0]\n",
    "\n",
    "# format data\n",
    "X_true = df_test.iloc[:, :-1].values\n",
    "y_true = df_test.iloc[:, -1].values\n",
    "\n",
    "X_true_maj = df_test_maj.iloc[:, :-1].values\n",
    "y_true_maj = df_test_maj.iloc[:, -1].values\n",
    "X_true_min = df_test_min.iloc[:, :-1].values\n",
    "y_true_min = df_test_min.iloc[:, -1].values\n",
    "\n",
    "sens_attrs_true = [df_test['cat']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950918b3",
   "metadata": {},
   "source": [
    "# Bias Injection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c60f7195-a5e7-46d4-9308-65ae9c5c8ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# measurement bias\n",
    "def inject_noise_num(df, feature, eps = 1):\n",
    "    for i in range(len(df[feature])):\n",
    "        df[feature].iloc[i] += np.random.normal(0, 1) * eps # standard normal\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "1a788ace-562a-4256-a853-40db73a61e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attribute': 'num1', 'min': -3.8192251856023876, 'max': 3.9666915723851903, '1st Quartile': -0.6817482352027133, '2nd Quartile': 0.0008528779581552966, '3rd Quartile': 0.6660929378389033}\n",
      "{'attribute': 'num1', 'min': -3.8192251856023876, 'max': 3.9666915723851903, '1st Quartile': -0.754584932140913, '2nd Quartile': -0.08391533535725751, '3rd Quartile': 0.5890338064548117}\n"
     ]
    }
   ],
   "source": [
    "from numpy import percentile\n",
    "def under_sample(df_minority_positive, beta):\n",
    "    X_min = df_minority_positive.iloc[:, :].values\n",
    "    \n",
    "    # keep each example with probability beta\n",
    "    num_dropped = 0\n",
    "    for i in range(len(X_min)):\n",
    "        if random.uniform(0,1) > beta:\n",
    "            X_min = np.delete(X_min, 0, axis=0)\n",
    "            num_dropped += 1\n",
    "    #print(\"Total Deleted: \", num_dropped, \"\\t % Deleted: \", num_dropped / len(df_minority_positive))\n",
    "    df_minority_positive = pd.DataFrame(pd.DataFrame(X_min))\n",
    "    df_minority_positive.columns = ['num1','num2','num3','cat','outcome']\n",
    "    return df_minority_positive\n",
    "\n",
    "\n",
    "def get_biased_data(df_train, beta):\n",
    "    df_majority = df_train[df_train['cat'] == 1]\n",
    "    df_minority = df_train[df_train['cat'] == 0]\n",
    "    \n",
    "    # unfavored group with negative label\n",
    "    df_minority_negative = df_minority[df_minority['outcome'] == 0.0]\n",
    "\n",
    "    # unfavored group with positive label (preferred)\n",
    "    df_minority_positive = df_minority[df_minority['outcome'] == 1.0]\n",
    "    \n",
    "    # data frame without positively labeled examples from minority class\n",
    "    df_total = pd.concat([df_majority, df_minority_negative])\n",
    "    \n",
    "    # under-sampling process\n",
    "    df_undersampled = under_sample(df_minority_positive, beta)\n",
    "\n",
    "    # combine undersampled and original favored class to create dataset\n",
    "    df_concat = pd.concat([df_total,df_undersampled])\n",
    "    \n",
    "    return df_concat.sample(frac=1) # permute data\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "# representation bias, under-sampling entire minority\n",
    "def get_biased_data(df_train, beta):\n",
    "    df_majority = df_train[df_train['cat'] == 1]\n",
    "    df_minority = df_train[df_train['cat'] == 0]\n",
    "    \n",
    "    # under-sampling process\n",
    "    df_undersampled = under_sample(df_minority, beta)\n",
    "\n",
    "    # combine undersampled and original favored class to create dataset\n",
    "    df_concat = pd.concat([df_majority,df_undersampled])\n",
    "    \n",
    "    return df_concat.sample(frac=1) # permute data\n",
    "'''\n",
    "\n",
    "def get_summary_num(df, feature):\n",
    "    res = dict()\n",
    "    res['attribute'] = feature\n",
    "\n",
    "    data_min, data_max = df[feature].min(), df[feature].max()\n",
    "    res['min'] = data_min\n",
    "    res['max'] = data_max\n",
    "\n",
    "    quartiles = percentile(df[feature], [25,50,75])\n",
    "    res['1st Quartile'] = quartiles[0]\n",
    "    res['2nd Quartile'] = quartiles[1]\n",
    "    res['3rd Quartile'] = quartiles[2]\n",
    "\n",
    "    return res\n",
    "\n",
    "#print(len(df_train[(df_train['cat'] == 0) & (df_train['outcome'] == 1)]))\n",
    "print(get_summary_num(df_train, 'num1'))\n",
    "df_concat = get_biased_data(df_train, 0.5)\n",
    "print(get_summary_num(df_concat, 'num1'))\n",
    "#print(len(df_concat[(df_concat['cat'] == 0) & (df_concat['outcome'] == 1)]))\n",
    "\n",
    "# for fairness measures later\n",
    "df_sens = df_concat['cat']\n",
    "\n",
    "# format data\n",
    "X_bias = df_concat.iloc[:, :-1].values\n",
    "y_bias = df_concat.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "213427a7-8fec-4a2a-a568-439e01b6b31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Nil-Jana's Suggestions '''\n",
    "\n",
    "def transform(df, is_test = False, is_train = False):\n",
    "\n",
    "    sens_feat = df.iloc[:, -2].values\n",
    "    outcome = df.iloc[:, -1].values\n",
    "    num_feats = df.iloc[:, :-2].values\n",
    "\n",
    "    trans_feats = [] # x1a - x3a\n",
    "    other_feats = [] # x1(1-a) - x3(1-a)\n",
    "    # -2 for sensitive feature and label\n",
    "    for i in range(len(df.columns) - 2):\n",
    "        num_feat = df.iloc[:, i].values\n",
    "        num_feat_transf = np.multiply(num_feat, sens_feat)\n",
    "        trans_feats += [num_feat_transf.reshape((len(df),))]\n",
    "        \n",
    "        num_feat_other = df.iloc[:, i].values\n",
    "        num_feat_other_transf = np.multiply(num_feat, (1-sens_feat))\n",
    "        other_feats += [num_feat_other_transf.reshape((len(df),))]\n",
    "\n",
    "\n",
    "    temp_data = np.hstack((other_feats[0].reshape((len(df),1)), other_feats[1].reshape((len(df),1)), other_feats[2].reshape((len(df),1)),\n",
    "                           trans_feats[0].reshape((len(df),1)), trans_feats[1].reshape((len(df),1)), trans_feats[2].reshape((len(df),1)),\n",
    "                           outcome.reshape((len(df),1))))\n",
    "\n",
    "    df_transf = pd.DataFrame(temp_data)\n",
    "    df_transf.columns = ['num1*(1-a)','num2*(1-a)','num3*(1-a)', 'num1*a','num2*a','num3*a','outcome']\n",
    "\n",
    "    # for fairness measures later\n",
    "    df_sens = df['cat']\n",
    "    maj_list = list(df[df['cat'] == 1].index)\n",
    "    min_list = list(df[df['cat'] == 0].index)\n",
    "    \n",
    "    for i in range(len(maj_list)):\n",
    "        if is_train: maj_list[i] = maj_list[i]\n",
    "        else: maj_list[i] = maj_list[i] - len(df)\n",
    "        \n",
    "    for i in range(len(min_list)):\n",
    "        if is_train:\n",
    "            min_list[i] = min_list[i]\n",
    "        else: min_list[i] = min_list[i] - len(df)\n",
    "\n",
    "    # format data\n",
    "    X_bias = df_transf.iloc[:, :-1].values\n",
    "    y_bias = df_transf.iloc[:, -1].values\n",
    "    \n",
    "    if not is_test:\n",
    "        return X_bias, y_bias, df_sens\n",
    "    else:\n",
    "        return df_transf, maj_list, min_list\n",
    "\n",
    "df_transf, _, _ = transform(df_test, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7ea20d",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "20ee4d59-b589-430e-b3e4-3dd928074d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def under(df, beta):\n",
    "    X_min = df.iloc[:, :].values\n",
    "    \n",
    "    # keep each example with probability beta\n",
    "    num_dropped = 0\n",
    "    for i in range(len(X_min)):\n",
    "        if random.uniform(0,1) > beta:\n",
    "            X_min = np.delete(X_min, 0, axis=0)\n",
    "            num_dropped += 1\n",
    "    print(\"Total Deleted: \", num_dropped, \"\\t % Deleted: \", num_dropped / len(df))\n",
    "    df = pd.DataFrame(pd.DataFrame(X_min))\n",
    "    df.columns = ['num1','num2','num3','cat','outcome']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "0daf0345-6898-447f-9101-07ab6fd06f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tradeoff_visualization_error(classifier, r, n, apply_fairness = True, verbose = False, num_iters = 10):\n",
    "    \n",
    "    total_fidel_maj = []\n",
    "    total_fidel_min = []\n",
    "    total_fidel = []\n",
    "    \n",
    "    total_disp_bias_train = []\n",
    "    total_disp_bo_train = []\n",
    "    total_disp_mitigated_train = []\n",
    "    \n",
    "    total_disp_bias_test = []\n",
    "    total_disp_bo_test = []\n",
    "    total_disp_mitigated_test = []\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "    \n",
    "        # 1 to 0 in increments of 0.1\n",
    "        bias_amts = np.divide(list(range(10,-1,-1)),10)\n",
    "\n",
    "        test_maj = []\n",
    "        test_min = []\n",
    "        total = []\n",
    "        \n",
    "        disp_bias_train = []\n",
    "        disp_bo_train = []\n",
    "        disp_mitigated_train = []\n",
    "        \n",
    "        disp_bias_test = []\n",
    "        disp_bo_test = []\n",
    "        disp_mitigated_test = []\n",
    "        \n",
    "        count = 0\n",
    "        outcome_continuous, df_synthetic = true_label_generation(r=r, eta=eta, n=2*n)\n",
    "        \n",
    "        threshold = 0.5\n",
    "        exact_bo_labels = np.where(outcome_continuous < threshold, 0, 1)\n",
    "        exact_bo_labels_train = np.array(exact_bo_labels[range(0,n)])\n",
    "        exact_bo_labels_test = np.array(exact_bo_labels[range(n,len(df_synthetic))])\n",
    "        \n",
    "        # split into train and test\n",
    "        df_train = df_synthetic.loc[range(0,n), :]\n",
    "        \n",
    "        df_train_transf, maj_list, min_list = transform(df_train, True, True)\n",
    "        \n",
    "        df_test = df_synthetic.loc[range(n, len(df_synthetic)),:]\n",
    "        df_test_transf, maj_list, min_list = transform(df_test, True, False)\n",
    "        \n",
    "        df_test_maj = df_test_transf.loc[maj_list]\n",
    "        df_test_min = df_test_transf.loc[min_list]\n",
    "\n",
    "        # format training data\n",
    "        X_true = df_train_transf.iloc[:, :-1].values\n",
    "        y_true = df_train_transf.iloc[:, -1].values\n",
    "        \n",
    "        # format test data\n",
    "        X_test = df_test_transf.iloc[:, :-1].values\n",
    "        X_test_maj = df_test_maj.iloc[:, :-1].values\n",
    "        X_test_min = df_test_min.iloc[:, :-1].values\n",
    "        y_test = df_test_transf.iloc[:, -1].values\n",
    "        y_test_maj = df_test_maj.iloc[:, -1].values\n",
    "        y_test_min = df_test_min.iloc[:, -1].values\n",
    "        \n",
    "        sens_attr_test = df_test['cat']\n",
    "        \n",
    "        for beta in bias_amts:\n",
    "            \n",
    "            if i == 0: print(\"Beta: \", beta, '\\n')\n",
    "\n",
    "            df_train_copy = df_train.copy()\n",
    "\n",
    "            df_majority = df_train_copy[df_train_copy['cat'] == 1]\n",
    "            df_minority = df_train_copy[df_train_copy['cat'] == 0]\n",
    "\n",
    "            # unfavored group with negative label\n",
    "            df_minority_negative = df_minority[df_minority['outcome'] == 0.0]\n",
    "\n",
    "            # unfavored group with positive label (preferred)\n",
    "            df_minority_positive = df_minority[df_minority['outcome'] == 1.0]\n",
    "\n",
    "            # data frame without positively labeled examples from minority class\n",
    "            df_total = pd.concat([df_majority, df_minority_negative])\n",
    "\n",
    "            # under-sampling process\n",
    "            df_undersampled = under(df_minority_positive, beta)\n",
    "\n",
    "            # combine undersampled and original favored class to create dataset\n",
    "            df_concat = pd.concat([df_total,df_undersampled]).sample(frac=1, random_state = 42)\n",
    "\n",
    "            # format data\n",
    "            X_bias_true, y_bias_true, df_sens = transform(df_concat)\n",
    "            \n",
    "            # model trained on biased data\n",
    "            classifier = LogisticRegression(solver = 'lbfgs', random_state = 42, penalty = 'none')\n",
    "            classifier_bias = classifier.fit(X_bias_true, y_bias_true)\n",
    "            \n",
    "            acc = accuracy_score(y_test,classifier_bias.predict(X_test))\n",
    "            #print(f'Biased classifier:')\n",
    "            #print(f'     Test accuracy = {acc}')\n",
    "            \n",
    "            m = classifier_bias\n",
    "            def classify(X): return m.predict(X)\n",
    "            error = ErrorRate()\n",
    "            error.load_data(X_bias_true, pd.Series(y_bias_true), sensitive_features=df_sens)\n",
    "            disparity = EqualizedOdds()\n",
    "            disparity.load_data(X_bias_true, pd.Series(y_bias_true), sensitive_features=df_sens)\n",
    "            #print(f'     Train error = {error.gamma(classify)[0]}')\n",
    "            #print(f'     Train disparity = {disparity.gamma(classify).max()}')\n",
    "            disp_bias_train += [disparity.gamma(classify).max()]\n",
    "\n",
    "            m = classifier_bias\n",
    "            def classify(X): return m.predict(X)\n",
    "            error = ErrorRate()\n",
    "            error.load_data(X_test, pd.Series(y_test), sensitive_features=sens_attr_test)\n",
    "            disparity = EqualizedOdds()\n",
    "            disparity.load_data(X_test, pd.Series(y_test), sensitive_features=sens_attr_test)\n",
    "            #print(f'     Test error = {error.gamma(classify)[0]}')\n",
    "            #print(f'     Test disparity = {disparity.gamma(classify).max()}')\n",
    "            disp_bias_test += [disparity.gamma(classify).max()]\n",
    "            \n",
    "            # Learned bayes optimal classifier\n",
    "            classifier = LogisticRegression(solver = 'lbfgs', random_state = 42, penalty = 'none')\n",
    "            classifier_b = classifier.fit(X_true, y_true)                \n",
    "            #classifier_b = clone(classifier).fit(X_true, y_true)\n",
    "            acc = accuracy_score(y_test,classifier_b.predict(X_test))\n",
    "            #print(f'Learned BO classifier:')\n",
    "            #print(f'     Test accuracy = {acc}')\n",
    "            \n",
    "            m = classifier_b\n",
    "            def classify(X): return m.predict(X)\n",
    "            error = ErrorRate()\n",
    "            error.load_data(X_bias_true, pd.Series(y_bias_true), sensitive_features=df_sens)\n",
    "            disparity = EqualizedOdds()\n",
    "            disparity.load_data(X_bias_true, pd.Series(y_bias_true), sensitive_features=df_sens)\n",
    "            #print(f'     Train error = {error.gamma(classify)[0]}')\n",
    "            #print(f'     Train disparity = {disparity.gamma(classify).max()}')\n",
    "            disp_bo_train += [disparity.gamma(classify).max()]\n",
    "\n",
    "            m = classifier_b\n",
    "            def classify(X): return m.predict(X)\n",
    "            error = ErrorRate()\n",
    "            error.load_data(X_test, pd.Series(y_test), sensitive_features=sens_attr_test)\n",
    "            disparity = EqualizedOdds()\n",
    "            disparity.load_data(X_test, pd.Series(y_test), sensitive_features=sens_attr_test)\n",
    "            #print(f'     Test error = {error.gamma(classify)[0]}')\n",
    "            #print(f'     Test disparity = {disparity.gamma(classify).max()}')\n",
    "            disp_bo_test += [disparity.gamma(classify).max()]\n",
    "            \n",
    "            \n",
    "            # Exact BO optimal classifier\n",
    "            #print(f'Exact BO classifier:')\n",
    "            acc = accuracy_score(y_test,exact_bo_labels_test)\n",
    "            #print(f'     Test accuracy = {acc}')\n",
    "            acc = accuracy_score(y_true,exact_bo_labels_train)\n",
    "            #print(f'     Train accuracy = {acc}')\n",
    "\n",
    "            if apply_fairness:\n",
    "                constraint = EqualizedOdds()\n",
    "                classifier = LogisticRegression(solver = 'lbfgs', random_state = 42, penalty = 'none', fit_intercept = False, max_iter = 200)   \n",
    "\n",
    "                classifier_mitigated_bias = GridSearch(estimator=classifier,\n",
    "                                                       constraints=constraint,\n",
    "                                                       selection_rule='tradeoff_optimization',\n",
    "                                                       constraint_weight=0.5,\n",
    "                                                       grid_size=10,\n",
    "                                                       grid_limit=2.0,\n",
    "                                                       grid_offset=None,\n",
    "                                                       grid=None,\n",
    "                                                       sample_weight_name='sample_weight')\n",
    "                                                       \n",
    "                classifier_mitigated_bias.fit(X_bias_true, y_bias_true, sensitive_features = df_sens)\n",
    "                \n",
    "                acc = accuracy_score(y_test,classifier_mitigated_bias.predict(X_test))\n",
    "                #print(f'Mitigated bias classifier:')\n",
    "                #print(f'     Test accuracy = {acc}')\n",
    "                \n",
    "                m = classifier_mitigated_bias\n",
    "                def classify(X): return m.predict(X)\n",
    "                error = ErrorRate()\n",
    "                error.load_data(X_bias_true, pd.Series(y_bias_true), sensitive_features=df_sens)\n",
    "                disparity = EqualizedOdds()\n",
    "                disparity.load_data(X_bias_true, pd.Series(y_bias_true), sensitive_features=df_sens)\n",
    "                #print(f'     Train error = {error.gamma(classify)[0]}')\n",
    "                #print(f'     Train disparity = {disparity.gamma(classify).max()}')\n",
    "                disp_mitigated_train += [disparity.gamma(classify).max()]\n",
    "                \n",
    "                m = classifier_mitigated_bias\n",
    "                def classify(X): return m.predict(X)\n",
    "                error = ErrorRate()\n",
    "                error.load_data(X_test, pd.Series(y_test), sensitive_features=sens_attr_test)\n",
    "                disparity = EqualizedOdds()\n",
    "                disparity.load_data(X_test, pd.Series(y_test), sensitive_features=sens_attr_test)\n",
    "                #print(f'     Test error = {error.gamma(classify)[0]}')\n",
    "                #print(f'     Test disparity = {disparity.gamma(classify).max()}')\n",
    "                disp_mitigated_test += [disparity.gamma(classify).max()]\n",
    "                \n",
    "                # Alternative fidelity of intervention model to no intervention model\n",
    "                alt_fid_train = accuracy_score(classifier_mitigated_bias.predict(X_bias_true),classifier_bias.predict(X_bias_true))\n",
    "                alt_fid_test = accuracy_score(classifier_mitigated_bias.predict(X_test),classifier_bias.predict(X_test))\n",
    "                #print(f'Alternative fidelity of intervention model to no intervention model: train = {alt_fid_train}, test = {alt_fid_test}')\n",
    "                \n",
    "\n",
    "            else:\n",
    "                classifier_mitigated_bias = clone(classifier_bias)\n",
    "                classifier_mitigated_bias.fit(X_bias_true, y_bias_true)\n",
    "                \n",
    "                # NOTE: disparities are the same as for classifier_bias\n",
    "\n",
    "                \n",
    "            # Fidelity in this step\n",
    "            fid = accuracy_score(classifier_mitigated_bias.predict(X_test),classifier_b.predict(X_test))\n",
    "            #print(f'Test set fidelity of learned BO classifier and mitigated_bias_classifier: {fid}')\n",
    "            fid = accuracy_score(classifier_mitigated_bias.predict(X_test),exact_bo_labels_test)\n",
    "            #print(f'Test set fidelity of exact BO classifier and mitigated biased classifier: {fid}')\n",
    "            fid = accuracy_score(classifier_bias.predict(X_test),classifier_b.predict(X_test))\n",
    "            #print(f'Test set fidelity of learned BO classifier and biased classifier: {fid}')\n",
    "            \n",
    "            # fidelity check\n",
    "            test_maj += [accuracy_score(classifier_mitigated_bias.predict(X_test_maj), classifier_b.predict(X_test_maj))]\n",
    "            test_min += [accuracy_score(classifier_mitigated_bias.predict(X_test_min), classifier_b.predict(X_test_min))]\n",
    "            total += [accuracy_score(classifier_mitigated_bias.predict(X_test), classifier_b.predict(X_test))]\n",
    "                \n",
    "            if verbose:\n",
    "                print(\"Finished Iteration: \", count)\n",
    "                count +=1\n",
    "        \n",
    "        #print(f'Fidelity test maj: {test_maj}')\n",
    "        #print(f'Fidelity test min: {test_maj}')\n",
    "        \n",
    "        total_fidel_maj.append(test_maj)\n",
    "        total_fidel_min.append(test_min)\n",
    "        total_fidel.append(total)\n",
    "        \n",
    "        total_disp_bias_train.append(disp_bias_train)\n",
    "        total_disp_bo_train.append(disp_bo_train)\n",
    "        total_disp_mitigated_train.append(disp_mitigated_train)\n",
    "\n",
    "        total_disp_bias_test.append(disp_bias_test)\n",
    "        total_disp_bo_test.append(disp_bo_test)\n",
    "        total_disp_mitigated_test.append(disp_mitigated_test)\n",
    "        \n",
    "        if verbose:\n",
    "                print(\"Finished Total Iteration: \", i+1)\n",
    "    \n",
    "    mean_fidel_maj = np.mean(total_fidel_maj, axis = 0)\n",
    "    mean_fidel_min = np.mean(total_fidel_min, axis = 0)\n",
    "    mean_fidel = np.mean(total_fidel, axis = 0)\n",
    "    \n",
    "    mean_disp_bias_train = np.mean(total_disp_bias_train, axis = 0)\n",
    "    mean_disp_bo_train = np.mean(total_disp_bo_train, axis = 0)\n",
    "    mean_disp_mitigated_train = np.mean(total_disp_mitigated_train, axis = 0)\n",
    "    \n",
    "    mean_disp_bias_test = np.mean(total_disp_bias_test, axis = 0)\n",
    "    mean_disp_bo_test = np.mean(total_disp_bo_test, axis = 0)\n",
    "    mean_disp_mitigated_test = np.mean(total_disp_mitigated_test, axis = 0)\n",
    "    \n",
    "    y_err_fidel_maj = np.std(total_fidel_maj, axis = 0)\n",
    "    y_err_fidel_min = np.std(total_fidel_min, axis = 0)\n",
    "    y_err_fidel = np.std(total_fidel, axis = 0)\n",
    "    \n",
    "    df = pd.DataFrame({\"Biased Train\" : mean_disp_bias_train,\n",
    "                       \"BO Train\" : mean_disp_bo_train,\n",
    "                       \"Mitigated Train\" : mean_disp_mitigated_train,\n",
    "                       \"Biased Test\" : mean_disp_bias_test,\n",
    "                       \"BO Test\" : mean_disp_bo_test,\n",
    "                       \"Mitigated Test\" : mean_disp_mitigated_test})\n",
    "   \n",
    "    return bias_amts, mean_fidel_maj, mean_fidel_min, mean_fidel, y_err_fidel_maj, y_err_fidel_min, y_err_fidel, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41379f6f-9cc4-4ab8-bf1c-5fd3dd0164ae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diff:  0.008458333333333345\n",
      "Beta:  1.0 \n",
      "\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  0\n",
      "Beta:  0.9 \n",
      "\n",
      "Total Deleted:  298 \t % Deleted:  0.10215975317106617\n",
      "Finished Iteration:  1\n",
      "Beta:  0.8 \n",
      "\n",
      "Total Deleted:  574 \t % Deleted:  0.19677751114158382\n",
      "Finished Iteration:  2\n",
      "Beta:  0.7 \n",
      "\n",
      "Total Deleted:  875 \t % Deleted:  0.2999657182036339\n",
      "Finished Iteration:  3\n",
      "Beta:  0.6 \n",
      "\n",
      "Total Deleted:  1180 \t % Deleted:  0.4045251971203291\n",
      "Finished Iteration:  4\n",
      "Beta:  0.5 \n",
      "\n",
      "Total Deleted:  1481 \t % Deleted:  0.5077134041823792\n",
      "Finished Iteration:  5\n",
      "Beta:  0.4 \n",
      "\n",
      "Total Deleted:  1766 \t % Deleted:  0.6054165238258484\n",
      "Finished Iteration:  6\n",
      "Beta:  0.3 \n",
      "\n",
      "Total Deleted:  2107 \t % Deleted:  0.7223174494343504\n",
      "Finished Iteration:  7\n",
      "Beta:  0.2 \n",
      "\n",
      "Total Deleted:  2329 \t % Deleted:  0.798423037367158\n",
      "Finished Iteration:  8\n",
      "Beta:  0.1 \n",
      "\n",
      "Total Deleted:  2625 \t % Deleted:  0.8998971546109016\n",
      "Finished Iteration:  9\n",
      "Beta:  0.0 \n",
      "\n",
      "Total Deleted:  2917 \t % Deleted:  1.0\n",
      "Finished Iteration:  10\n",
      "Finished Total Iteration:  1\n",
      "Diff:  0.010937499999999989\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  0\n",
      "Total Deleted:  283 \t % Deleted:  0.0950621430970776\n",
      "Finished Iteration:  1\n",
      "Total Deleted:  627 \t % Deleted:  0.2106147127981189\n",
      "Finished Iteration:  2\n",
      "Total Deleted:  905 \t % Deleted:  0.30399731273093716\n",
      "Finished Iteration:  3\n",
      "Total Deleted:  1189 \t % Deleted:  0.39939536446086665\n",
      "Finished Iteration:  4\n",
      "Total Deleted:  1507 \t % Deleted:  0.5062143097077595\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "eta = 0.4\n",
    "n = 30000\n",
    "r = 0.2\n",
    "num_iters = 50\n",
    "\n",
    "bias_amts, mean_fidel_maj, mean_fidel_min, mean_fidel, y_err_fidel_maj, y_err_fidel_min, y_err_fidel, df = \\\n",
    "tradeoff_visualization_error(classifier,r = r, n = n, apply_fairness=True,verbose=True, num_iters=num_iters)\n",
    "\n",
    "df.to_csv(f'with_intervention_numiters{num_iters}_n{n}_eta{eta}.csv', index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948164ae-809d-4631-8add-f7266195301c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "plt.errorbar(bias_amts, mean_fidel_maj, yerr = y_err_fidel_maj, label = 'Fidelity Majority', color = \"red\")\n",
    "plt.errorbar(bias_amts, mean_fidel_min, yerr = y_err_fidel_min, label = 'Fidelity Minority', color = \"green\")\n",
    "plt.errorbar(bias_amts, mean_fidel, yerr = y_err_fidel, label = 'Total Fidelity', color = \"blue\")\n",
    "plt.xlabel(\"Beta Value\")\n",
    "plt.ylabel(\"Fidelity\")\n",
    "plt.xlim(1.05, -0.05)\n",
    "plt.legend()\n",
    "plt.title(\"Fidelity Plot\")\n",
    "plt.savefig(f'with_intervention_numiters{num_iters}_n{n}_eta{eta}.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1c7bc0-82a2-4e23-a36d-4fe8208c2b02",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "eta = 0.4\n",
    "n = 30000\n",
    "r = 0.2\n",
    "num_iters = 50\n",
    "\n",
    "bias_amts, mean_fidel_maj2, mean_fidel_min2, mean_fidel2, y_err_fidel_maj2, y_err_fidel_min2, y_err_fidel2, df = \\\n",
    "tradeoff_visualization_error(classifier,r = r, n = n, apply_fairness=False,verbose=True, num_iters=num_iters)\n",
    "\n",
    "df.to_csv(f'with_intervention_numiters{num_iters}_n{n}_eta{eta}.csv', index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d0b897-cd83-4f5d-84b2-a1113b8702a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "plt.errorbar(bias_amts, mean_fidel_maj2, yerr = y_err_fidel_maj2, label = 'Fidelity Majority', color = \"red\")\n",
    "plt.errorbar(bias_amts, mean_fidel_min2, yerr = y_err_fidel_min2, label = 'Fidelity Minority', color = \"green\")\n",
    "plt.errorbar(bias_amts, mean_fidel2, yerr = y_err_fidel2, label = 'Total Fidelity', color = \"blue\")\n",
    "plt.xlabel(\"Beta Value\")\n",
    "plt.ylabel(\"Fidelity\")\n",
    "plt.xlim(1.05, -0.05)\n",
    "plt.legend()\n",
    "plt.title(\"Fidelity Plot\")\n",
    "plt.savefig(f'without_intervention_numiters{num_iters}_n{n}_eta{eta}.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bfad8c-86a7-45fc-86ff-c319095bbbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "eta = 0.4\n",
    "n = 100000\n",
    "r = 0.2\n",
    "num_iters = 10\n",
    "\n",
    "bias_amts, mean_fidel_maj2, mean_fidel_min2, mean_fidel2, y_err_fidel_maj2, y_err_fidel_min2, y_err_fidel2, df = \\\n",
    "tradeoff_visualization_error(classifier,r = r, n = n, apply_fairness=True,verbose=True, num_iters=num_iters)\n",
    "\n",
    "df.to_csv(f'with_intervention_numiters{num_iters}_n{n}_eta{eta}.csv', index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e63d175-1794-445b-8186-55fb6db9ba21",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "plt.errorbar(bias_amts, mean_fidel_maj2, yerr = y_err_fidel_maj2, label = 'Fidelity Majority', color = \"red\")\n",
    "plt.errorbar(bias_amts, mean_fidel_min2, yerr = y_err_fidel_min2, label = 'Fidelity Minority', color = \"green\")\n",
    "plt.errorbar(bias_amts, mean_fidel2, yerr = y_err_fidel2, label = 'Total Fidelity', color = \"blue\")\n",
    "plt.xlabel(\"Beta Value\")\n",
    "plt.ylabel(\"Fidelity\")\n",
    "plt.xlim(1.05, -0.05)\n",
    "plt.legend()\n",
    "plt.title(\"Fidelity Plot\")\n",
    "plt.savefig(f'with_intervention_numiters{num_iters}_n{n}_eta{eta}.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340dbb77-89b3-4f1b-bcee-1c363b174cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "eta = 0.4\n",
    "n = 100000\n",
    "r = 0.2\n",
    "num_iters = 10\n",
    "\n",
    "bias_amts, mean_fidel_maj2, mean_fidel_min2, mean_fidel2, y_err_fidel_maj2, y_err_fidel_min2, y_err_fidel2, df = \\\n",
    "tradeoff_visualization_error(classifier,r = r, n = n, apply_fairness=False,verbose=True, num_iters=num_iters)\n",
    "\n",
    "df.to_csv(f'with_intervention_numiters{num_iters}_n{n}_eta{eta}.csv', index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954995cb-3f63-4b90-802b-fc4a7c2e4cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "plt.errorbar(bias_amts, mean_fidel_maj2, yerr = y_err_fidel_maj2, label = 'Fidelity Majority', color = \"red\")\n",
    "plt.errorbar(bias_amts, mean_fidel_min2, yerr = y_err_fidel_min2, label = 'Fidelity Minority', color = \"green\")\n",
    "plt.errorbar(bias_amts, mean_fidel2, yerr = y_err_fidel2, label = 'Total Fidelity', color = \"blue\")\n",
    "plt.xlabel(\"Beta Value\")\n",
    "plt.ylabel(\"Fidelity\")\n",
    "plt.xlim(1.05, -0.05)\n",
    "plt.legend()\n",
    "plt.title(\"Fidelity Plot\")\n",
    "plt.savefig(f'without_intervention_numiters{num_iters}_n{n}_eta{eta}.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a719050b-374d-457a-91bb-ef2df12f8774",
   "metadata": {},
   "source": [
    "# Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4a842d-544e-4b41-992a-022c43cf3d2d",
   "metadata": {},
   "source": [
    "## Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25440d5e-3a5b-431f-b4d0-078ff50ec04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def representation(r, n, apply_fairness = True, verbose = False, num_iters = 10, inter = False, diff_base = False):\n",
    "    \n",
    "    total_fidel_maj = []\n",
    "    total_fidel_min = []\n",
    "    total_fidel = []\n",
    "    \n",
    "    total_disp_bias_train = []\n",
    "    total_disp_bo_train = []\n",
    "    total_disp_mitigated_train = []\n",
    "    \n",
    "    total_disp_bias_test = []\n",
    "    total_disp_bo_test = []\n",
    "    total_disp_mitigated_test = []\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "    \n",
    "        # 1 to 0 in increments of 0.1\n",
    "        bias_amts = np.divide(list(range(10,-1,-1)),10)\n",
    "        bias_amts[-1] = 0.01\n",
    "\n",
    "        test_maj = []\n",
    "        test_min = []\n",
    "        total = []\n",
    "        \n",
    "        disp_bias_train = []\n",
    "        disp_bo_train = []\n",
    "        disp_mitigated_train = []\n",
    "        \n",
    "        disp_bias_test = []\n",
    "        disp_bo_test = []\n",
    "        disp_mitigated_test = []\n",
    "        \n",
    "        count = 0\n",
    "        \n",
    "        if diff_base: \n",
    "            outcome_continuous, df_synthetic = true_label_generation(r=r, eta=eta, n=2*n, maj_means =[0.7,0.7,0.7])\n",
    "        else: outcome_continuous, df_synthetic = true_label_generation(r=r, eta=eta, n=2*n)        \n",
    "        \n",
    "        threshold = 0.5\n",
    "        exact_bo_labels = np.where(outcome_continuous < threshold, 0, 1)\n",
    "        exact_bo_labels_train = np.array(exact_bo_labels[range(0,n)])\n",
    "        exact_bo_labels_test = np.array(exact_bo_labels[range(n,len(df_synthetic))])\n",
    "        \n",
    "        # split into train and test\n",
    "        df_train = df_synthetic.loc[range(0,n), :]\n",
    "        \n",
    "        df_train_transf, maj_list, min_list = transform(df_train, True, True)\n",
    "        \n",
    "        df_test = df_synthetic.loc[range(n, len(df_synthetic)),:]\n",
    "        df_test_transf, maj_list, min_list = transform(df_test, True, False)\n",
    "        \n",
    "        df_test_maj = df_test_transf.loc[maj_list]\n",
    "        df_test_min = df_test_transf.loc[min_list]\n",
    "\n",
    "        # format training data\n",
    "        X_true = df_train_transf.iloc[:, :-1].values\n",
    "        y_true = df_train_transf.iloc[:, -1].values\n",
    "        \n",
    "        # format test data\n",
    "        X_test = df_test_transf.iloc[:, :-1].values\n",
    "        X_test_maj = df_test_maj.iloc[:, :-1].values\n",
    "        X_test_min = df_test_min.iloc[:, :-1].values\n",
    "        y_test = df_test_transf.iloc[:, -1].values\n",
    "        y_test_maj = df_test_maj.iloc[:, -1].values\n",
    "        y_test_min = df_test_min.iloc[:, -1].values\n",
    "        \n",
    "        sens_attr_test = df_test['cat']\n",
    "        \n",
    "        for beta in bias_amts:\n",
    "            \n",
    "            if i == 0: print(\"Beta: \", beta, '\\n')\n",
    "\n",
    "            df_train_copy = df_train.copy()\n",
    "\n",
    "            df_majority = df_train_copy[df_train_copy['cat'] == 1]\n",
    "            df_minority = df_train_copy[df_train_copy['cat'] == 0]\n",
    "\n",
    "            if inter:\n",
    "                # unfavored group with negative label\n",
    "                df_minority_negative = df_minority[df_minority['outcome'] == 0.0]\n",
    "\n",
    "                # unfavored group with positive label (preferred)\n",
    "                df_minority_positive = df_minority[df_minority['outcome'] == 1.0]\n",
    "\n",
    "                # data frame without positively labeled examples from minority class\n",
    "                df_total = pd.concat([df_majority, df_minority_negative])\n",
    "\n",
    "                df_undersampled = under(df_minority_positive, beta)\n",
    "\n",
    "                # combine undersampled and original favored class to create dataset\n",
    "                df_concat = pd.concat([df_total,df_undersampled]).sample(frac=1, random_state = 42) # permute data\n",
    "\n",
    "            else:\n",
    "                df_undersampled = under(df_minority, beta)\n",
    "\n",
    "                # combine undersampled and original favored class to create dataset\n",
    "                df_concat = pd.concat([df_majority,df_undersampled])\n",
    "                df_concat.sample(frac=1, random_state = 42) # permute data\n",
    "\n",
    "            # format data\n",
    "            X_bias_true, y_bias_true, df_sens = transform(df_concat)\n",
    "            \n",
    "            # model trained on biased data\n",
    "            classifier = LogisticRegression(solver = 'lbfgs', random_state = 42, penalty = 'none')\n",
    "            classifier_bias = classifier.fit(X_bias_true, y_bias_true)\n",
    "            \n",
    "            acc = accuracy_score(y_test,classifier_bias.predict(X_test))\n",
    "            #print(f'Biased classifier:')\n",
    "            #print(f'     Test accuracy = {acc}')\n",
    "            \n",
    "            m = classifier_bias\n",
    "            def classify(X): return m.predict(X)\n",
    "            error = ErrorRate()\n",
    "            error.load_data(X_bias_true, pd.Series(y_bias_true), sensitive_features=df_sens)\n",
    "            disparity = EqualizedOdds()\n",
    "            disparity.load_data(X_bias_true, pd.Series(y_bias_true), sensitive_features=df_sens)\n",
    "            #print(f'     Train error = {error.gamma(classify)[0]}')\n",
    "            #print(f'     Train disparity = {disparity.gamma(classify).max()}')\n",
    "            disp_bias_train += [disparity.gamma(classify).max()]\n",
    "\n",
    "            m = classifier_bias\n",
    "            def classify(X): return m.predict(X)\n",
    "            error = ErrorRate()\n",
    "            error.load_data(X_test, pd.Series(y_test), sensitive_features=sens_attr_test)\n",
    "            disparity = EqualizedOdds()\n",
    "            disparity.load_data(X_test, pd.Series(y_test), sensitive_features=sens_attr_test)\n",
    "            #print(f'     Test error = {error.gamma(classify)[0]}')\n",
    "            #print(f'     Test disparity = {disparity.gamma(classify).max()}')\n",
    "            disp_bias_test += [disparity.gamma(classify).max()]\n",
    "            \n",
    "            # Learned bayes optimal classifier\n",
    "            classifier = LogisticRegression(solver = 'lbfgs', random_state = 42, penalty = 'none')\n",
    "            classifier_b = classifier.fit(X_true, y_true)                \n",
    "            #classifier_b = clone(classifier).fit(X_true, y_true)\n",
    "            acc = accuracy_score(y_test,classifier_b.predict(X_test))\n",
    "            #print(f'Learned BO classifier:')\n",
    "            #print(f'     Test accuracy = {acc}')\n",
    "            \n",
    "            m = classifier_b\n",
    "            def classify(X): return m.predict(X)\n",
    "            error = ErrorRate()\n",
    "            error.load_data(X_bias_true, pd.Series(y_bias_true), sensitive_features=df_sens)\n",
    "            disparity = EqualizedOdds()\n",
    "            disparity.load_data(X_bias_true, pd.Series(y_bias_true), sensitive_features=df_sens)\n",
    "            #print(f'     Train error = {error.gamma(classify)[0]}')\n",
    "            #print(f'     Train disparity = {disparity.gamma(classify).max()}')\n",
    "            disp_bo_train += [disparity.gamma(classify).max()]\n",
    "\n",
    "            m = classifier_b\n",
    "            def classify(X): return m.predict(X)\n",
    "            error = ErrorRate()\n",
    "            error.load_data(X_test, pd.Series(y_test), sensitive_features=sens_attr_test)\n",
    "            disparity = EqualizedOdds()\n",
    "            disparity.load_data(X_test, pd.Series(y_test), sensitive_features=sens_attr_test)\n",
    "            #print(f'     Test error = {error.gamma(classify)[0]}')\n",
    "            #print(f'     Test disparity = {disparity.gamma(classify).max()}')\n",
    "            disp_bo_test += [disparity.gamma(classify).max()]\n",
    "            \n",
    "            \n",
    "            # Exact BO optimal classifier\n",
    "            #print(f'Exact BO classifier:')\n",
    "            acc = accuracy_score(y_test,exact_bo_labels_test)\n",
    "            #print(f'     Test accuracy = {acc}')\n",
    "            acc = accuracy_score(y_true,exact_bo_labels_train)\n",
    "            #print(f'     Train accuracy = {acc}')\n",
    "\n",
    "            if apply_fairness:\n",
    "                constraint = EqualizedOdds()\n",
    "                classifier = LogisticRegression(solver = 'lbfgs', random_state = 42, penalty = 'none', fit_intercept = False, max_iter = 200)   \n",
    "\n",
    "                classifier_mitigated_bias = GridSearch(estimator=classifier,\n",
    "                                                       constraints=constraint,\n",
    "                                                       selection_rule='tradeoff_optimization',\n",
    "                                                       constraint_weight=0.5,\n",
    "                                                       grid_size=10,\n",
    "                                                       grid_limit=2.0,\n",
    "                                                       grid_offset=None,\n",
    "                                                       grid=None,\n",
    "                                                       sample_weight_name='sample_weight')\n",
    "                                                       \n",
    "                classifier_mitigated_bias.fit(X_bias_true, y_bias_true, sensitive_features = df_sens)\n",
    "                \n",
    "                acc = accuracy_score(y_test,classifier_mitigated_bias.predict(X_test))\n",
    "                #print(f'Mitigated bias classifier:')\n",
    "                #print(f'     Test accuracy = {acc}')\n",
    "                \n",
    "                m = classifier_mitigated_bias\n",
    "                def classify(X): return m.predict(X)\n",
    "                error = ErrorRate()\n",
    "                error.load_data(X_bias_true, pd.Series(y_bias_true), sensitive_features=df_sens)\n",
    "                disparity = EqualizedOdds()\n",
    "                disparity.load_data(X_bias_true, pd.Series(y_bias_true), sensitive_features=df_sens)\n",
    "                #print(f'     Train error = {error.gamma(classify)[0]}')\n",
    "                #print(f'     Train disparity = {disparity.gamma(classify).max()}')\n",
    "                disp_mitigated_train += [disparity.gamma(classify).max()]\n",
    "                \n",
    "                m = classifier_mitigated_bias\n",
    "                def classify(X): return m.predict(X)\n",
    "                error = ErrorRate()\n",
    "                error.load_data(X_test, pd.Series(y_test), sensitive_features=sens_attr_test)\n",
    "                disparity = EqualizedOdds()\n",
    "                disparity.load_data(X_test, pd.Series(y_test), sensitive_features=sens_attr_test)\n",
    "                #print(f'     Test error = {error.gamma(classify)[0]}')\n",
    "                #print(f'     Test disparity = {disparity.gamma(classify).max()}')\n",
    "                disp_mitigated_test += [disparity.gamma(classify).max()]\n",
    "                \n",
    "                # Alternative fidelity of intervention model to no intervention model\n",
    "                alt_fid_train = accuracy_score(classifier_mitigated_bias.predict(X_bias_true),classifier_bias.predict(X_bias_true))\n",
    "                alt_fid_test = accuracy_score(classifier_mitigated_bias.predict(X_test),classifier_bias.predict(X_test))\n",
    "                #print(f'Alternative fidelity of intervention model to no intervention model: train = {alt_fid_train}, test = {alt_fid_test}')\n",
    "                \n",
    "\n",
    "            else:\n",
    "                classifier_mitigated_bias = clone(classifier_bias)\n",
    "                classifier_mitigated_bias.fit(X_bias_true, y_bias_true)\n",
    "                \n",
    "                # NOTE: disparities are the same as for classifier_bias\n",
    "\n",
    "                \n",
    "            # Fidelity in this step\n",
    "            fid = accuracy_score(classifier_mitigated_bias.predict(X_test),classifier_b.predict(X_test))\n",
    "            #print(f'Test set fidelity of learned BO classifier and mitigated_bias_classifier: {fid}')\n",
    "            fid = accuracy_score(classifier_mitigated_bias.predict(X_test),exact_bo_labels_test)\n",
    "            #print(f'Test set fidelity of exact BO classifier and mitigated biased classifier: {fid}')\n",
    "            fid = accuracy_score(classifier_bias.predict(X_test),classifier_b.predict(X_test))\n",
    "            #print(f'Test set fidelity of learned BO classifier and biased classifier: {fid}')\n",
    "            \n",
    "            # fidelity check\n",
    "            test_maj += [accuracy_score(classifier_mitigated_bias.predict(X_test_maj), classifier_b.predict(X_test_maj))]\n",
    "            test_min += [accuracy_score(classifier_mitigated_bias.predict(X_test_min), classifier_b.predict(X_test_min))]\n",
    "            total += [accuracy_score(classifier_mitigated_bias.predict(X_test), classifier_b.predict(X_test))]\n",
    "                \n",
    "            if verbose:\n",
    "                print(\"Finished Iteration: \", count)\n",
    "                count +=1\n",
    "        \n",
    "        #print(f'Fidelity test maj: {test_maj}')\n",
    "        #print(f'Fidelity test min: {test_maj}')\n",
    "        \n",
    "        total_fidel_maj.append(test_maj)\n",
    "        total_fidel_min.append(test_min)\n",
    "        total_fidel.append(total)\n",
    "        \n",
    "        total_disp_bias_train.append(disp_bias_train)\n",
    "        total_disp_bo_train.append(disp_bo_train)\n",
    "        total_disp_mitigated_train.append(disp_mitigated_train)\n",
    "\n",
    "        total_disp_bias_test.append(disp_bias_test)\n",
    "        total_disp_bo_test.append(disp_bo_test)\n",
    "        total_disp_mitigated_test.append(disp_mitigated_test)\n",
    "        \n",
    "        if verbose:\n",
    "                print(\"Finished Total Iteration: \", i+1)\n",
    "    \n",
    "    mean_fidel_maj = np.mean(total_fidel_maj, axis = 0)\n",
    "    mean_fidel_min = np.mean(total_fidel_min, axis = 0)\n",
    "    mean_fidel = np.mean(total_fidel, axis = 0)\n",
    "    \n",
    "    mean_disp_bias_train = np.mean(total_disp_bias_train, axis = 0)\n",
    "    mean_disp_bo_train = np.mean(total_disp_bo_train, axis = 0)\n",
    "    mean_disp_mitigated_train = np.mean(total_disp_mitigated_train, axis = 0)\n",
    "    \n",
    "    mean_disp_bias_test = np.mean(total_disp_bias_test, axis = 0)\n",
    "    mean_disp_bo_test = np.mean(total_disp_bo_test, axis = 0)\n",
    "    mean_disp_mitigated_test = np.mean(total_disp_mitigated_test, axis = 0)\n",
    "    \n",
    "    y_err_fidel_maj = np.std(total_fidel_maj, axis = 0)\n",
    "    y_err_fidel_min = np.std(total_fidel_min, axis = 0)\n",
    "    y_err_fidel = np.std(total_fidel, axis = 0)\n",
    "    \n",
    "    df = pd.DataFrame({\"Biased Train\" : mean_disp_bias_train,\n",
    "                       \"BO Train\" : mean_disp_bo_train,\n",
    "                       \"Mitigated Train\" : mean_disp_mitigated_train,\n",
    "                       \"Biased Test\" : mean_disp_bias_test,\n",
    "                       \"BO Test\" : mean_disp_bo_test,\n",
    "                       \"Mitigated Test\" : mean_disp_mitigated_test})\n",
    "   \n",
    "    return bias_amts, mean_fidel_maj, mean_fidel_min, mean_fidel, y_err_fidel_maj, y_err_fidel_min, y_err_fidel, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60507e31-79c5-4ee7-a6e7-0143ed7b4f10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "eta = 0.2\n",
    "n = 30000\n",
    "r = 0.2\n",
    "num_iters = 10\n",
    "\n",
    "bias_amts, mean_fidel_maj21, mean_fidel_min21, mean_fidel21, y_err_fidel_maj21, y_err_fidel_min21, y_err_fidel21, df = \\\n",
    "representation(r = r, n = n, apply_fairness=True,verbose=True, num_iters=num_iters, diff_base = False, inter = False)\n",
    "\n",
    "df.to_csv('repr_no_inter.csv', index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61ebf16-57e2-47ca-9c5c-b33d3e7c28b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bias_amts = np.divide(list(range(10,-1,-1)),10)\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.errorbar(bias_amts, 1-mean_fidel_maj21, yerr = y_err_fidel_maj21, label = 'Fidelity Majority', color = \"red\")\n",
    "plt.errorbar(bias_amts, 1-mean_fidel_min21, yerr = y_err_fidel_min21, label = 'Fidelity Minority', color = \"green\")\n",
    "plt.xlabel(\"Beta Value for Minority\")\n",
    "plt.ylabel(\"Fidelity\")\n",
    "plt.xlim(1.05, -0.05)\n",
    "plt.ylim(0.7, 1)\n",
    "# plt.ylim(0.95, 1)\n",
    "plt.legend()\n",
    "plt.title(\"Fidelity Plot\")\n",
    "plt.savefig('repr_no_inter.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9c94ae-52f6-4015-9971-46ed43d8c1df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "eta = 0.2\n",
    "n = 30000\n",
    "r = 0.2\n",
    "num_iters = 10\n",
    "\n",
    "bias_amts, mean_fidel_maj21, mean_fidel_min22, mean_fidel22, y_err_fidel_maj22, y_err_fidel_min22, y_err_fidel22, df = \\\n",
    "representation(r = r, n = n, apply_fairness=True,verbose=True, num_iters=num_iters, diff_base = False, inter = True)\n",
    "\n",
    "df.to_csv('repr_inter.csv', index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa9b967-bcf5-4dd5-9ee5-4bb1a0528973",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bias_amts = np.divide(list(range(10,-1,-1)),10)\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.errorbar(bias_amts, 1-mean_fidel_maj22, yerr = y_err_fidel_maj22, label = 'Fidelity Majority', color = \"red\")\n",
    "plt.errorbar(bias_amts, 1-mean_fidel_min22, yerr = y_err_fidel_min22, label = 'Fidelity Minority', color = \"green\")\n",
    "plt.xlabel(\"Beta Value for Minority\")\n",
    "plt.ylabel(\"Fidelity\")\n",
    "plt.xlim(1.05, -0.05)\n",
    "plt.ylim(0.7, 1)\n",
    "# plt.ylim(0.95, 1)\n",
    "plt.legend()\n",
    "plt.title(\"Fidelity Plot\")\n",
    "plt.savefig('repr_inter.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8590be-6740-467c-8a4c-2232d7918caa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "eta = 0.2\n",
    "n = 30000\n",
    "r = 0.2\n",
    "num_iters = 10\n",
    "\n",
    "bias_amts, mean_fidel_maj21, mean_fidel_min22, mean_fidel22, y_err_fidel_maj22, y_err_fidel_min22, y_err_fidel22, df = \\\n",
    "representation(r = r, n = n, apply_fairness=True,verbose=True, num_iters=num_iters, diff_base = True, inter = True)\n",
    "\n",
    "df.to_csv('repr_diff_base.csv', index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db45ad24-7410-4508-80e8-1a603a247728",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bias_amts = np.divide(list(range(10,-1,-1)),10)\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.errorbar(bias_amts, 1-mean_fidel_maj23, yerr = y_err_fidel_maj23, label = 'Fidelity Majority', color = \"red\")\n",
    "plt.errorbar(bias_amts, 1-mean_fidel_min23, yerr = y_err_fidel_min23, label = 'Fidelity Minority', color = \"green\")\n",
    "plt.xlabel(\"Beta Value for Minority\")\n",
    "plt.ylabel(\"Fidelity\")\n",
    "plt.xlim(1.05, -0.05)\n",
    "plt.ylim(0.7, 1)\n",
    "# plt.ylim(0.95, 1)\n",
    "plt.legend()\n",
    "plt.title(\"Fidelity Plot\")\n",
    "plt.savefig('repr_diff_base.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83012064-1e3e-47c5-83b9-e49c38db2e90",
   "metadata": {},
   "source": [
    "## Label Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83663e42-1cb4-4ecb-991e-bd4d83ed3d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flip(df_synthetic, eta):\n",
    "    labels = df_synthetic['outcome'].values\n",
    "    #print('Before:', df_synthetic['outcome'].value_counts())\n",
    "    num_flipped = 0\n",
    "    for i in range(len(labels)):\n",
    "        if random.uniform(0,1) <= eta:\n",
    "            labels[i] = 1 if labels[i] == 0 else 0\n",
    "            num_flipped += 1\n",
    "    df_synthetic['outcome'] = labels\n",
    "    #print('After:', df_synthetic['outcome'].value_counts())\n",
    "    print('Bias Num Flipped: ', num_flipped, \"\\tRate: \", num_flipped / len(df_synthetic))\n",
    "    return df_synthetic\n",
    "\n",
    "def get_noise(df, beta, inter = False):\n",
    "    df_majority = df[df['cat'] == 1]\n",
    "    df_minority = df[df['cat'] == 0]\n",
    "    \n",
    "    if inter:\n",
    "        # unfavored group with negative label\n",
    "        df_minority_negative = df_minority[df_minority['outcome'] == 0.0]\n",
    "\n",
    "        # unfavored group with positive label (preferred)\n",
    "        df_minority_positive = df_minority[df_minority['outcome'] == 1.0]\n",
    "\n",
    "        # data frame without positively labeled examples from minority class\n",
    "        df_total = pd.concat([df_majority, df_minority_negative])\n",
    "        \n",
    "        df_undersampled = flip(df_minority_positive, beta)\n",
    "\n",
    "        # combine undersampled and original favored class to create dataset\n",
    "        df_concat = pd.concat([df_total,df_undersampled])\n",
    "\n",
    "        return df_concat.sample(frac=1, random_state = 42) # permute data\n",
    "    \n",
    "    else:\n",
    "        df_undersampled = flip(df_minority, beta)\n",
    "\n",
    "        # combine undersampled and original favored class to create dataset\n",
    "        df_concat = pd.concat([df_majority,df_undersampled])\n",
    "\n",
    "        return df_concat.sample(frac=1, random_state = 42) # permute data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92a2670-6214-46a0-8845-1a19baeeee82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_noise(r, n, apply_fairness = True, verbose = False, num_iters = 10, inter = False, diff_base = False):\n",
    "    \n",
    "    total_fidel_maj = []\n",
    "    total_fidel_min = []\n",
    "    total_fidel = []\n",
    "    \n",
    "    total_disp_bias_train = []\n",
    "    total_disp_bo_train = []\n",
    "    total_disp_mitigated_train = []\n",
    "    \n",
    "    total_disp_bias_test = []\n",
    "    total_disp_bo_test = []\n",
    "    total_disp_mitigated_test = []\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "    \n",
    "        # 1 to 0 in increments of 0.1\n",
    "        bias_amts = np.divide(list(range(1,11,1)),22)\n",
    "\n",
    "        test_maj = []\n",
    "        test_min = []\n",
    "        total = []\n",
    "        \n",
    "        disp_bias_train = []\n",
    "        disp_bo_train = []\n",
    "        disp_mitigated_train = []\n",
    "        \n",
    "        disp_bias_test = []\n",
    "        disp_bo_test = []\n",
    "        disp_mitigated_test = []\n",
    "        \n",
    "        count = 0\n",
    "        \n",
    "        if diff_base: \n",
    "            outcome_continuous, df_synthetic = true_label_generation(r=r, eta=eta, n=2*n, maj_means =[0.7,0.7,0.7])\n",
    "        else: outcome_continuous, df_synthetic = true_label_generation(r=r, eta=eta, n=2*n)        \n",
    "        \n",
    "        threshold = 0.5\n",
    "        exact_bo_labels = np.where(outcome_continuous < threshold, 0, 1)\n",
    "        exact_bo_labels_train = np.array(exact_bo_labels[range(0,n)])\n",
    "        exact_bo_labels_test = np.array(exact_bo_labels[range(n,len(df_synthetic))])\n",
    "        \n",
    "        # split into train and test\n",
    "        df_train = df_synthetic.loc[range(0,n), :]\n",
    "        \n",
    "        df_train_transf, maj_list, min_list = transform(df_train, True, True)\n",
    "        \n",
    "        df_test = df_synthetic.loc[range(n, len(df_synthetic)),:]\n",
    "        df_test_transf, maj_list, min_list = transform(df_test, True, False)\n",
    "        \n",
    "        df_test_maj = df_test_transf.loc[maj_list]\n",
    "        df_test_min = df_test_transf.loc[min_list]\n",
    "\n",
    "        # format training data\n",
    "        X_true = df_train_transf.iloc[:, :-1].values\n",
    "        y_true = df_train_transf.iloc[:, -1].values\n",
    "        \n",
    "        # format test data\n",
    "        X_test = df_test_transf.iloc[:, :-1].values\n",
    "        X_test_maj = df_test_maj.iloc[:, :-1].values\n",
    "        X_test_min = df_test_min.iloc[:, :-1].values\n",
    "        y_test = df_test_transf.iloc[:, -1].values\n",
    "        y_test_maj = df_test_maj.iloc[:, -1].values\n",
    "        y_test_min = df_test_min.iloc[:, -1].values\n",
    "        \n",
    "        sens_attr_test = df_test['cat']\n",
    "        \n",
    "        for beta in bias_amts:\n",
    "            \n",
    "            if i == 0: print(\"Beta: \", beta, '\\n')\n",
    "\n",
    "            df_train_copy = df_train.copy()\n",
    "\n",
    "            df_majority = df_train_copy[df_train_copy['cat'] == 1]\n",
    "            df_minority = df_train_copy[df_train_copy['cat'] == 0]\n",
    "\n",
    "            # unfavored group with negative label\n",
    "            df_minority_negative = df_minority[df_minority['outcome'] == 0.0]\n",
    "\n",
    "            # unfavored group with positive label (preferred)\n",
    "            df_minority_positive = df_minority[df_minority['outcome'] == 1.0]\n",
    "\n",
    "            # data frame without positively labeled examples from minority class\n",
    "            df_total = pd.concat([df_majority, df_minority_negative])\n",
    "\n",
    "            # under-sampling process\n",
    "            df_undersampled = under(df_minority_positive, 1)\n",
    "\n",
    "            # combine undersampled and original favored class to create dataset\n",
    "            df_concat1 = pd.concat([df_total,df_undersampled])\n",
    "            \n",
    "            df_concat = get_noise(df_concat1, beta, inter) # group-dependent label noise\n",
    "\n",
    "            # format data\n",
    "            X_bias_true, y_bias_true, df_sens = transform(df_concat)\n",
    "            \n",
    "            # model trained on biased data\n",
    "            classifier = LogisticRegression(solver = 'lbfgs', random_state = 42, penalty = 'none')\n",
    "            classifier_bias = classifier.fit(X_bias_true, y_bias_true)\n",
    "            \n",
    "            acc = accuracy_score(y_test,classifier_bias.predict(X_test))\n",
    "            #print(f'Biased classifier:')\n",
    "            #print(f'     Test accuracy = {acc}')\n",
    "            \n",
    "            m = classifier_bias\n",
    "            def classify(X): return m.predict(X)\n",
    "            error = ErrorRate()\n",
    "            error.load_data(X_bias_true, pd.Series(y_bias_true), sensitive_features=df_sens)\n",
    "            disparity = EqualizedOdds()\n",
    "            disparity.load_data(X_bias_true, pd.Series(y_bias_true), sensitive_features=df_sens)\n",
    "            #print(f'     Train error = {error.gamma(classify)[0]}')\n",
    "            #print(f'     Train disparity = {disparity.gamma(classify).max()}')\n",
    "            disp_bias_train += [disparity.gamma(classify).max()]\n",
    "\n",
    "            m = classifier_bias\n",
    "            def classify(X): return m.predict(X)\n",
    "            error = ErrorRate()\n",
    "            error.load_data(X_test, pd.Series(y_test), sensitive_features=sens_attr_test)\n",
    "            disparity = EqualizedOdds()\n",
    "            disparity.load_data(X_test, pd.Series(y_test), sensitive_features=sens_attr_test)\n",
    "            #print(f'     Test error = {error.gamma(classify)[0]}')\n",
    "            #print(f'     Test disparity = {disparity.gamma(classify).max()}')\n",
    "            disp_bias_test += [disparity.gamma(classify).max()]\n",
    "            \n",
    "            # Learned bayes optimal classifier\n",
    "            classifier = LogisticRegression(solver = 'lbfgs', random_state = 42, penalty = 'none')\n",
    "            classifier_b = classifier.fit(X_true, y_true)                \n",
    "            #classifier_b = clone(classifier).fit(X_true, y_true)\n",
    "            acc = accuracy_score(y_test,classifier_b.predict(X_test))\n",
    "            #print(f'Learned BO classifier:')\n",
    "            #print(f'     Test accuracy = {acc}')\n",
    "            \n",
    "            m = classifier_b\n",
    "            def classify(X): return m.predict(X)\n",
    "            error = ErrorRate()\n",
    "            error.load_data(X_bias_true, pd.Series(y_bias_true), sensitive_features=df_sens)\n",
    "            disparity = EqualizedOdds()\n",
    "            disparity.load_data(X_bias_true, pd.Series(y_bias_true), sensitive_features=df_sens)\n",
    "            #print(f'     Train error = {error.gamma(classify)[0]}')\n",
    "            #print(f'     Train disparity = {disparity.gamma(classify).max()}')\n",
    "            disp_bo_train += [disparity.gamma(classify).max()]\n",
    "\n",
    "            m = classifier_b\n",
    "            def classify(X): return m.predict(X)\n",
    "            error = ErrorRate()\n",
    "            error.load_data(X_test, pd.Series(y_test), sensitive_features=sens_attr_test)\n",
    "            disparity = EqualizedOdds()\n",
    "            disparity.load_data(X_test, pd.Series(y_test), sensitive_features=sens_attr_test)\n",
    "            #print(f'     Test error = {error.gamma(classify)[0]}')\n",
    "            #print(f'     Test disparity = {disparity.gamma(classify).max()}')\n",
    "            disp_bo_test += [disparity.gamma(classify).max()]\n",
    "            \n",
    "            \n",
    "            # Exact BO optimal classifier\n",
    "            #print(f'Exact BO classifier:')\n",
    "            acc = accuracy_score(y_test,exact_bo_labels_test)\n",
    "            #print(f'     Test accuracy = {acc}')\n",
    "            acc = accuracy_score(y_true,exact_bo_labels_train)\n",
    "            #print(f'     Train accuracy = {acc}')\n",
    "\n",
    "            if apply_fairness:\n",
    "                constraint = EqualizedOdds()\n",
    "                classifier = LogisticRegression(solver = 'lbfgs', random_state = 42, penalty = 'none', fit_intercept = False, max_iter = 200)   \n",
    "\n",
    "                classifier_mitigated_bias = GridSearch(estimator=classifier,\n",
    "                                                       constraints=constraint,\n",
    "                                                       selection_rule='tradeoff_optimization',\n",
    "                                                       constraint_weight=0.5,\n",
    "                                                       grid_size=10,\n",
    "                                                       grid_limit=2.0,\n",
    "                                                       grid_offset=None,\n",
    "                                                       grid=None,\n",
    "                                                       sample_weight_name='sample_weight')\n",
    "                                                       \n",
    "                classifier_mitigated_bias.fit(X_bias_true, y_bias_true, sensitive_features = df_sens)\n",
    "                \n",
    "                acc = accuracy_score(y_test,classifier_mitigated_bias.predict(X_test))\n",
    "                #print(f'Mitigated bias classifier:')\n",
    "                #print(f'     Test accuracy = {acc}')\n",
    "                \n",
    "                m = classifier_mitigated_bias\n",
    "                def classify(X): return m.predict(X)\n",
    "                error = ErrorRate()\n",
    "                error.load_data(X_bias_true, pd.Series(y_bias_true), sensitive_features=df_sens)\n",
    "                disparity = EqualizedOdds()\n",
    "                disparity.load_data(X_bias_true, pd.Series(y_bias_true), sensitive_features=df_sens)\n",
    "                #print(f'     Train error = {error.gamma(classify)[0]}')\n",
    "                #print(f'     Train disparity = {disparity.gamma(classify).max()}')\n",
    "                disp_mitigated_train += [disparity.gamma(classify).max()]\n",
    "                \n",
    "                m = classifier_mitigated_bias\n",
    "                def classify(X): return m.predict(X)\n",
    "                error = ErrorRate()\n",
    "                error.load_data(X_test, pd.Series(y_test), sensitive_features=sens_attr_test)\n",
    "                disparity = EqualizedOdds()\n",
    "                disparity.load_data(X_test, pd.Series(y_test), sensitive_features=sens_attr_test)\n",
    "                #print(f'     Test error = {error.gamma(classify)[0]}')\n",
    "                #print(f'     Test disparity = {disparity.gamma(classify).max()}')\n",
    "                disp_mitigated_test += [disparity.gamma(classify).max()]\n",
    "                \n",
    "                # Alternative fidelity of intervention model to no intervention model\n",
    "                alt_fid_train = accuracy_score(classifier_mitigated_bias.predict(X_bias_true),classifier_bias.predict(X_bias_true))\n",
    "                alt_fid_test = accuracy_score(classifier_mitigated_bias.predict(X_test),classifier_bias.predict(X_test))\n",
    "                #print(f'Alternative fidelity of intervention model to no intervention model: train = {alt_fid_train}, test = {alt_fid_test}')\n",
    "                \n",
    "\n",
    "            else:\n",
    "                classifier_mitigated_bias = clone(classifier_bias)\n",
    "                classifier_mitigated_bias.fit(X_bias_true, y_bias_true)\n",
    "                \n",
    "                # NOTE: disparities are the same as for classifier_bias\n",
    "\n",
    "                \n",
    "            # Fidelity in this step\n",
    "            fid = accuracy_score(classifier_mitigated_bias.predict(X_test),classifier_b.predict(X_test))\n",
    "            #print(f'Test set fidelity of learned BO classifier and mitigated_bias_classifier: {fid}')\n",
    "            fid = accuracy_score(classifier_mitigated_bias.predict(X_test),exact_bo_labels_test)\n",
    "            #print(f'Test set fidelity of exact BO classifier and mitigated biased classifier: {fid}')\n",
    "            fid = accuracy_score(classifier_bias.predict(X_test),classifier_b.predict(X_test))\n",
    "            #print(f'Test set fidelity of learned BO classifier and biased classifier: {fid}')\n",
    "            \n",
    "            # fidelity check\n",
    "            test_maj += [accuracy_score(classifier_mitigated_bias.predict(X_test_maj), classifier_b.predict(X_test_maj))]\n",
    "            test_min += [accuracy_score(classifier_mitigated_bias.predict(X_test_min), classifier_b.predict(X_test_min))]\n",
    "            total += [accuracy_score(classifier_mitigated_bias.predict(X_test), classifier_b.predict(X_test))]\n",
    "                \n",
    "            if verbose:\n",
    "                print(\"Finished Iteration: \", count)\n",
    "                count +=1\n",
    "        \n",
    "        #print(f'Fidelity test maj: {test_maj}')\n",
    "        #print(f'Fidelity test min: {test_maj}')\n",
    "        \n",
    "        total_fidel_maj.append(test_maj)\n",
    "        total_fidel_min.append(test_min)\n",
    "        total_fidel.append(total)\n",
    "        \n",
    "        total_disp_bias_train.append(disp_bias_train)\n",
    "        total_disp_bo_train.append(disp_bo_train)\n",
    "        total_disp_mitigated_train.append(disp_mitigated_train)\n",
    "\n",
    "        total_disp_bias_test.append(disp_bias_test)\n",
    "        total_disp_bo_test.append(disp_bo_test)\n",
    "        total_disp_mitigated_test.append(disp_mitigated_test)\n",
    "        \n",
    "        if verbose:\n",
    "                print(\"Finished Total Iteration: \", i+1)\n",
    "    \n",
    "    mean_fidel_maj = np.mean(total_fidel_maj, axis = 0)\n",
    "    mean_fidel_min = np.mean(total_fidel_min, axis = 0)\n",
    "    mean_fidel = np.mean(total_fidel, axis = 0)\n",
    "    \n",
    "    mean_disp_bias_train = np.mean(total_disp_bias_train, axis = 0)\n",
    "    mean_disp_bo_train = np.mean(total_disp_bo_train, axis = 0)\n",
    "    mean_disp_mitigated_train = np.mean(total_disp_mitigated_train, axis = 0)\n",
    "    \n",
    "    mean_disp_bias_test = np.mean(total_disp_bias_test, axis = 0)\n",
    "    mean_disp_bo_test = np.mean(total_disp_bo_test, axis = 0)\n",
    "    mean_disp_mitigated_test = np.mean(total_disp_mitigated_test, axis = 0)\n",
    "    \n",
    "    y_err_fidel_maj = np.std(total_fidel_maj, axis = 0)\n",
    "    y_err_fidel_min = np.std(total_fidel_min, axis = 0)\n",
    "    y_err_fidel = np.std(total_fidel, axis = 0)\n",
    "    \n",
    "    df = pd.DataFrame({\"Biased Train\" : mean_disp_bias_train,\n",
    "                       \"BO Train\" : mean_disp_bo_train,\n",
    "                       \"Mitigated Train\" : mean_disp_mitigated_train,\n",
    "                       \"Biased Test\" : mean_disp_bias_test,\n",
    "                       \"BO Test\" : mean_disp_bo_test,\n",
    "                       \"Mitigated Test\" : mean_disp_mitigated_test})\n",
    "   \n",
    "    return bias_amts, mean_fidel_maj, mean_fidel_min, mean_fidel, y_err_fidel_maj, y_err_fidel_min, y_err_fidel, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be596a58-8b4f-4adb-b9b9-01f13a984f11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "eta = 0.2\n",
    "n = 30000\n",
    "r = 0.2\n",
    "num_iters = 10\n",
    "\n",
    "bias_amts, mean_fidel_maj24, mean_fidel_min24, mean_fidel24, y_err_fidel_maj24, y_err_fidel_min24, y_err_fidel24, df = \\\n",
    "label_noise(r = r, n = n, apply_fairness=True,verbose=True, num_iters=num_iters, diff_base = False, inter = False)\n",
    "\n",
    "df.to_csv('label_noise_no_inter.csv', index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a85d73b-c547-44a1-9fbe-a2e155a35249",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bias_amts = np.divide(list(range(1,11,1)),22)\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.errorbar(bias_amts, 1-mean_fidel_maj24, yerr = y_err_fidel_maj24, label = 'Fidelity Majority', color = \"red\")\n",
    "plt.errorbar(bias_amts, 1-mean_fidel_min24, yerr = y_err_fidel_min24, label = 'Fidelity Minority', color = \"green\")\n",
    "plt.xlabel(\"Eta Minority Value\")\n",
    "plt.ylabel(\"Fidelity\")\n",
    "#plt.ylim(0.7,1)\n",
    "plt.legend()\n",
    "plt.title(\"Fidelity Plot\")\n",
    "plt.savefig('label_noise_no_inter.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b4cf46-bd7e-4c96-af0b-d10298355ab9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "eta = 0.2\n",
    "n = 30000\n",
    "r = 0.2\n",
    "num_iters = 10\n",
    "\n",
    "bias_amts, mean_fidel_maj25, mean_fidel_min25, mean_fidel25, y_err_fidel_maj25, y_err_fidel_min25, y_err_fidel25, df = \\\n",
    "label_noise(r = r, n = n, apply_fairness=True,verbose=True, num_iters=num_iters, diff_base = False, inter = True)\n",
    "\n",
    "df.to_csv('label_noise_inter.csv', index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569c31a0-a07e-478b-8738-6f068836bd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_amts = np.divide(list(range(1,11,1)),22)\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.errorbar(bias_amts, 1-mean_fidel_maj25, yerr = y_err_fidel_maj25, label = 'Fidelity Majority', color = \"red\")\n",
    "plt.errorbar(bias_amts, 1-mean_fidel_min25, yerr = y_err_fidel_min25, label = 'Fidelity Minority', color = \"green\")\n",
    "plt.xlabel(\"Eta Minority Value\")\n",
    "plt.ylabel(\"Fidelity\")\n",
    "#plt.ylim(0.7,1)\n",
    "plt.legend()\n",
    "plt.title(\"Fidelity Plot\")\n",
    "plt.savefig('label_noise_inter.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0392aa7e-446f-406f-8bf1-44b8ad247ffb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "eta = 0.2\n",
    "n = 30000\n",
    "r = 0.2\n",
    "num_iters = 10\n",
    "\n",
    "bias_amts, mean_fidel_maj26, mean_fidel_min26, mean_fidel26, y_err_fidel_maj26, y_err_fidel_min26, y_err_fidel26, df = \\\n",
    "label_noise(r = r, n = n, apply_fairness=True,verbose=True, num_iters=num_iters, diff_base = True, inter = True)\n",
    "\n",
    "df.to_csv('label_noise_diff_base.csv', index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012893d8-819b-4e3e-b724-36bf6e91fdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_amts = np.divide(list(range(1,11,1)),22)\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.errorbar(bias_amts, 1-mean_fidel_maj26, yerr = y_err_fidel_maj26, label = 'Fidelity Majority', color = \"red\")\n",
    "plt.errorbar(bias_amts, 1-mean_fidel_min26, yerr = y_err_fidel_min26, label = 'Fidelity Minority', color = \"green\")\n",
    "plt.xlabel(\"Eta Minority Value\")\n",
    "plt.ylabel(\"Fidelity\")\n",
    "#plt.ylim(0.7,1)\n",
    "plt.legend()\n",
    "plt.title(\"Fidelity Plot\")\n",
    "plt.savefig('label_noise_diff_base.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e5dce5-46b4-4d5c-a267-772edce57f04",
   "metadata": {},
   "source": [
    "## Feature Noise/Missingness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de841c3f-270f-44f2-ae25-a1e8c7761b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inject_noise_num(df, feature, eta, eps = 1):\n",
    "    feats = df[feature].values\n",
    "    #print('Before:', df_synthetic['outcome'].value_counts())\n",
    "    num_flipped = 0\n",
    "    for i in range(len(feats)):\n",
    "        if random.uniform(0,1) <= eta:\n",
    "            feats[i] = 0\n",
    "            num_flipped += 1\n",
    "    df[feature] = feats\n",
    "    print('Num Flipped: ', num_flipped, \"\\tRate: \", num_flipped / len(df))\n",
    "    return df\n",
    "\n",
    "# measurement bias\n",
    "def get_biased_data(df, eps, inter):\n",
    "    df_majority = df[df['cat'] == 1]\n",
    "    df_minority = df[df['cat'] == 0]\n",
    "    \n",
    "    if inter:\n",
    "    \n",
    "        # unfavored group with negative label\n",
    "        df_minority_negative = df_minority[df_minority['outcome'] == 0.0]\n",
    "\n",
    "        # unfavored group with positive label (preferred)\n",
    "        df_minority_positive = df_minority[df_minority['outcome'] == 1.0]\n",
    "\n",
    "        # data frame without positively labeled examples from minority class\n",
    "        df_total = pd.concat([df_majority, df_minority_negative])\n",
    "\n",
    "        # under-sampling process\n",
    "        df_bias = inject_noise_num(df_minority_positive, 'num1', eps)\n",
    "\n",
    "        # combine undersampled and original favored class to create dataset\n",
    "        df_concat = pd.concat([df_total,df_bias])\n",
    "    \n",
    "    else:\n",
    "        # under-sampling process\n",
    "        df_bias = inject_noise_num(df_minority, 'num1', eps)\n",
    "        # combine undersampled and original favored class to create dataset\n",
    "        df_concat = pd.concat([df_majority,df_bias])\n",
    "    \n",
    "    return df_concat.sample(frac=1, random_state = 42) # permute data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8940a097-802a-41ea-a450-e3dd7bf94dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_noise(r, n, apply_fairness = True, verbose = False, num_iters = 10, inter = False, diff_base = False):\n",
    "    \n",
    "    total_fidel_maj = []\n",
    "    total_fidel_min = []\n",
    "    total_fidel = []\n",
    "    \n",
    "    total_disp_bias_train = []\n",
    "    total_disp_bo_train = []\n",
    "    total_disp_mitigated_train = []\n",
    "    \n",
    "    total_disp_bias_test = []\n",
    "    total_disp_bo_test = []\n",
    "    total_disp_mitigated_test = []\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "    \n",
    "        # 1 to 0 in increments of 0.1\n",
    "        bias_amts = np.divide(list(range(0,11,1)),10)\n",
    "        bias_amts[0] = 0.01 # doesn't work with 0\n",
    "\n",
    "        test_maj = []\n",
    "        test_min = []\n",
    "        total = []\n",
    "        \n",
    "        disp_bias_train = []\n",
    "        disp_bo_train = []\n",
    "        disp_mitigated_train = []\n",
    "        \n",
    "        disp_bias_test = []\n",
    "        disp_bo_test = []\n",
    "        disp_mitigated_test = []\n",
    "        \n",
    "        count = 0\n",
    "        \n",
    "        if diff_base: \n",
    "            outcome_continuous, df_synthetic = true_label_generation(r=r, eta=eta, n=2*n, maj_means =[0.7,0.7,0.7])\n",
    "        else: outcome_continuous, df_synthetic = true_label_generation(r=r, eta=eta, n=2*n)        \n",
    "        \n",
    "        threshold = 0.5\n",
    "        exact_bo_labels = np.where(outcome_continuous < threshold, 0, 1)\n",
    "        exact_bo_labels_train = np.array(exact_bo_labels[range(0,n)])\n",
    "        exact_bo_labels_test = np.array(exact_bo_labels[range(n,len(df_synthetic))])\n",
    "        \n",
    "        # split into train and test\n",
    "        df_train = df_synthetic.loc[range(0,n), :]\n",
    "        \n",
    "        df_train_transf, maj_list, min_list = transform(df_train, True, True)\n",
    "        \n",
    "        df_test = df_synthetic.loc[range(n, len(df_synthetic)),:]\n",
    "        df_test_transf, maj_list, min_list = transform(df_test, True, False)\n",
    "        \n",
    "        df_test_maj = df_test_transf.loc[maj_list]\n",
    "        df_test_min = df_test_transf.loc[min_list]\n",
    "\n",
    "        # format training data\n",
    "        X_true = df_train_transf.iloc[:, :-1].values\n",
    "        y_true = df_train_transf.iloc[:, -1].values\n",
    "        \n",
    "        # format test data\n",
    "        X_test = df_test_transf.iloc[:, :-1].values\n",
    "        X_test_maj = df_test_maj.iloc[:, :-1].values\n",
    "        X_test_min = df_test_min.iloc[:, :-1].values\n",
    "        y_test = df_test_transf.iloc[:, -1].values\n",
    "        y_test_maj = df_test_maj.iloc[:, -1].values\n",
    "        y_test_min = df_test_min.iloc[:, -1].values\n",
    "        \n",
    "        sens_attr_test = df_test['cat']\n",
    "        \n",
    "        for beta in bias_amts:\n",
    "            \n",
    "            if i == 0: print(\"Beta: \", beta, '\\n')\n",
    "\n",
    "            df_train_copy = df_train.copy()\n",
    "\n",
    "            df_majority = df_train_copy[df_train_copy['cat'] == 1]\n",
    "            df_minority = df_train_copy[df_train_copy['cat'] == 0]\n",
    "\n",
    "            # unfavored group with negative label\n",
    "            df_minority_negative = df_minority[df_minority['outcome'] == 0.0]\n",
    "\n",
    "            # unfavored group with positive label (preferred)\n",
    "            df_minority_positive = df_minority[df_minority['outcome'] == 1.0]\n",
    "\n",
    "            # data frame without positively labeled examples from minority class\n",
    "            df_total = pd.concat([df_majority, df_minority_negative])\n",
    "\n",
    "            # under-sampling process\n",
    "            df_undersampled = under(df_minority_positive, 1)\n",
    "\n",
    "            # combine undersampled and original favored class to create dataset\n",
    "            df_concat1 = pd.concat([df_total,df_undersampled])\n",
    "            \n",
    "            df_concat = get_biased_data(df_concat1, beta, inter)\n",
    "\n",
    "            # format data\n",
    "            X_bias_true, y_bias_true, df_sens = transform(df_concat)\n",
    "            \n",
    "            # model trained on biased data\n",
    "            classifier = LogisticRegression(solver = 'lbfgs', random_state = 42, penalty = 'none')\n",
    "            classifier_bias = classifier.fit(X_bias_true, y_bias_true)\n",
    "            \n",
    "            acc = accuracy_score(y_test,classifier_bias.predict(X_test))\n",
    "            #print(f'Biased classifier:')\n",
    "            #print(f'     Test accuracy = {acc}')\n",
    "            \n",
    "            m = classifier_bias\n",
    "            def classify(X): return m.predict(X)\n",
    "            error = ErrorRate()\n",
    "            error.load_data(X_bias_true, pd.Series(y_bias_true), sensitive_features=df_sens)\n",
    "            disparity = EqualizedOdds()\n",
    "            disparity.load_data(X_bias_true, pd.Series(y_bias_true), sensitive_features=df_sens)\n",
    "            #print(f'     Train error = {error.gamma(classify)[0]}')\n",
    "            #print(f'     Train disparity = {disparity.gamma(classify).max()}')\n",
    "            disp_bias_train += [disparity.gamma(classify).max()]\n",
    "\n",
    "            m = classifier_bias\n",
    "            def classify(X): return m.predict(X)\n",
    "            error = ErrorRate()\n",
    "            error.load_data(X_test, pd.Series(y_test), sensitive_features=sens_attr_test)\n",
    "            disparity = EqualizedOdds()\n",
    "            disparity.load_data(X_test, pd.Series(y_test), sensitive_features=sens_attr_test)\n",
    "            #print(f'     Test error = {error.gamma(classify)[0]}')\n",
    "            #print(f'     Test disparity = {disparity.gamma(classify).max()}')\n",
    "            disp_bias_test += [disparity.gamma(classify).max()]\n",
    "            \n",
    "            # Learned bayes optimal classifier\n",
    "            classifier = LogisticRegression(solver = 'lbfgs', random_state = 42, penalty = 'none')\n",
    "            classifier_b = classifier.fit(X_true, y_true)                \n",
    "            #classifier_b = clone(classifier).fit(X_true, y_true)\n",
    "            acc = accuracy_score(y_test,classifier_b.predict(X_test))\n",
    "            #print(f'Learned BO classifier:')\n",
    "            #print(f'     Test accuracy = {acc}')\n",
    "            \n",
    "            m = classifier_b\n",
    "            def classify(X): return m.predict(X)\n",
    "            error = ErrorRate()\n",
    "            error.load_data(X_bias_true, pd.Series(y_bias_true), sensitive_features=df_sens)\n",
    "            disparity = EqualizedOdds()\n",
    "            disparity.load_data(X_bias_true, pd.Series(y_bias_true), sensitive_features=df_sens)\n",
    "            #print(f'     Train error = {error.gamma(classify)[0]}')\n",
    "            #print(f'     Train disparity = {disparity.gamma(classify).max()}')\n",
    "            disp_bo_train += [disparity.gamma(classify).max()]\n",
    "\n",
    "            m = classifier_b\n",
    "            def classify(X): return m.predict(X)\n",
    "            error = ErrorRate()\n",
    "            error.load_data(X_test, pd.Series(y_test), sensitive_features=sens_attr_test)\n",
    "            disparity = EqualizedOdds()\n",
    "            disparity.load_data(X_test, pd.Series(y_test), sensitive_features=sens_attr_test)\n",
    "            #print(f'     Test error = {error.gamma(classify)[0]}')\n",
    "            #print(f'     Test disparity = {disparity.gamma(classify).max()}')\n",
    "            disp_bo_test += [disparity.gamma(classify).max()]\n",
    "            \n",
    "            \n",
    "            # Exact BO optimal classifier\n",
    "            #print(f'Exact BO classifier:')\n",
    "            acc = accuracy_score(y_test,exact_bo_labels_test)\n",
    "            #print(f'     Test accuracy = {acc}')\n",
    "            acc = accuracy_score(y_true,exact_bo_labels_train)\n",
    "            #print(f'     Train accuracy = {acc}')\n",
    "\n",
    "            if apply_fairness:\n",
    "                constraint = EqualizedOdds()\n",
    "                classifier = LogisticRegression(solver = 'lbfgs', random_state = 42, penalty = 'none', fit_intercept = False, max_iter = 200)   \n",
    "\n",
    "                classifier_mitigated_bias = GridSearch(estimator=classifier,\n",
    "                                                       constraints=constraint,\n",
    "                                                       selection_rule='tradeoff_optimization',\n",
    "                                                       constraint_weight=0.5,\n",
    "                                                       grid_size=10,\n",
    "                                                       grid_limit=2.0,\n",
    "                                                       grid_offset=None,\n",
    "                                                       grid=None,\n",
    "                                                       sample_weight_name='sample_weight')\n",
    "                                                       \n",
    "                classifier_mitigated_bias.fit(X_bias_true, y_bias_true, sensitive_features = df_sens)\n",
    "                \n",
    "                acc = accuracy_score(y_test,classifier_mitigated_bias.predict(X_test))\n",
    "                #print(f'Mitigated bias classifier:')\n",
    "                #print(f'     Test accuracy = {acc}')\n",
    "                \n",
    "                m = classifier_mitigated_bias\n",
    "                def classify(X): return m.predict(X)\n",
    "                error = ErrorRate()\n",
    "                error.load_data(X_bias_true, pd.Series(y_bias_true), sensitive_features=df_sens)\n",
    "                disparity = EqualizedOdds()\n",
    "                disparity.load_data(X_bias_true, pd.Series(y_bias_true), sensitive_features=df_sens)\n",
    "                #print(f'     Train error = {error.gamma(classify)[0]}')\n",
    "                #print(f'     Train disparity = {disparity.gamma(classify).max()}')\n",
    "                disp_mitigated_train += [disparity.gamma(classify).max()]\n",
    "                \n",
    "                m = classifier_mitigated_bias\n",
    "                def classify(X): return m.predict(X)\n",
    "                error = ErrorRate()\n",
    "                error.load_data(X_test, pd.Series(y_test), sensitive_features=sens_attr_test)\n",
    "                disparity = EqualizedOdds()\n",
    "                disparity.load_data(X_test, pd.Series(y_test), sensitive_features=sens_attr_test)\n",
    "                #print(f'     Test error = {error.gamma(classify)[0]}')\n",
    "                #print(f'     Test disparity = {disparity.gamma(classify).max()}')\n",
    "                disp_mitigated_test += [disparity.gamma(classify).max()]\n",
    "                \n",
    "                # Alternative fidelity of intervention model to no intervention model\n",
    "                alt_fid_train = accuracy_score(classifier_mitigated_bias.predict(X_bias_true),classifier_bias.predict(X_bias_true))\n",
    "                alt_fid_test = accuracy_score(classifier_mitigated_bias.predict(X_test),classifier_bias.predict(X_test))\n",
    "                #print(f'Alternative fidelity of intervention model to no intervention model: train = {alt_fid_train}, test = {alt_fid_test}')\n",
    "                \n",
    "\n",
    "            else:\n",
    "                classifier_mitigated_bias = clone(classifier_bias)\n",
    "                classifier_mitigated_bias.fit(X_bias_true, y_bias_true)\n",
    "                \n",
    "                # NOTE: disparities are the same as for classifier_bias\n",
    "\n",
    "                \n",
    "            # Fidelity in this step\n",
    "            fid = accuracy_score(classifier_mitigated_bias.predict(X_test),classifier_b.predict(X_test))\n",
    "            #print(f'Test set fidelity of learned BO classifier and mitigated_bias_classifier: {fid}')\n",
    "            fid = accuracy_score(classifier_mitigated_bias.predict(X_test),exact_bo_labels_test)\n",
    "            #print(f'Test set fidelity of exact BO classifier and mitigated biased classifier: {fid}')\n",
    "            fid = accuracy_score(classifier_bias.predict(X_test),classifier_b.predict(X_test))\n",
    "            #print(f'Test set fidelity of learned BO classifier and biased classifier: {fid}')\n",
    "            \n",
    "            # fidelity check\n",
    "            test_maj += [accuracy_score(classifier_mitigated_bias.predict(X_test_maj), classifier_b.predict(X_test_maj))]\n",
    "            test_min += [accuracy_score(classifier_mitigated_bias.predict(X_test_min), classifier_b.predict(X_test_min))]\n",
    "            total += [accuracy_score(classifier_mitigated_bias.predict(X_test), classifier_b.predict(X_test))]\n",
    "                \n",
    "            if verbose:\n",
    "                print(\"Finished Iteration: \", count)\n",
    "                count +=1\n",
    "        \n",
    "        #print(f'Fidelity test maj: {test_maj}')\n",
    "        #print(f'Fidelity test min: {test_maj}')\n",
    "        \n",
    "        total_fidel_maj.append(test_maj)\n",
    "        total_fidel_min.append(test_min)\n",
    "        total_fidel.append(total)\n",
    "        \n",
    "        total_disp_bias_train.append(disp_bias_train)\n",
    "        total_disp_bo_train.append(disp_bo_train)\n",
    "        total_disp_mitigated_train.append(disp_mitigated_train)\n",
    "\n",
    "        total_disp_bias_test.append(disp_bias_test)\n",
    "        total_disp_bo_test.append(disp_bo_test)\n",
    "        total_disp_mitigated_test.append(disp_mitigated_test)\n",
    "        \n",
    "        if verbose:\n",
    "                print(\"Finished Total Iteration: \", i+1)\n",
    "    \n",
    "    mean_fidel_maj = np.mean(total_fidel_maj, axis = 0)\n",
    "    mean_fidel_min = np.mean(total_fidel_min, axis = 0)\n",
    "    mean_fidel = np.mean(total_fidel, axis = 0)\n",
    "    \n",
    "    mean_disp_bias_train = np.mean(total_disp_bias_train, axis = 0)\n",
    "    mean_disp_bo_train = np.mean(total_disp_bo_train, axis = 0)\n",
    "    mean_disp_mitigated_train = np.mean(total_disp_mitigated_train, axis = 0)\n",
    "    \n",
    "    mean_disp_bias_test = np.mean(total_disp_bias_test, axis = 0)\n",
    "    mean_disp_bo_test = np.mean(total_disp_bo_test, axis = 0)\n",
    "    mean_disp_mitigated_test = np.mean(total_disp_mitigated_test, axis = 0)\n",
    "    \n",
    "    y_err_fidel_maj = np.std(total_fidel_maj, axis = 0)\n",
    "    y_err_fidel_min = np.std(total_fidel_min, axis = 0)\n",
    "    y_err_fidel = np.std(total_fidel, axis = 0)\n",
    "    \n",
    "    df = pd.DataFrame({\"Biased Train\" : mean_disp_bias_train,\n",
    "                       \"BO Train\" : mean_disp_bo_train,\n",
    "                       \"Mitigated Train\" : mean_disp_mitigated_train,\n",
    "                       \"Biased Test\" : mean_disp_bias_test,\n",
    "                       \"BO Test\" : mean_disp_bo_test,\n",
    "                       \"Mitigated Test\" : mean_disp_mitigated_test})\n",
    "   \n",
    "    return bias_amts, mean_fidel_maj, mean_fidel_min, mean_fidel, y_err_fidel_maj, y_err_fidel_min, y_err_fidel, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb42019-2897-46ba-82c2-dc1bf85608d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc155cbf-30ac-4b4b-ae71-ecbb2ef26f6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "eta = 0.2\n",
    "n = 30000\n",
    "r = 0.2\n",
    "num_iters = 10\n",
    "\n",
    "bias_amts, mean_fidel_maj27, mean_fidel_min27, mean_fidel27, y_err_fidel_maj27, y_err_fidel_min27, y_err_fidel27, df = \\\n",
    "label_noise(r = r, n = n, apply_fairness=True,verbose=True, num_iters=num_iters, diff_base = False, inter = False)\n",
    "\n",
    "df.to_csv('featmissing_no_inter.csv', index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070066c7-979c-4c74-b16e-3e57edfa99bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bias_amts = np.divide(list(range(0,11,1)),10)\n",
    "bias_amts[0] = 0.01 # doesn't work with 0\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.errorbar(bias_amts, 1-mean_fidel_maj27, yerr = y_err_fidel_maj27, label = 'Fidelity Majority', color = \"red\")\n",
    "plt.errorbar(bias_amts, 1-mean_fidel_min27, yerr = y_err_fidel_min27, label = 'Fidelity Minority', color = \"green\")\n",
    "plt.xlabel(\"Eta Minority Value\")\n",
    "plt.ylabel(\"Fidelity\")\n",
    "plt.ylim(0.7, 1)\n",
    "plt.legend()\n",
    "plt.title(\"Fidelity Plot\")\n",
    "plt.savefig('featmissing_no_inter.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c795647-4294-4493-b70f-d13db39da17a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "eta = 0.2\n",
    "n = 30000\n",
    "r = 0.2\n",
    "num_iters = 10\n",
    "\n",
    "bias_amts, mean_fidel_maj28, mean_fidel_min28, mean_fidel28, y_err_fidel_maj28, y_err_fidel_min28, y_err_fidel28, df = \\\n",
    "label_noise(r = r, n = n, apply_fairness=True,verbose=True, num_iters=num_iters, diff_base = False, inter = True)\n",
    "\n",
    "df.to_csv('featmissing_inter.csv', index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f42439-f076-4ab9-a435-da96d4e5405f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_amts = np.divide(list(range(0,11,1)),10)\n",
    "bias_amts[0] = 0.01 # doesn't work with 0\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.errorbar(bias_amts, 1-mean_fidel_maj28, yerr = y_err_fidel_maj28, label = 'Fidelity Majority', color = \"red\")\n",
    "plt.errorbar(bias_amts, 1-mean_fidel_min28, yerr = y_err_fidel_min28, label = 'Fidelity Minority', color = \"green\")\n",
    "plt.xlabel(\"Eta Minority Value\")\n",
    "plt.ylabel(\"Fidelity\")\n",
    "plt.ylim(0.7, 1)\n",
    "plt.legend()\n",
    "plt.title(\"Fidelity Plot\")\n",
    "plt.savefig('featmissing_inter.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48847578-9444-4f60-a690-6b1277c34644",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "eta = 0.2\n",
    "n = 30000\n",
    "r = 0.2\n",
    "num_iters = 10\n",
    "\n",
    "bias_amts, mean_fidel_maj29, mean_fidel_min29, mean_fidel29, y_err_fidel_maj29, y_err_fidel_min29, y_err_fidel29, df = \\\n",
    "label_noise(r = r, n = n, apply_fairness=True,verbose=True, num_iters=num_iters, diff_base = True, inter = True)\n",
    "\n",
    "df.to_csv('featmissing_diff_base.csv', index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcc46d7-ec05-45fe-9ec3-c45729984da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_amts = np.divide(list(range(0,11,1)),10)\n",
    "bias_amts[0] = 0.01 # doesn't work with 0\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.errorbar(bias_amts, 1-mean_fidel_maj29, yerr = y_err_fidel_maj29, label = 'Fidelity Majority', color = \"red\")\n",
    "plt.errorbar(bias_amts, 1-mean_fidel_min29, yerr = y_err_fidel_min29, label = 'Fidelity Minority', color = \"green\")\n",
    "plt.xlabel(\"Eta Minority Value\")\n",
    "plt.ylabel(\"Fidelity\")\n",
    "plt.ylim(0.7, 1)\n",
    "plt.legend()\n",
    "plt.title(\"Fidelity Plot\")\n",
    "plt.savefig('featmissing_diff_base.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32815e3a-00af-4da5-93c6-2fcd13f89eef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b8827a-9501-4419-aaee-b6892cff64c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
