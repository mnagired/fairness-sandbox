{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5e91243",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-08T20:41:26.879421Z",
     "start_time": "2021-06-08T20:41:26.876140Z"
    }
   },
   "source": [
    "# Under-Representation Bias (w/ Synthetic Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00909876",
   "metadata": {},
   "source": [
    "This notebook recreates the finding that Equalized Odds constrained model can recover from under-representation bias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a42ced1",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "Please run the code block below to install the necessary packages (if needed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a692724",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T20:42:02.586362Z",
     "start_time": "2021-06-10T20:42:02.581026Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "from sklearn.base import clone\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, roc_curve, auc\n",
    "from collections import Counter\n",
    "\n",
    "import fairlearn\n",
    "from fairlearn.metrics import *\n",
    "from fairlearn.reductions import *\n",
    "import aif360\n",
    "\n",
    "import copy, random\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da59e303",
   "metadata": {},
   "source": [
    "# Synthetic Dataset Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d46dc53-d064-4c1f-8845-891216d77cde",
   "metadata": {},
   "source": [
    "## Parameters (User Input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e4c1bcf-a112-4a4f-9d02-a6724a2268b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "r is the proportion of training examples in the minority group, \n",
    "\n",
    "which means 1-r is proportion of examples in the majority group\n",
    "\n",
    "eta is the probability of flipping the label\n",
    "\n",
    "n is the number of training examples\n",
    "\n",
    "beta is the probability of keeping a positively labeled example\n",
    "from the minority class\n",
    "\n",
    "NOTE: results can be replicated if and only if the following condition holds:\n",
    "\n",
    "(1-r)(1-2*eta) + r((1-eta)*beta - eta) > 0\n",
    "\n",
    "'''\n",
    "def get_params(r = 1/3, eta = 1/4, n = 2000, beta = 0.5):\n",
    "    return r, eta, n, beta\n",
    "\n",
    "r, eta, n, beta = get_params(r = 1/3, eta = 0, n = 30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e7d3c98-54c3-4198-9f03-528bebb016cc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constraint:  0.19999999999999996\n",
      "yes! 0.2 0.4 1.0\n",
      "constraint:  0.18799999999999997\n",
      "yes! 0.2 0.4 0.9\n",
      "constraint:  0.17599999999999996\n",
      "yes! 0.2 0.4 0.8\n",
      "constraint:  0.16399999999999998\n",
      "yes! 0.2 0.4 0.7\n",
      "constraint:  0.15199999999999997\n",
      "yes! 0.2 0.4 0.6\n",
      "constraint:  0.13999999999999996\n",
      "yes! 0.2 0.4 0.5\n",
      "constraint:  0.12799999999999997\n",
      "yes! 0.2 0.4 0.4\n",
      "constraint:  0.11599999999999996\n",
      "yes! 0.2 0.4 0.3\n",
      "constraint:  0.10399999999999997\n",
      "yes! 0.2 0.4 0.2\n",
      "constraint:  0.09199999999999997\n",
      "yes! 0.2 0.4 0.1\n",
      "constraint:  0.07999999999999996\n",
      "yes! 0.2 0.4 0.0\n"
     ]
    }
   ],
   "source": [
    "# check if above constraint holds\n",
    "def check_constraints(r, eta, beta):\n",
    "    first = (1-r)*(1-2*eta)\n",
    "    second = r * ((1-eta)*beta - eta)\n",
    "    res = first + second\n",
    "    print(\"constraint: \", res)\n",
    "    print(\"yes!\", r, eta, beta) if res > 0 else print(\"no\", r, eta, beta)\n",
    "    \n",
    "bias_amts = np.divide(list(range(10, -1, -1)),10)\n",
    "\n",
    "for beta in bias_amts:\n",
    "    check_constraints(r=0.2, eta=0.4, beta=beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270ebbd2-719c-42d5-8de4-dce388fc0a51",
   "metadata": {},
   "source": [
    "## True Label Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74f9b8a4-c293-4c8a-9c8c-72d28f624312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create minority and majority groups\n",
    "def get_cat_features(n, r):\n",
    "    num_minority = int(r * n)\n",
    "    num_majority = n - num_minority\n",
    "    \n",
    "    minority = np.zeros((num_minority, 1))\n",
    "    majority = np.ones((num_majority, 1))\n",
    "    \n",
    "    cat_features = np.vstack((minority, majority))\n",
    "    #np.random.shuffle(cat_features) # this is what causes us to not recover coeffs\n",
    "    \n",
    "    return cat_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "acba1061-c071-4a58-ae21-706122184dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# return labels from Bayes Optimal Classifier\n",
    "def get_bayes_optimal_labels(features, effect_param, threshold = 0.5):\n",
    "    outcome_continuous = 1/(1+np.exp(-np.matmul(features, effect_param)))\n",
    "    #return outcome_continuous, np.random.binomial(1,outcome_continuous) # bernoulli to simulate LR's probabilistic nature\n",
    "    return outcome_continuous, np.where(outcome_continuous < threshold, 0, 1)\n",
    "\n",
    "# flip labels with probability eta\n",
    "def flip_labels(df_synthetic, eta):\n",
    "    labels = df_synthetic['outcome']\n",
    "    #print('Before:', df_synthetic['outcome'].value_counts())\n",
    "    num_flipped = 0\n",
    "    for i in range(len(labels)):\n",
    "        if random.uniform(0,1) <= eta:\n",
    "            labels[i] = 1 if labels[i] == 0 else 0\n",
    "            num_flipped += 1\n",
    "    df_synthetic['outcome'] = labels\n",
    "    #print('After:', df_synthetic['outcome'].value_counts())\n",
    "    #print('Num Flipped: ', num_flipped, \"\\tRate: \", num_flipped / len(df_synthetic))\n",
    "    return df_synthetic\n",
    "\n",
    "def flip_labels2(df_majority, eta_maj, df_minority, eta_min):\n",
    "    labels_maj = df_majority['outcome'].values\n",
    "    labels_min = df_minority['outcome'].values\n",
    "    \n",
    "    num_flipped_maj = 0\n",
    "    for i in range(len(labels_maj)):\n",
    "        if random.uniform(0,1) <= eta_maj:\n",
    "            labels_maj[i] = 1 if labels_maj[i] == 0 else 0\n",
    "            num_flipped_maj += 1\n",
    "    df_majority['outcome'] = labels_maj\n",
    "    print('Num Flipped Maj: ', num_flipped_maj, \"\\tRate: \", num_flipped_maj / len(df_majority))\n",
    "    \n",
    "    num_flipped_min = 0\n",
    "    for i in range(len(labels_min)):\n",
    "        if random.uniform(0,1) <= eta_min:\n",
    "            labels_min[i] = 1 if labels_min[i] == 0 else 0\n",
    "            num_flipped_min += 1\n",
    "    df_minority['outcome'] = labels_min\n",
    "    print('Num Flipped Min: ', num_flipped_min, \"\\tRate: \", num_flipped_min / len(df_minority))\n",
    "    \n",
    "    df_concat = pd.concat([df_majority, df_minority])\n",
    "    return df_concat.sample(frac=1, random_state = 42) # permute data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9faebb17-ed24-453b-a179-16933179c612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diff:  0.00550000000000006\n"
     ]
    }
   ],
   "source": [
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "'''\n",
    "\n",
    "create synthetic data with:\n",
    "    3 numerical features (Gaussian), 1 categorical (sensitive attribute) \n",
    "    logistic outcome model s.t. outcome = Indicator[logit(effect_param*features) >= 0.5]\n",
    "    \n",
    "create minority/majority groups according to r param\n",
    "\n",
    "simulate Bayes Optimal Classifiers for minority and majority\n",
    "\n",
    "flip labels according to eta param\n",
    "\n",
    "ensure equal base rates (proportion of positive examples) across both groups\n",
    "\n",
    "'''\n",
    "\n",
    "def true_label_generation(r, eta, n, maj_means = [0,0,0]):\n",
    "\n",
    "    ''' \n",
    "    delete this variable to allow user to control percentage of positively labeled examples\n",
    "    eg: let outcome_continuous >= 0.2 implies 80% positively labeled samples\n",
    "    '''\n",
    "    # causal effect params\n",
    "    maj_params = [-0.7, 0.5, 1.5]\n",
    "    effect_param_min = [0.5, -0.2, 0.1]\n",
    "    #effect_param_maj = [i + np.random.uniform(low = -1, high = 1) for i in maj_params]\n",
    "    effect_param_maj = maj_params\n",
    "    \n",
    "    num_min = int(n*r)\n",
    "    num_maj = n - num_min\n",
    "\n",
    "    # required: len(cat_probabilities) = n_cat_features\n",
    "    n_cat_features = 2\n",
    "    cat_probabilities = [0.5, 0.5] \n",
    "\n",
    "    # numerical feature params\n",
    "    means = [0, 0, 0]\n",
    "    cov_matrix = [[1, 0, 0], [0, 1, 0], [0, 0, 1]]\n",
    "\n",
    "    # features\n",
    "    cat_features = get_cat_features(r=r, n=n)\n",
    "    \n",
    "    num_features_min = np.random.multivariate_normal(means, cov_matrix, num_min)\n",
    "    #num_features_min = np.random.normal(0, 1, num_min)\n",
    "    num_features_maj = np.random.multivariate_normal(maj_means, cov_matrix, num_maj)\n",
    "    # num_features_maj = np.array([i + np.random.uniform(low = -15, high = 10) for i in num_features_maj])\n",
    "    #num_features_min = np.random.normal(0, 1, num_min)\n",
    "\n",
    "    num_features = np.concatenate((num_features_min, num_features_maj))\n",
    "\n",
    "    # outcomes\n",
    "    outcome_continuous_min, outcome_binary_min = get_bayes_optimal_labels(features=num_features_min, effect_param=effect_param_min, threshold = 0.5)\n",
    "    #outcome_binary_min = np.where(np.matmul(num_features_min, effect_param_min) > 0.5, 1, 0)\n",
    "    outcome_continuous_maj, outcome_binary_maj = get_bayes_optimal_labels(features=num_features_maj, effect_param=effect_param_maj, threshold = 0.5)\n",
    "    #outcome_binary_maj = np.where(np.matmul(num_features_maj, effect_param_maj) > 0.5, 1, 0)\n",
    "    \n",
    "    outcome = np.hstack((outcome_binary_min,outcome_binary_maj)).reshape(n,1)\n",
    "    outcome_continuous = np.hstack((outcome_continuous_min,outcome_continuous_maj)).reshape(n,1)\n",
    "    temp_data = np.hstack((num_features,cat_features, outcome, outcome_continuous))\n",
    "    #print(outcome_continuous)\n",
    "    #print(np.where(outcome_continuous < 0.5, 0, 1))\n",
    "    np.random.shuffle(temp_data) # randomly shuffle the data\n",
    "    \n",
    "    df_synthetic = pd.DataFrame(temp_data)\n",
    "    df_synthetic.columns = ['num1','num2','num3','cat','outcome','outcome_cont']\n",
    "    \n",
    "    outcome_continuous = df_synthetic.outcome_cont\n",
    "    df_synthetic = df_synthetic[['num1','num2','num3','cat','outcome']]\n",
    "    \n",
    "    df_majority = df_synthetic[df_synthetic['cat'] == 1]\n",
    "    df_minority = df_synthetic[df_synthetic['cat'] == 0]\n",
    "    \n",
    "    #print('% Positive Majority: ', df_majority['outcome'].value_counts()[1] / len(df_majority))\n",
    "    #print('\\n% Positive Minority: ', df_minority['outcome'].value_counts()[1] / len(df_minority))\n",
    "    print('Diff: ', df_majority['outcome'].value_counts()[1] / len(df_majority)- df_minority['outcome'].value_counts()[1] / len(df_minority))\n",
    "    #print('\\nTotal: ', df_majority['outcome'].value_counts())\n",
    "    \n",
    "    df_synthetic = flip_labels(df_synthetic, eta)\n",
    "    \n",
    "    # uncomment below to add diff eta for majority/minority\n",
    "    #df_synthetic = flip_labels2(df_majority, 0.2, df_minority, eta) # group dependent label noise\n",
    "    \n",
    "    return outcome_continuous, df_synthetic \n",
    "\n",
    "outcome_continuous, df_synthetic = true_label_generation(r=r, eta=eta, n=2*n, maj_means = [0,0,0])\n",
    "#xx = np.where(outcome_continuous < 0.5, 0, 1)\n",
    "#print(eta)\n",
    "#print(accuracy_score(xx,df_synthetic.outcome.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a7fd87-924b-41b0-a505-25cac190e8cd",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d385bc3-625f-4726-a2b3-7dac659fda04",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "240d919e-9d87-4429-9f42-e4f2a938fac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and test\n",
    "df_train = df_synthetic.loc[range(0,int(n/2)), :]\n",
    "# if original dataset has odd number of samples, remove 1 sample to be even\n",
    "if (n % 2 == 1):\n",
    "    df_test = df_synthetic.loc[range(int(n/2)+1, n), :]\n",
    "else:\n",
    "    df_test = df_synthetic.loc[range(int(n/2), n), :]\n",
    "\n",
    "df_fidel = df_synthetic.loc[range(n, len(df_synthetic)),:]\n",
    "outcome_cts = outcome_continuous[n:len(df_synthetic)]\n",
    "    \n",
    "df_test_maj = df_test[df_test['cat'] == 1]\n",
    "df_test_min = df_test[df_test['cat'] == 0]\n",
    "\n",
    "# format data\n",
    "X_true = df_test.iloc[:, :-1].values\n",
    "y_true = df_test.iloc[:, -1].values\n",
    "\n",
    "X_true_maj = df_test_maj.iloc[:, :-1].values\n",
    "y_true_maj = df_test_maj.iloc[:, -1].values\n",
    "X_true_min = df_test_min.iloc[:, :-1].values\n",
    "y_true_min = df_test_min.iloc[:, -1].values\n",
    "\n",
    "sens_attrs_true = [df_test['cat']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950918b3",
   "metadata": {},
   "source": [
    "# Bias Injection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c60f7195-a5e7-46d4-9308-65ae9c5c8ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# measurement bias\n",
    "def inject_noise_num(df, feature, eps = 1):\n",
    "    for i in range(len(df[feature])):\n",
    "        df[feature].iloc[i] += np.random.normal(0, 1) * eps # standard normal\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a788ace-562a-4256-a853-40db73a61e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attribute': 'num1', 'min': -3.536167190440467, 'max': 3.6024150242105293, '1st Quartile': -0.6799701958948478, '2nd Quartile': -0.007891937037584867, '3rd Quartile': 0.6561290006125509}\n",
      "{'attribute': 'num1', 'min': -3.536167190440467, 'max': 3.491666985559067, '1st Quartile': -0.7442475207394728, '2nd Quartile': -0.07944159937711642, '3rd Quartile': 0.5793751959247074}\n"
     ]
    }
   ],
   "source": [
    "from numpy import percentile\n",
    "def under_sample(df_minority_positive, beta):\n",
    "    X_min = df_minority_positive.iloc[:, :].values\n",
    "    \n",
    "    # keep each example with probability beta\n",
    "    num_dropped = 0\n",
    "    for i in range(len(X_min)):\n",
    "        if random.uniform(0,1) > beta:\n",
    "            X_min = np.delete(X_min, 0, axis=0)\n",
    "            num_dropped += 1\n",
    "    #print(\"Total Deleted: \", num_dropped, \"\\t % Deleted: \", num_dropped / len(df_minority_positive))\n",
    "    df_minority_positive = pd.DataFrame(pd.DataFrame(X_min))\n",
    "    df_minority_positive.columns = ['num1','num2','num3','cat','outcome']\n",
    "    return df_minority_positive\n",
    "\n",
    "\n",
    "def get_biased_data(df_train, beta):\n",
    "    df_majority = df_train[df_train['cat'] == 1]\n",
    "    df_minority = df_train[df_train['cat'] == 0]\n",
    "    \n",
    "    # unfavored group with negative label\n",
    "    df_minority_negative = df_minority[df_minority['outcome'] == 0.0]\n",
    "\n",
    "    # unfavored group with positive label (preferred)\n",
    "    df_minority_positive = df_minority[df_minority['outcome'] == 1.0]\n",
    "    \n",
    "    # data frame without positively labeled examples from minority class\n",
    "    df_total = pd.concat([df_majority, df_minority_negative])\n",
    "    \n",
    "    # under-sampling process\n",
    "    df_undersampled = under_sample(df_minority_positive, beta)\n",
    "\n",
    "    # combine undersampled and original favored class to create dataset\n",
    "    df_concat = pd.concat([df_total,df_undersampled])\n",
    "    \n",
    "    return df_concat.sample(frac=1) # permute data\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "# representation bias, under-sampling entire minority\n",
    "def get_biased_data(df_train, beta):\n",
    "    df_majority = df_train[df_train['cat'] == 1]\n",
    "    df_minority = df_train[df_train['cat'] == 0]\n",
    "    \n",
    "    # under-sampling process\n",
    "    df_undersampled = under_sample(df_minority, beta)\n",
    "\n",
    "    # combine undersampled and original favored class to create dataset\n",
    "    df_concat = pd.concat([df_majority,df_undersampled])\n",
    "    \n",
    "    return df_concat.sample(frac=1) # permute data\n",
    "'''\n",
    "\n",
    "def get_summary_num(df, feature):\n",
    "    res = dict()\n",
    "    res['attribute'] = feature\n",
    "\n",
    "    data_min, data_max = df[feature].min(), df[feature].max()\n",
    "    res['min'] = data_min\n",
    "    res['max'] = data_max\n",
    "\n",
    "    quartiles = percentile(df[feature], [25,50,75])\n",
    "    res['1st Quartile'] = quartiles[0]\n",
    "    res['2nd Quartile'] = quartiles[1]\n",
    "    res['3rd Quartile'] = quartiles[2]\n",
    "\n",
    "    return res\n",
    "\n",
    "#print(len(df_train[(df_train['cat'] == 0) & (df_train['outcome'] == 1)]))\n",
    "print(get_summary_num(df_train, 'num1'))\n",
    "df_concat = get_biased_data(df_train, 0.5)\n",
    "print(get_summary_num(df_concat, 'num1'))\n",
    "#print(len(df_concat[(df_concat['cat'] == 0) & (df_concat['outcome'] == 1)]))\n",
    "\n",
    "# for fairness measures later\n",
    "df_sens = df_concat['cat']\n",
    "\n",
    "# format data\n",
    "X_bias = df_concat.iloc[:, :-1].values\n",
    "y_bias = df_concat.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "213427a7-8fec-4a2a-a568-439e01b6b31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Nil-Jana's Suggestions '''\n",
    "\n",
    "def transform(df, is_test = False, is_train = False):\n",
    "\n",
    "    sens_feat = df.iloc[:, -2].values\n",
    "    outcome = df.iloc[:, -1].values\n",
    "    num_feats = df.iloc[:, :-2].values\n",
    "\n",
    "    trans_feats = [] # x1a - x3a\n",
    "    other_feats = [] # x1(1-a) - x3(1-a)\n",
    "    # -2 for sensitive feature and label\n",
    "    for i in range(len(df.columns) - 2):\n",
    "        num_feat = df.iloc[:, i].values\n",
    "        num_feat_transf = np.multiply(num_feat, sens_feat)\n",
    "        trans_feats += [num_feat_transf.reshape((len(df),))]\n",
    "        \n",
    "        num_feat_other = df.iloc[:, i].values\n",
    "        num_feat_other_transf = np.multiply(num_feat, (1-sens_feat))\n",
    "        other_feats += [num_feat_other_transf.reshape((len(df),))]\n",
    "\n",
    "\n",
    "    temp_data = np.hstack((other_feats[0].reshape((len(df),1)), other_feats[1].reshape((len(df),1)), other_feats[2].reshape((len(df),1)),\n",
    "                           trans_feats[0].reshape((len(df),1)), trans_feats[1].reshape((len(df),1)), trans_feats[2].reshape((len(df),1)),\n",
    "                           outcome.reshape((len(df),1))))\n",
    "\n",
    "    df_transf = pd.DataFrame(temp_data)\n",
    "    df_transf.columns = ['num1*(1-a)','num2*(1-a)','num3*(1-a)', 'num1*a','num2*a','num3*a','outcome']\n",
    "\n",
    "    # for fairness measures later\n",
    "    df_sens = df['cat']\n",
    "    maj_list = list(df[df['cat'] == 1].index)\n",
    "    min_list = list(df[df['cat'] == 0].index)\n",
    "    \n",
    "    for i in range(len(maj_list)):\n",
    "        if is_train: maj_list[i] = maj_list[i]\n",
    "        else: maj_list[i] = maj_list[i] - len(df)\n",
    "        \n",
    "    for i in range(len(min_list)):\n",
    "        if is_train:\n",
    "            min_list[i] = min_list[i]\n",
    "        else: min_list[i] = min_list[i] - len(df)\n",
    "\n",
    "    # format data\n",
    "    X_bias = df_transf.iloc[:, :-1].values\n",
    "    y_bias = df_transf.iloc[:, -1].values\n",
    "    \n",
    "    if not is_test:\n",
    "        return X_bias, y_bias, df_sens\n",
    "    else:\n",
    "        return df_transf, maj_list, min_list\n",
    "\n",
    "df_transf, _, _ = transform(df_test, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7ea20d",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "20ee4d59-b589-430e-b3e4-3dd928074d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def under(df, beta):\n",
    "    X_min = df.iloc[:, :].values\n",
    "    \n",
    "    # keep each example with probability beta\n",
    "    num_dropped = 0\n",
    "    for i in range(len(X_min)):\n",
    "        if random.uniform(0,1) > beta:\n",
    "            X_min = np.delete(X_min, 0, axis=0)\n",
    "            num_dropped += 1\n",
    "    print(\"Total Deleted: \", num_dropped, \"\\t % Deleted: \", num_dropped / len(df))\n",
    "    df = pd.DataFrame(pd.DataFrame(X_min))\n",
    "    df.columns = ['num1','num2','num3','cat','outcome']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0daf0345-6898-447f-9101-07ab6fd06f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tradeoff_visualization_error(classifier, r, n, apply_fairness = True, verbose = False, num_iters = 10):\n",
    "    \n",
    "    total_fidel_maj = []\n",
    "    total_fidel_min = []\n",
    "    total_fidel = []\n",
    "    \n",
    "    total_disp_bias_train = []\n",
    "    total_disp_bo_train = []\n",
    "    total_disp_mitigated_train = []\n",
    "    \n",
    "    total_disp_bias_test = []\n",
    "    total_disp_bo_test = []\n",
    "    total_disp_mitigated_test = []\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "    \n",
    "        # 1 to 0 in increments of 0.1\n",
    "        bias_amts = np.divide(list(range(10,-1,-1)),10)\n",
    "\n",
    "        test_maj = []\n",
    "        test_min = []\n",
    "        total = []\n",
    "        \n",
    "        disp_bias_train = []\n",
    "        disp_bo_train = []\n",
    "        disp_mitigated_train = []\n",
    "        \n",
    "        disp_bias_test = []\n",
    "        disp_bo_test = []\n",
    "        disp_mitigated_test = []\n",
    "        \n",
    "        count = 0\n",
    "        outcome_continuous, df_synthetic = true_label_generation(r=r, eta=eta, n=2*n)\n",
    "        \n",
    "        threshold = 0.5\n",
    "        exact_bo_labels = np.where(outcome_continuous < threshold, 0, 1)\n",
    "        exact_bo_labels_train = np.array(exact_bo_labels[range(0,n)])\n",
    "        exact_bo_labels_test = np.array(exact_bo_labels[range(n,len(df_synthetic))])\n",
    "        \n",
    "        # split into train and test\n",
    "        df_train = df_synthetic.loc[range(0,n), :]\n",
    "        \n",
    "        df_train_transf, maj_list, min_list = transform(df_train, True, True)\n",
    "        \n",
    "        df_test = df_synthetic.loc[range(n, len(df_synthetic)),:]\n",
    "        df_test_transf, maj_list, min_list = transform(df_test, True, False)\n",
    "        \n",
    "        df_test_maj = df_test_transf.loc[maj_list]\n",
    "        df_test_min = df_test_transf.loc[min_list]\n",
    "\n",
    "        # format training data\n",
    "        X_true = df_train_transf.iloc[:, :-1].values\n",
    "        y_true = df_train_transf.iloc[:, -1].values\n",
    "        \n",
    "        # format test data\n",
    "        X_test = df_test_transf.iloc[:, :-1].values\n",
    "        X_test_maj = df_test_maj.iloc[:, :-1].values\n",
    "        X_test_min = df_test_min.iloc[:, :-1].values\n",
    "        y_test = df_test_transf.iloc[:, -1].values\n",
    "        y_test_maj = df_test_maj.iloc[:, -1].values\n",
    "        y_test_min = df_test_min.iloc[:, -1].values\n",
    "        \n",
    "        sens_attr_test = df_test['cat']\n",
    "        \n",
    "        for beta in bias_amts:\n",
    "            \n",
    "            if i == 0: print(\"Beta: \", beta, '\\n')\n",
    "\n",
    "            df_train_copy = df_train.copy()\n",
    "\n",
    "            df_majority = df_train_copy[df_train_copy['cat'] == 1]\n",
    "            df_minority = df_train_copy[df_train_copy['cat'] == 0]\n",
    "\n",
    "            # unfavored group with negative label\n",
    "            df_minority_negative = df_minority[df_minority['outcome'] == 0.0]\n",
    "\n",
    "            # unfavored group with positive label (preferred)\n",
    "            df_minority_positive = df_minority[df_minority['outcome'] == 1.0]\n",
    "\n",
    "            # data frame without positively labeled examples from minority class\n",
    "            df_total = pd.concat([df_majority, df_minority_negative])\n",
    "\n",
    "            # under-sampling process\n",
    "            df_undersampled = under(df_minority_positive, beta)\n",
    "\n",
    "            # combine undersampled and original favored class to create dataset\n",
    "            df_concat = pd.concat([df_total,df_undersampled]).sample(frac=1, random_state = 42)\n",
    "\n",
    "            # format data\n",
    "            X_bias_true, y_bias_true, df_sens = transform(df_concat)\n",
    "            \n",
    "            # model trained on biased data\n",
    "            classifier = LogisticRegression(solver = 'lbfgs', random_state = 42, penalty = 'none')\n",
    "            classifier_bias = classifier.fit(X_bias_true, y_bias_true)\n",
    "            \n",
    "            acc = accuracy_score(y_test,classifier_bias.predict(X_test))\n",
    "            #print(f'Biased classifier:')\n",
    "            #print(f'     Test accuracy = {acc}')\n",
    "            \n",
    "            m = classifier_bias\n",
    "            def classify(X): return m.predict(X)\n",
    "            error = ErrorRate()\n",
    "            error.load_data(X_bias_true, pd.Series(y_bias_true), sensitive_features=df_sens)\n",
    "            disparity = EqualizedOdds()\n",
    "            disparity.load_data(X_bias_true, pd.Series(y_bias_true), sensitive_features=df_sens)\n",
    "            #print(f'     Train error = {error.gamma(classify)[0]}')\n",
    "            #print(f'     Train disparity = {disparity.gamma(classify).max()}')\n",
    "            disp_bias_train += [disparity.gamma(classify).max()]\n",
    "\n",
    "            m = classifier_bias\n",
    "            def classify(X): return m.predict(X)\n",
    "            error = ErrorRate()\n",
    "            error.load_data(X_test, pd.Series(y_test), sensitive_features=sens_attr_test)\n",
    "            disparity = EqualizedOdds()\n",
    "            disparity.load_data(X_test, pd.Series(y_test), sensitive_features=sens_attr_test)\n",
    "            #print(f'     Test error = {error.gamma(classify)[0]}')\n",
    "            #print(f'     Test disparity = {disparity.gamma(classify).max()}')\n",
    "            disp_bias_test += [disparity.gamma(classify).max()]\n",
    "            \n",
    "            # Learned bayes optimal classifier\n",
    "            classifier = LogisticRegression(solver = 'lbfgs', random_state = 42, penalty = 'none')\n",
    "            classifier_b = classifier.fit(X_true, y_true)                \n",
    "            #classifier_b = clone(classifier).fit(X_true, y_true)\n",
    "            acc = accuracy_score(y_test,classifier_b.predict(X_test))\n",
    "            #print(f'Learned BO classifier:')\n",
    "            #print(f'     Test accuracy = {acc}')\n",
    "            \n",
    "            m = classifier_b\n",
    "            def classify(X): return m.predict(X)\n",
    "            error = ErrorRate()\n",
    "            error.load_data(X_bias_true, pd.Series(y_bias_true), sensitive_features=df_sens)\n",
    "            disparity = EqualizedOdds()\n",
    "            disparity.load_data(X_bias_true, pd.Series(y_bias_true), sensitive_features=df_sens)\n",
    "            #print(f'     Train error = {error.gamma(classify)[0]}')\n",
    "            #print(f'     Train disparity = {disparity.gamma(classify).max()}')\n",
    "            disp_bo_train += [disparity.gamma(classify).max()]\n",
    "\n",
    "            m = classifier_b\n",
    "            def classify(X): return m.predict(X)\n",
    "            error = ErrorRate()\n",
    "            error.load_data(X_test, pd.Series(y_test), sensitive_features=sens_attr_test)\n",
    "            disparity = EqualizedOdds()\n",
    "            disparity.load_data(X_test, pd.Series(y_test), sensitive_features=sens_attr_test)\n",
    "            #print(f'     Test error = {error.gamma(classify)[0]}')\n",
    "            #print(f'     Test disparity = {disparity.gamma(classify).max()}')\n",
    "            disp_bo_test += [disparity.gamma(classify).max()]\n",
    "            \n",
    "            \n",
    "            # Exact BO optimal classifier\n",
    "            #print(f'Exact BO classifier:')\n",
    "            acc = accuracy_score(y_test,exact_bo_labels_test)\n",
    "            #print(f'     Test accuracy = {acc}')\n",
    "            acc = accuracy_score(y_true,exact_bo_labels_train)\n",
    "            #print(f'     Train accuracy = {acc}')\n",
    "\n",
    "            if apply_fairness:\n",
    "                constraint = EqualizedOdds()\n",
    "                classifier = LogisticRegression(solver = 'lbfgs', random_state = 42, penalty = 'none', fit_intercept = False, max_iter = 200)   \n",
    "\n",
    "                classifier_mitigated_bias = GridSearch(estimator=classifier,\n",
    "                                                       constraints=constraint,\n",
    "                                                       selection_rule='tradeoff_optimization',\n",
    "                                                       constraint_weight=0.5,\n",
    "                                                       grid_size=10,\n",
    "                                                       grid_limit=2.0,\n",
    "                                                       grid_offset=None,\n",
    "                                                       grid=None,\n",
    "                                                       sample_weight_name='sample_weight')\n",
    "                                                       \n",
    "                classifier_mitigated_bias.fit(X_bias_true, y_bias_true, sensitive_features = df_sens)\n",
    "                \n",
    "                acc = accuracy_score(y_test,classifier_mitigated_bias.predict(X_test))\n",
    "                #print(f'Mitigated bias classifier:')\n",
    "                #print(f'     Test accuracy = {acc}')\n",
    "                \n",
    "                m = classifier_mitigated_bias\n",
    "                def classify(X): return m.predict(X)\n",
    "                error = ErrorRate()\n",
    "                error.load_data(X_bias_true, pd.Series(y_bias_true), sensitive_features=df_sens)\n",
    "                disparity = EqualizedOdds()\n",
    "                disparity.load_data(X_bias_true, pd.Series(y_bias_true), sensitive_features=df_sens)\n",
    "                #print(f'     Train error = {error.gamma(classify)[0]}')\n",
    "                #print(f'     Train disparity = {disparity.gamma(classify).max()}')\n",
    "                disp_mitigated_train += [disparity.gamma(classify).max()]\n",
    "                \n",
    "                m = classifier_mitigated_bias\n",
    "                def classify(X): return m.predict(X)\n",
    "                error = ErrorRate()\n",
    "                error.load_data(X_test, pd.Series(y_test), sensitive_features=sens_attr_test)\n",
    "                disparity = EqualizedOdds()\n",
    "                disparity.load_data(X_test, pd.Series(y_test), sensitive_features=sens_attr_test)\n",
    "                #print(f'     Test error = {error.gamma(classify)[0]}')\n",
    "                #print(f'     Test disparity = {disparity.gamma(classify).max()}')\n",
    "                disp_mitigated_test += [disparity.gamma(classify).max()]\n",
    "                \n",
    "                # Alternative fidelity of intervention model to no intervention model\n",
    "                alt_fid_train = accuracy_score(classifier_mitigated_bias.predict(X_bias_true),classifier_bias.predict(X_bias_true))\n",
    "                alt_fid_test = accuracy_score(classifier_mitigated_bias.predict(X_test),classifier_bias.predict(X_test))\n",
    "                #print(f'Alternative fidelity of intervention model to no intervention model: train = {alt_fid_train}, test = {alt_fid_test}')\n",
    "                \n",
    "\n",
    "            else:\n",
    "                classifier_mitigated_bias = clone(classifier_bias)\n",
    "                classifier_mitigated_bias.fit(X_bias_true, y_bias_true)\n",
    "                \n",
    "                disp_mitigated_train += [0]\n",
    "                disp_mitigated_test += [0]\n",
    "                \n",
    "                # NOTE: disparities are the same as for classifier_bias\n",
    "\n",
    "                \n",
    "            # Fidelity in this step\n",
    "            fid = accuracy_score(classifier_mitigated_bias.predict(X_test),classifier_b.predict(X_test))\n",
    "            #print(f'Test set fidelity of learned BO classifier and mitigated_bias_classifier: {fid}')\n",
    "            fid = accuracy_score(classifier_mitigated_bias.predict(X_test),exact_bo_labels_test)\n",
    "            #print(f'Test set fidelity of exact BO classifier and mitigated biased classifier: {fid}')\n",
    "            fid = accuracy_score(classifier_bias.predict(X_test),classifier_b.predict(X_test))\n",
    "            #print(f'Test set fidelity of learned BO classifier and biased classifier: {fid}')\n",
    "            \n",
    "            # fidelity check\n",
    "            test_maj += [accuracy_score(classifier_mitigated_bias.predict(X_test_maj), classifier_b.predict(X_test_maj))]\n",
    "            test_min += [accuracy_score(classifier_mitigated_bias.predict(X_test_min), classifier_b.predict(X_test_min))]\n",
    "            total += [accuracy_score(classifier_mitigated_bias.predict(X_test), classifier_b.predict(X_test))]\n",
    "                \n",
    "            if verbose:\n",
    "                print(\"Finished Iteration: \", count)\n",
    "                count +=1\n",
    "        \n",
    "        #print(f'Fidelity test maj: {test_maj}')\n",
    "        #print(f'Fidelity test min: {test_maj}')\n",
    "        \n",
    "        total_fidel_maj.append(test_maj)\n",
    "        total_fidel_min.append(test_min)\n",
    "        total_fidel.append(total)\n",
    "        \n",
    "        total_disp_bias_train.append(disp_bias_train)\n",
    "        total_disp_bo_train.append(disp_bo_train)\n",
    "        total_disp_mitigated_train.append(disp_mitigated_train)\n",
    "\n",
    "        total_disp_bias_test.append(disp_bias_test)\n",
    "        total_disp_bo_test.append(disp_bo_test)\n",
    "        total_disp_mitigated_test.append(disp_mitigated_test)\n",
    "        \n",
    "        if verbose:\n",
    "                print(\"Finished Total Iteration: \", i+1)\n",
    "    \n",
    "    mean_fidel_maj = np.mean(total_fidel_maj, axis = 0)\n",
    "    mean_fidel_min = np.mean(total_fidel_min, axis = 0)\n",
    "    mean_fidel = np.mean(total_fidel, axis = 0)\n",
    "    \n",
    "    mean_disp_bias_train = np.mean(total_disp_bias_train, axis = 0)\n",
    "    mean_disp_bo_train = np.mean(total_disp_bo_train, axis = 0)\n",
    "    mean_disp_mitigated_train = np.mean(total_disp_mitigated_train, axis = 0)\n",
    "    \n",
    "    mean_disp_bias_test = np.mean(total_disp_bias_test, axis = 0)\n",
    "    mean_disp_bo_test = np.mean(total_disp_bo_test, axis = 0)\n",
    "    mean_disp_mitigated_test = np.mean(total_disp_mitigated_test, axis = 0)\n",
    "    \n",
    "    y_err_fidel_maj = np.std(total_fidel_maj, axis = 0)\n",
    "    y_err_fidel_min = np.std(total_fidel_min, axis = 0)\n",
    "    y_err_fidel = np.std(total_fidel, axis = 0)\n",
    "    \n",
    "    df = pd.DataFrame({\"Biased Train\" : mean_disp_bias_train,\n",
    "                       \"BO Train\" : mean_disp_bo_train,\n",
    "                       \"Mitigated Train\" : mean_disp_mitigated_train,\n",
    "                       \"Biased Test\" : mean_disp_bias_test,\n",
    "                       \"BO Test\" : mean_disp_bo_test,\n",
    "                       \"Mitigated Test\" : mean_disp_mitigated_test})\n",
    "   \n",
    "    return bias_amts, mean_fidel_maj, mean_fidel_min, mean_fidel, y_err_fidel_maj, y_err_fidel_min, y_err_fidel, df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a5baa9-4d72-4f82-b02f-4104567d698f",
   "metadata": {},
   "source": [
    "### Section 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccbcfad-6294-4d9b-bc3c-53ae73eeea1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run experiments for section 4 and save data\n",
    "import warnings\n",
    "import time\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "for n in [300,3000,30000]:\n",
    "\n",
    "    random.seed(42)\n",
    "    np.random.seed(42)\n",
    "\n",
    "    eta = 0.4\n",
    "    r = 0.2\n",
    "    num_iters = 50\n",
    "    \n",
    "    print(f'n={n}')\n",
    "    \n",
    "    print(f'Run simulation with intervention...')\n",
    "    start_time = time.time()\n",
    "    apply_fairness = True\n",
    "    bias_amts, mean_fidel_maj, mean_fidel_min, mean_fidel, y_err_fidel_maj, y_err_fidel_min, y_err_fidel, df = \\\n",
    "    tradeoff_visualization_error(r=r, n=n, apply_fairness=apply_fairness,verbose=False, num_iters=num_iters)\n",
    "    df_res = pd.DataFrame(np.array([bias_amts, mean_fidel_maj, mean_fidel_min, mean_fidel, y_err_fidel_maj, y_err_fidel_min, y_err_fidel]).T, columns=['bias_amts', 'mean_fidel_maj', 'mean_fidel_min', 'mean_fidel', 'y_err_fidel_maj', 'y_err_fidel_min', 'y_err_fidel'])\n",
    "    df.to_csv(f'disp_with_intervention_numiters{num_iters}_n{n}_eta{eta}_fairness{apply_fairness}.csv', index = True)\n",
    "    df_res.to_csv(f'fid_with_intervention_numiters{num_iters}_n{n}_eta{eta}_fairness{apply_fairness}.csv', index = True)\n",
    "    print(f'Took {(time.time() - start_time)/60} minutes.')\n",
    "    \n",
    "    print(f'Run simulation without intervention...')\n",
    "    start_time = time.time()\n",
    "    apply_fairness = False\n",
    "    bias_amts, mean_fidel_maj, mean_fidel_min, mean_fidel, y_err_fidel_maj, y_err_fidel_min, y_err_fidel, df = \\\n",
    "    tradeoff_visualization_error(r = r, n = n, apply_fairness=apply_fairness,verbose=False, num_iters=num_iters)\n",
    "    df_res = pd.DataFrame(np.array([bias_amts, mean_fidel_maj, mean_fidel_min, mean_fidel, y_err_fidel_maj, y_err_fidel_min, y_err_fidel]).T, columns=['bias_amts', 'mean_fidel_maj', 'mean_fidel_min', 'mean_fidel', 'y_err_fidel_maj', 'y_err_fidel_min', 'y_err_fidel'])\n",
    "    df.to_csv(f'disp_with_intervention_numiters{num_iters}_n{n}_eta{eta}_fairness{apply_fairness}.csv', index = True)\n",
    "    df_res.to_csv(f'fid_with_intervention_numiters{num_iters}_n{n}_eta{eta}_fairness{apply_fairness}.csv', index = True)\n",
    "    print(f'Took {(time.time() - start_time)/60} minutes.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708d011d-2931-43aa-acee-0af5a8fc555c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make plots for section 4\n",
    "\n",
    "for n in [300,3000,30000]:\n",
    "\n",
    "    eta = 0.4\n",
    "    num_iters = 50\n",
    "\n",
    "    df_fair = pd.read_csv(f'fid_with_intervention_numiters{num_iters}_n{n}_eta{eta}_fairnessTrue.csv')\n",
    "    df_bias = pd.read_csv(f'fid_with_intervention_numiters{num_iters}_n{n}_eta{eta}_fairnessFalse.csv')\n",
    "\n",
    "    a = 0.5\n",
    "    fig, ax = plt.subplots(figsize=(5,5))\n",
    "    markers, caps, bars = ax.errorbar(1-df_fair.bias_amts, df_fair.mean_fidel_maj, yerr = df_fair.y_err_fidel_maj, \n",
    "                                      label = 'With intervention majority group', color = \"orange\",\n",
    "                                      capsize=4, capthick=2)\n",
    "    [bar.set_alpha(a) for bar in bars]\n",
    "    [cap.set_alpha(a) for cap in caps]\n",
    "    markers, caps, bars = ax.errorbar(1-df_fair.bias_amts, df_fair.mean_fidel_min, yerr = df_fair.y_err_fidel_min, \n",
    "                                      label = 'With intervention minority group', color = \"orange\",\n",
    "                                      capsize=4, capthick=2, linestyle='--')\n",
    "    [bar.set_linestyle('--') for bar in bars]\n",
    "    [bar.set_alpha(a) for bar in bars]\n",
    "    [cap.set_alpha(a) for cap in caps]\n",
    "    markers, caps, bars = ax.errorbar(1-df_bias.bias_amts, df_bias.mean_fidel_maj, yerr = df_bias.y_err_fidel_maj, \n",
    "                                      label = 'Without intervention majority group', color = \"blue\",\n",
    "                                      capsize=4, capthick=2)\n",
    "    [bar.set_alpha(a) for bar in bars]\n",
    "    [cap.set_alpha(a) for cap in caps]\n",
    "    markers, caps, bars = ax.errorbar(1-df_bias.bias_amts, df_bias.mean_fidel_min, yerr = df_bias.y_err_fidel_min, \n",
    "                                      label = 'Without intervention minority group', color = \"blue\",\n",
    "                                      capsize=4, capthick=2, linestyle='--')\n",
    "    [bar.set_linestyle('--') for bar in bars]\n",
    "    [bar.set_alpha(a) for bar in bars]\n",
    "    [cap.set_alpha(a) for cap in caps]\n",
    "    \n",
    "    #plt.errorbar(df_fair.bias_amts, df_fair.mean_fidel_maj, yerr = df_fair.y_err_fidel_maj, label = 'With intervention majority group', color = \"orange\")\n",
    "    #plt.errorbar(df_fair.bias_amts, df_fair.mean_fidel_min, yerr = df_fair.y_err_fidel_min, label = 'With intervention minority group', color = \"orange\", linestyle = '--')\n",
    "    #plt.errorbar(df_bias.bias_amts, df_bias.mean_fidel_maj, yerr = df_bias.y_err_fidel_maj, label = 'Without intervention majority group', color = \"blue\")\n",
    "    #plt.errorbar(df_bias.bias_amts, df_bias.mean_fidel_min, yerr = df_bias.y_err_fidel_min, label = 'Without intervention minority group', color = \"blue\", linestyle = '--')\n",
    "    #plt.errorbar(bias_amts, mean_fidel, yerr = y_err_fidel, label = 'Total Fidelity', color = \"blue\")\n",
    "    \n",
    "    plt.xlabel(r\"Amount of injected underrep. bias $1-\\beta$\")\n",
    "    plt.ylabel(f\"Fidelity with n={n}\")\n",
    "    #plt.xlim(1.05, -0.05)\n",
    "    plt.ylim(0.35, 1.05)\n",
    "    if n == 30000:\n",
    "        plt.legend()\n",
    "    plt.savefig(f'section4_n{n}.jpg',dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfd0797-183d-4df3-8909-2f413c7767e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in [300,3000,30000]:\n",
    "\n",
    "    eta = 0.4\n",
    "    num_iters = 50\n",
    "\n",
    "    df_fair = pd.read_csv(f'fid_with_intervention_numiters{num_iters}_n{n}_eta{eta}_fairnessTrue.csv')\n",
    "    df_bias = pd.read_csv(f'fid_with_intervention_numiters{num_iters}_n{n}_eta{eta}_fairnessFalse.csv')\n",
    "    \n",
    "    print(f'\\nn={n}')\n",
    "    print(f'Overall fid = {df_fair.mean_fidel.describe()}')\n",
    "    print(f'Min fid = {df_fair.mean_fidel_min.describe()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41379f6f-9cc4-4ab8-bf1c-5fd3dd0164ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "eta = 0.2\n",
    "n = 30000\n",
    "r = 0.2\n",
    "num_iters = 10\n",
    "\n",
    "bias_amts, mean_fidel_maj, mean_fidel_min, mean_fidel, y_err_fidel_maj, y_err_fidel_min, y_err_fidel, df = \\\n",
    "tradeoff_visualization_error(classifier,r = r, n = n, apply_fairness=True,verbose=True, num_iters=num_iters)\n",
    "\n",
    "df.to_csv(f'with_intervention_numiters{num_iters}_n{n}_eta{eta}.csv', index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948164ae-809d-4631-8add-f7266195301c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "plt.errorbar(bias_amts, mean_fidel_maj, yerr = y_err_fidel_maj, label = 'Fidelity Majority', color = \"red\")\n",
    "plt.errorbar(bias_amts, mean_fidel_min, yerr = y_err_fidel_min, label = 'Fidelity Minority', color = \"green\")\n",
    "#plt.errorbar(bias_amts, mean_fidel, yerr = y_err_fidel, label = 'Total Fidelity', color = \"blue\")\n",
    "plt.xlabel(\"Beta Value\")\n",
    "plt.ylabel(\"Fidelity\")\n",
    "plt.xlim(1.05, -0.05)\n",
    "plt.ylim(0.92, 1)\n",
    "plt.legend()\n",
    "plt.title(\"Fidelity Plot\")\n",
    "plt.savefig(f'with_intervention_numiters{num_iters}_n{n}_eta{eta}.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1c7bc0-82a2-4e23-a36d-4fe8208c2b02",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "eta = 0.4\n",
    "n = 30000\n",
    "r = 0.2\n",
    "num_iters = 10\n",
    "\n",
    "bias_amts, mean_fidel_maj2, mean_fidel_min2, mean_fidel2, y_err_fidel_maj2, y_err_fidel_min2, y_err_fidel2, df = \\\n",
    "tradeoff_visualization_error(classifier,r = r, n = n, apply_fairness=False,verbose=True, num_iters=num_iters)\n",
    "\n",
    "df.to_csv(f'without_intervention_numiters{num_iters}_n{n}_eta{eta}.csv', index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d0b897-cd83-4f5d-84b2-a1113b8702a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "plt.errorbar(bias_amts, mean_fidel_maj2, yerr = y_err_fidel_maj2, label = 'Fidelity Majority', color = \"red\")\n",
    "plt.errorbar(bias_amts, mean_fidel_min2, yerr = y_err_fidel_min2, label = 'Fidelity Minority', color = \"green\")\n",
    "plt.errorbar(bias_amts, mean_fidel2, yerr = y_err_fidel2, label = 'Total Fidelity', color = \"blue\")\n",
    "plt.xlabel(\"Beta Value\")\n",
    "plt.ylabel(\"Fidelity\")\n",
    "plt.xlim(1.05, -0.05)\n",
    "plt.legend()\n",
    "plt.title(\"Fidelity Plot\")\n",
    "plt.savefig(f'without_intervention_numiters{num_iters}_n{n}_eta{eta}.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bfad8c-86a7-45fc-86ff-c319095bbbc6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "eta = 0.4\n",
    "n = 100000\n",
    "r = 0.2\n",
    "num_iters = 10\n",
    "\n",
    "bias_amts, mean_fidel_maj2, mean_fidel_min2, mean_fidel2, y_err_fidel_maj2, y_err_fidel_min2, y_err_fidel2, df = \\\n",
    "tradeoff_visualization_error(classifier,r = r, n = n, apply_fairness=True,verbose=True, num_iters=num_iters)\n",
    "\n",
    "df.to_csv(f'with_intervention_numiters{num_iters}_n{n}_eta{eta}.csv', index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e63d175-1794-445b-8186-55fb6db9ba21",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "plt.errorbar(bias_amts, mean_fidel_maj2, yerr = y_err_fidel_maj2, label = 'Fidelity Majority', color = \"red\")\n",
    "plt.errorbar(bias_amts, mean_fidel_min2, yerr = y_err_fidel_min2, label = 'Fidelity Minority', color = \"green\")\n",
    "plt.errorbar(bias_amts, mean_fidel2, yerr = y_err_fidel2, label = 'Total Fidelity', color = \"blue\")\n",
    "plt.xlabel(\"Beta Value\")\n",
    "plt.ylabel(\"Fidelity\")\n",
    "plt.xlim(1.05, -0.05)\n",
    "plt.legend()\n",
    "plt.title(\"Fidelity Plot\")\n",
    "plt.savefig(f'with_intervention_numiters{num_iters}_n{n}_eta{eta}.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340dbb77-89b3-4f1b-bcee-1c363b174cc3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "eta = 0.4\n",
    "n = 100000\n",
    "r = 0.2\n",
    "num_iters = 10\n",
    "\n",
    "bias_amts, mean_fidel_maj2, mean_fidel_min2, mean_fidel2, y_err_fidel_maj2, y_err_fidel_min2, y_err_fidel2, df = \\\n",
    "tradeoff_visualization_error(classifier,r = r, n = n, apply_fairness=False,verbose=True, num_iters=num_iters)\n",
    "\n",
    "df.to_csv(f'with_intervention_numiters{num_iters}_n{n}_eta{eta}.csv', index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954995cb-3f63-4b90-802b-fc4a7c2e4cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "plt.errorbar(bias_amts, mean_fidel_maj2, yerr = y_err_fidel_maj2, label = 'Fidelity Majority', color = \"red\")\n",
    "plt.errorbar(bias_amts, mean_fidel_min2, yerr = y_err_fidel_min2, label = 'Fidelity Minority', color = \"green\")\n",
    "plt.errorbar(bias_amts, mean_fidel2, yerr = y_err_fidel2, label = 'Total Fidelity', color = \"blue\")\n",
    "plt.xlabel(\"Beta Value\")\n",
    "plt.ylabel(\"Fidelity\")\n",
    "plt.xlim(1.05, -0.05)\n",
    "plt.legend()\n",
    "plt.title(\"Fidelity Plot\")\n",
    "plt.savefig(f'without_intervention_numiters{num_iters}_n{n}_eta{eta}.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a719050b-374d-457a-91bb-ef2df12f8774",
   "metadata": {},
   "source": [
    "# Exploration (Section 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4a842d-544e-4b41-992a-022c43cf3d2d",
   "metadata": {},
   "source": [
    "## Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "25440d5e-3a5b-431f-b4d0-078ff50ec04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def representation(r, n, apply_fairness = True, verbose = False, num_iters = 10, inter = False, diff_base = False):\n",
    "    \n",
    "    total_fidel_maj = []\n",
    "    total_fidel_min = []\n",
    "    total_fidel = []\n",
    "    \n",
    "    total_disp_bias_train = []\n",
    "    total_disp_bo_train = []\n",
    "    total_disp_mitigated_train = []\n",
    "    \n",
    "    total_disp_bias_test = []\n",
    "    total_disp_bo_test = []\n",
    "    total_disp_mitigated_test = []\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "    \n",
    "        # 1 to 0 in increments of 0.1\n",
    "        bias_amts = np.divide(list(range(10,-1,-1)),10)\n",
    "        bias_amts[-1] = 0.01\n",
    "\n",
    "        test_maj = []\n",
    "        test_min = []\n",
    "        total = []\n",
    "        \n",
    "        disp_bias_train = []\n",
    "        disp_bo_train = []\n",
    "        disp_mitigated_train = []\n",
    "        \n",
    "        disp_bias_test = []\n",
    "        disp_bo_test = []\n",
    "        disp_mitigated_test = []\n",
    "        \n",
    "        count = 0\n",
    "        \n",
    "        if diff_base: \n",
    "            outcome_continuous, df_synthetic = true_label_generation(r=r, eta=eta, n=2*n, maj_means =[0.7,0.7,0.7])\n",
    "        else: outcome_continuous, df_synthetic = true_label_generation(r=r, eta=eta, n=2*n)        \n",
    "        \n",
    "        threshold = 0.5\n",
    "        exact_bo_labels = np.where(outcome_continuous < threshold, 0, 1)\n",
    "        exact_bo_labels_train = np.array(exact_bo_labels[range(0,n)])\n",
    "        exact_bo_labels_test = np.array(exact_bo_labels[range(n,len(df_synthetic))])\n",
    "        \n",
    "        # split into train and test\n",
    "        df_train = df_synthetic.loc[range(0,n), :]\n",
    "        \n",
    "        df_train_transf, maj_list, min_list = transform(df_train, True, True)\n",
    "        \n",
    "        df_test = df_synthetic.loc[range(n, len(df_synthetic)),:]\n",
    "        df_test_transf, maj_list, min_list = transform(df_test, True, False)\n",
    "        \n",
    "        df_test_maj = df_test_transf.loc[maj_list]\n",
    "        df_test_min = df_test_transf.loc[min_list]\n",
    "\n",
    "        # format training data\n",
    "        X_true = df_train_transf.iloc[:, :-1].values\n",
    "        y_true = df_train_transf.iloc[:, -1].values\n",
    "        \n",
    "        # format test data\n",
    "        X_test = df_test_transf.iloc[:, :-1].values\n",
    "        X_test_maj = df_test_maj.iloc[:, :-1].values\n",
    "        X_test_min = df_test_min.iloc[:, :-1].values\n",
    "        y_test = df_test_transf.iloc[:, -1].values\n",
    "        y_test_maj = df_test_maj.iloc[:, -1].values\n",
    "        y_test_min = df_test_min.iloc[:, -1].values\n",
    "        \n",
    "        sens_attr_test = df_test['cat']\n",
    "        \n",
    "        for beta in bias_amts:\n",
    "            \n",
    "            if i == 0: print(\"Beta: \", beta, '\\n')\n",
    "\n",
    "            df_train_copy = df_train.copy()\n",
    "\n",
    "            df_majority = df_train_copy[df_train_copy['cat'] == 1]\n",
    "            df_minority = df_train_copy[df_train_copy['cat'] == 0]\n",
    "\n",
    "            if inter:\n",
    "                # unfavored group with negative label\n",
    "                df_minority_negative = df_minority[df_minority['outcome'] == 0.0]\n",
    "\n",
    "                # unfavored group with positive label (preferred)\n",
    "                df_minority_positive = df_minority[df_minority['outcome'] == 1.0]\n",
    "\n",
    "                # data frame without positively labeled examples from minority class\n",
    "                df_total = pd.concat([df_majority, df_minority_negative])\n",
    "\n",
    "                df_undersampled = under(df_minority_positive, beta)\n",
    "\n",
    "                # combine undersampled and original favored class to create dataset\n",
    "                df_concat = pd.concat([df_total,df_undersampled]).sample(frac=1, random_state = 42) # permute data\n",
    "\n",
    "            else:\n",
    "                df_undersampled = under(df_minority, beta)\n",
    "\n",
    "                # combine undersampled and original favored class to create dataset\n",
    "                df_concat = pd.concat([df_majority,df_undersampled])\n",
    "                df_concat.sample(frac=1, random_state = 42) # permute data\n",
    "\n",
    "            # format data\n",
    "            X_bias_true, y_bias_true, df_sens = transform(df_concat)\n",
    "            \n",
    "            # model trained on biased data\n",
    "            classifier = LogisticRegression(solver = 'lbfgs', random_state = 42, penalty = 'none')\n",
    "            classifier_bias = classifier.fit(X_bias_true, y_bias_true)\n",
    "            \n",
    "            acc = accuracy_score(y_test,classifier_bias.predict(X_test))\n",
    "            #print(f'Biased classifier:')\n",
    "            #print(f'     Test accuracy = {acc}')\n",
    "            \n",
    "            m = classifier_bias\n",
    "            def classify(X): return m.predict(X)\n",
    "            error = ErrorRate()\n",
    "            error.load_data(X_bias_true, pd.Series(y_bias_true), sensitive_features=df_sens)\n",
    "            disparity = EqualizedOdds()\n",
    "            disparity.load_data(X_bias_true, pd.Series(y_bias_true), sensitive_features=df_sens)\n",
    "            #print(f'     Train error = {error.gamma(classify)[0]}')\n",
    "            #print(f'     Train disparity = {disparity.gamma(classify).max()}')\n",
    "            disp_bias_train += [disparity.gamma(classify).max()]\n",
    "\n",
    "            m = classifier_bias\n",
    "            def classify(X): return m.predict(X)\n",
    "            error = ErrorRate()\n",
    "            error.load_data(X_test, pd.Series(y_test), sensitive_features=sens_attr_test)\n",
    "            disparity = EqualizedOdds()\n",
    "            disparity.load_data(X_test, pd.Series(y_test), sensitive_features=sens_attr_test)\n",
    "            #print(f'     Test error = {error.gamma(classify)[0]}')\n",
    "            #print(f'     Test disparity = {disparity.gamma(classify).max()}')\n",
    "            disp_bias_test += [disparity.gamma(classify).max()]\n",
    "            \n",
    "            # Learned bayes optimal classifier\n",
    "            classifier = LogisticRegression(solver = 'lbfgs', random_state = 42, penalty = 'none')\n",
    "            classifier_b = classifier.fit(X_true, y_true)                \n",
    "            #classifier_b = clone(classifier).fit(X_true, y_true)\n",
    "            acc = accuracy_score(y_test,classifier_b.predict(X_test))\n",
    "            #print(f'Learned BO classifier:')\n",
    "            #print(f'     Test accuracy = {acc}')\n",
    "            \n",
    "            m = classifier_b\n",
    "            def classify(X): return m.predict(X)\n",
    "            error = ErrorRate()\n",
    "            error.load_data(X_bias_true, pd.Series(y_bias_true), sensitive_features=df_sens)\n",
    "            disparity = EqualizedOdds()\n",
    "            disparity.load_data(X_bias_true, pd.Series(y_bias_true), sensitive_features=df_sens)\n",
    "            #print(f'     Train error = {error.gamma(classify)[0]}')\n",
    "            #print(f'     Train disparity = {disparity.gamma(classify).max()}')\n",
    "            disp_bo_train += [disparity.gamma(classify).max()]\n",
    "\n",
    "            m = classifier_b\n",
    "            def classify(X): return m.predict(X)\n",
    "            error = ErrorRate()\n",
    "            error.load_data(X_test, pd.Series(y_test), sensitive_features=sens_attr_test)\n",
    "            disparity = EqualizedOdds()\n",
    "            disparity.load_data(X_test, pd.Series(y_test), sensitive_features=sens_attr_test)\n",
    "            #print(f'     Test error = {error.gamma(classify)[0]}')\n",
    "            #print(f'     Test disparity = {disparity.gamma(classify).max()}')\n",
    "            disp_bo_test += [disparity.gamma(classify).max()]\n",
    "            \n",
    "            \n",
    "            # Exact BO optimal classifier\n",
    "            #print(f'Exact BO classifier:')\n",
    "            acc = accuracy_score(y_test,exact_bo_labels_test)\n",
    "            #print(f'     Test accuracy = {acc}')\n",
    "            acc = accuracy_score(y_true,exact_bo_labels_train)\n",
    "            #print(f'     Train accuracy = {acc}')\n",
    "\n",
    "            if apply_fairness:\n",
    "                constraint = EqualizedOdds()\n",
    "                classifier = LogisticRegression(solver = 'lbfgs', random_state = 42, penalty = 'none', fit_intercept = False, max_iter = 200)   \n",
    "\n",
    "                classifier_mitigated_bias = GridSearch(estimator=classifier,\n",
    "                                                       constraints=constraint,\n",
    "                                                       selection_rule='tradeoff_optimization',\n",
    "                                                       constraint_weight=0.5,\n",
    "                                                       grid_size=10,\n",
    "                                                       grid_limit=2.0,\n",
    "                                                       grid_offset=None,\n",
    "                                                       grid=None,\n",
    "                                                       sample_weight_name='sample_weight')\n",
    "                                                       \n",
    "                classifier_mitigated_bias.fit(X_bias_true, y_bias_true, sensitive_features = df_sens)\n",
    "                \n",
    "                acc = accuracy_score(y_test,classifier_mitigated_bias.predict(X_test))\n",
    "                #print(f'Mitigated bias classifier:')\n",
    "                #print(f'     Test accuracy = {acc}')\n",
    "                \n",
    "                m = classifier_mitigated_bias\n",
    "                def classify(X): return m.predict(X)\n",
    "                error = ErrorRate()\n",
    "                error.load_data(X_bias_true, pd.Series(y_bias_true), sensitive_features=df_sens)\n",
    "                disparity = EqualizedOdds()\n",
    "                disparity.load_data(X_bias_true, pd.Series(y_bias_true), sensitive_features=df_sens)\n",
    "                #print(f'     Train error = {error.gamma(classify)[0]}')\n",
    "                #print(f'     Train disparity = {disparity.gamma(classify).max()}')\n",
    "                disp_mitigated_train += [disparity.gamma(classify).max()]\n",
    "                \n",
    "                m = classifier_mitigated_bias\n",
    "                def classify(X): return m.predict(X)\n",
    "                error = ErrorRate()\n",
    "                error.load_data(X_test, pd.Series(y_test), sensitive_features=sens_attr_test)\n",
    "                disparity = EqualizedOdds()\n",
    "                disparity.load_data(X_test, pd.Series(y_test), sensitive_features=sens_attr_test)\n",
    "                #print(f'     Test error = {error.gamma(classify)[0]}')\n",
    "                #print(f'     Test disparity = {disparity.gamma(classify).max()}')\n",
    "                disp_mitigated_test += [disparity.gamma(classify).max()]\n",
    "                \n",
    "                # Alternative fidelity of intervention model to no intervention model\n",
    "                alt_fid_train = accuracy_score(classifier_mitigated_bias.predict(X_bias_true),classifier_bias.predict(X_bias_true))\n",
    "                alt_fid_test = accuracy_score(classifier_mitigated_bias.predict(X_test),classifier_bias.predict(X_test))\n",
    "                #print(f'Alternative fidelity of intervention model to no intervention model: train = {alt_fid_train}, test = {alt_fid_test}')\n",
    "                \n",
    "\n",
    "            else:\n",
    "                classifier_mitigated_bias = clone(classifier_bias)\n",
    "                classifier_mitigated_bias.fit(X_bias_true, y_bias_true)\n",
    "                \n",
    "                # NOTE: disparities are the same as for classifier_bias\n",
    "\n",
    "                \n",
    "            # Fidelity in this step\n",
    "            fid = accuracy_score(classifier_mitigated_bias.predict(X_test),classifier_b.predict(X_test))\n",
    "            #print(f'Test set fidelity of learned BO classifier and mitigated_bias_classifier: {fid}')\n",
    "            fid = accuracy_score(classifier_mitigated_bias.predict(X_test),exact_bo_labels_test)\n",
    "            #print(f'Test set fidelity of exact BO classifier and mitigated biased classifier: {fid}')\n",
    "            fid = accuracy_score(classifier_bias.predict(X_test),classifier_b.predict(X_test))\n",
    "            #print(f'Test set fidelity of learned BO classifier and biased classifier: {fid}')\n",
    "            \n",
    "            # fidelity check\n",
    "            test_maj += [accuracy_score(classifier_mitigated_bias.predict(X_test_maj), classifier_b.predict(X_test_maj))]\n",
    "            test_min += [accuracy_score(classifier_mitigated_bias.predict(X_test_min), classifier_b.predict(X_test_min))]\n",
    "            total += [accuracy_score(classifier_mitigated_bias.predict(X_test), classifier_b.predict(X_test))]\n",
    "                \n",
    "            if verbose:\n",
    "                print(\"Finished Iteration: \", count)\n",
    "                count +=1\n",
    "        \n",
    "        #print(f'Fidelity test maj: {test_maj}')\n",
    "        #print(f'Fidelity test min: {test_maj}')\n",
    "        \n",
    "        total_fidel_maj.append(test_maj)\n",
    "        total_fidel_min.append(test_min)\n",
    "        total_fidel.append(total)\n",
    "        \n",
    "        total_disp_bias_train.append(disp_bias_train)\n",
    "        total_disp_bo_train.append(disp_bo_train)\n",
    "        total_disp_mitigated_train.append(disp_mitigated_train)\n",
    "\n",
    "        total_disp_bias_test.append(disp_bias_test)\n",
    "        total_disp_bo_test.append(disp_bo_test)\n",
    "        total_disp_mitigated_test.append(disp_mitigated_test)\n",
    "        \n",
    "        if verbose:\n",
    "                print(\"Finished Total Iteration: \", i+1)\n",
    "    \n",
    "    mean_fidel_maj = np.mean(total_fidel_maj, axis = 0)\n",
    "    mean_fidel_min = np.mean(total_fidel_min, axis = 0)\n",
    "    mean_fidel = np.mean(total_fidel, axis = 0)\n",
    "    \n",
    "    mean_disp_bias_train = np.mean(total_disp_bias_train, axis = 0)\n",
    "    mean_disp_bo_train = np.mean(total_disp_bo_train, axis = 0)\n",
    "    mean_disp_mitigated_train = np.mean(total_disp_mitigated_train, axis = 0)\n",
    "    \n",
    "    mean_disp_bias_test = np.mean(total_disp_bias_test, axis = 0)\n",
    "    mean_disp_bo_test = np.mean(total_disp_bo_test, axis = 0)\n",
    "    mean_disp_mitigated_test = np.mean(total_disp_mitigated_test, axis = 0)\n",
    "    \n",
    "    y_err_fidel_maj = np.std(total_fidel_maj, axis = 0)\n",
    "    y_err_fidel_min = np.std(total_fidel_min, axis = 0)\n",
    "    y_err_fidel = np.std(total_fidel, axis = 0)\n",
    "    \n",
    "    df = pd.DataFrame({\"Biased Train\" : mean_disp_bias_train,\n",
    "                       \"BO Train\" : mean_disp_bo_train,\n",
    "                       \"Mitigated Train\" : mean_disp_mitigated_train,\n",
    "                       \"Biased Test\" : mean_disp_bias_test,\n",
    "                       \"BO Test\" : mean_disp_bo_test,\n",
    "                       \"Mitigated Test\" : mean_disp_mitigated_test})\n",
    "   \n",
    "    return bias_amts, mean_fidel_maj, mean_fidel_min, mean_fidel, y_err_fidel_maj, y_err_fidel_min, y_err_fidel, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60507e31-79c5-4ee7-a6e7-0143ed7b4f10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "eta = 0.2\n",
    "n = 30000\n",
    "r = 0.2\n",
    "num_iters = 10\n",
    "\n",
    "bias_amts, mean_fidel_maj, mean_fidel_min, mean_fidel, y_err_fidel_maj, y_err_fidel_min, y_err_fidel, df = \\\n",
    "representation(r = r, n = n, apply_fairness=True,verbose=True, num_iters=num_iters, diff_base = False, inter = False)\n",
    "\n",
    "df.to_csv('disp_repr_no_inter.csv', index = True)\n",
    "df_res = pd.DataFrame(np.array([bias_amts, mean_fidel_maj, mean_fidel_min, mean_fidel, y_err_fidel_maj, y_err_fidel_min, y_err_fidel]).T, columns=['bias_amts', 'mean_fidel_maj', 'mean_fidel_min', 'mean_fidel', 'y_err_fidel_maj', 'y_err_fidel_min', 'y_err_fidel'])\n",
    "df_res.to_csv(f'fid_repr_no_inter.csv', index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61ebf16-57e2-47ca-9c5c-b33d3e7c28b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bias_amts = np.divide(list(range(10,-1,-1)),10)\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.errorbar(bias_amts, mean_fidel_maj21, yerr = y_err_fidel_maj21, label = 'Fidelity Majority', color = \"red\")\n",
    "plt.errorbar(bias_amts, mean_fidel_min21, yerr = y_err_fidel_min21, label = 'Fidelity Minority', color = \"green\")\n",
    "plt.xlabel(\"Beta Value for Minority\")\n",
    "plt.ylabel(\"Fidelity\")\n",
    "plt.xlim(1.05, -0.05)\n",
    "plt.ylim(0.92, 1)\n",
    "plt.legend()\n",
    "plt.title(\"Fidelity Plot\")\n",
    "plt.savefig('repr_no_inter.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12b6ff14-455b-446c-b0d8-362651411d6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_repr = pd.read_csv('fid_repr_no_inter.csv')\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.errorbar(df_repr.bias_amts, df_repr.mean_fidel_maj, yerr = df_repr.y_err_fidel_maj, label = 'Fidelity Majority', color = \"red\")\n",
    "plt.errorbar(df_repr.bias_amts, df_repr.mean_fidel_min, yerr = df_repr.y_err_fidel_min, label = 'Fidelity Minority', color = \"green\")\n",
    "plt.xlabel(\"Beta Value for Minority\")\n",
    "plt.ylabel(\"Fidelity\")\n",
    "plt.xlim(1.05, -0.05)\n",
    "plt.ylim(0.92, 1)\n",
    "plt.legend()\n",
    "plt.title(\"Fidelity Plot\")\n",
    "plt.savefig('repr_no_inter.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5f9c94ae-52f6-4015-9971-46ed43d8c1df",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diff:  0.008458333333333345\n",
      "Beta:  1.0 \n",
      "\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  0\n",
      "Beta:  0.9 \n",
      "\n",
      "Total Deleted:  296 \t % Deleted:  0.10088616223585549\n",
      "Finished Iteration:  1\n",
      "Beta:  0.8 \n",
      "\n",
      "Total Deleted:  576 \t % Deleted:  0.19631901840490798\n",
      "Finished Iteration:  2\n",
      "Beta:  0.7 \n",
      "\n",
      "Total Deleted:  878 \t % Deleted:  0.2992501704158146\n",
      "Finished Iteration:  3\n",
      "Beta:  0.6 \n",
      "\n",
      "Total Deleted:  1187 \t % Deleted:  0.4045671438309475\n",
      "Finished Iteration:  4\n",
      "Beta:  0.5 \n",
      "\n",
      "Total Deleted:  1481 \t % Deleted:  0.5047716428084527\n",
      "Finished Iteration:  5\n",
      "Beta:  0.4 \n",
      "\n",
      "Total Deleted:  1786 \t % Deleted:  0.6087252897068848\n",
      "Finished Iteration:  6\n",
      "Beta:  0.3 \n",
      "\n",
      "Total Deleted:  2118 \t % Deleted:  0.721881390593047\n",
      "Finished Iteration:  7\n",
      "Beta:  0.2 \n",
      "\n",
      "Total Deleted:  2338 \t % Deleted:  0.7968643490115883\n",
      "Finished Iteration:  8\n",
      "Beta:  0.1 \n",
      "\n",
      "Total Deleted:  2634 \t % Deleted:  0.8977505112474438\n",
      "Finished Iteration:  9\n",
      "Beta:  0.01 \n",
      "\n",
      "Total Deleted:  2901 \t % Deleted:  0.9887525562372188\n",
      "Finished Iteration:  10\n",
      "Finished Total Iteration:  1\n",
      "Diff:  0.010937499999999989\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  0\n",
      "Total Deleted:  301 \t % Deleted:  0.09976798143851508\n",
      "Finished Iteration:  1\n",
      "Total Deleted:  640 \t % Deleted:  0.2121312562147829\n",
      "Finished Iteration:  2\n",
      "Total Deleted:  927 \t % Deleted:  0.3072588664235996\n",
      "Finished Iteration:  3\n",
      "Total Deleted:  1212 \t % Deleted:  0.4017235664567451\n",
      "Finished Iteration:  4\n",
      "Total Deleted:  1515 \t % Deleted:  0.5021544580709314\n",
      "Finished Iteration:  5\n",
      "Total Deleted:  1779 \t % Deleted:  0.5896586012595293\n",
      "Finished Iteration:  6\n",
      "Total Deleted:  2191 \t % Deleted:  0.7262180974477959\n",
      "Finished Iteration:  7\n",
      "Total Deleted:  2423 \t % Deleted:  0.8031156778256546\n",
      "Finished Iteration:  8\n",
      "Total Deleted:  2719 \t % Deleted:  0.9012263838249918\n",
      "Finished Iteration:  9\n",
      "Total Deleted:  2991 \t % Deleted:  0.9913821677162744\n",
      "Finished Iteration:  10\n",
      "Finished Total Iteration:  2\n",
      "Diff:  8.333333333332416e-05\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  0\n",
      "Total Deleted:  278 \t % Deleted:  0.0929144385026738\n",
      "Finished Iteration:  1\n",
      "Total Deleted:  640 \t % Deleted:  0.21390374331550802\n",
      "Finished Iteration:  2\n",
      "Total Deleted:  908 \t % Deleted:  0.303475935828877\n",
      "Finished Iteration:  3\n",
      "Total Deleted:  1140 \t % Deleted:  0.3810160427807487\n",
      "Finished Iteration:  4\n",
      "Total Deleted:  1511 \t % Deleted:  0.5050133689839572\n",
      "Finished Iteration:  5\n",
      "Total Deleted:  1809 \t % Deleted:  0.6046122994652406\n",
      "Finished Iteration:  6\n",
      "Total Deleted:  2098 \t % Deleted:  0.7012032085561497\n",
      "Finished Iteration:  7\n",
      "Total Deleted:  2405 \t % Deleted:  0.8038101604278075\n",
      "Finished Iteration:  8\n",
      "Total Deleted:  2672 \t % Deleted:  0.893048128342246\n",
      "Finished Iteration:  9\n",
      "Total Deleted:  2959 \t % Deleted:  0.9889705882352942\n",
      "Finished Iteration:  10\n",
      "Finished Total Iteration:  3\n",
      "Diff:  -0.00520833333333337\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  0\n",
      "Total Deleted:  307 \t % Deleted:  0.10082101806239738\n",
      "Finished Iteration:  1\n",
      "Total Deleted:  609 \t % Deleted:  0.2\n",
      "Finished Iteration:  2\n",
      "Total Deleted:  903 \t % Deleted:  0.296551724137931\n",
      "Finished Iteration:  3\n",
      "Total Deleted:  1227 \t % Deleted:  0.40295566502463054\n",
      "Finished Iteration:  4\n",
      "Total Deleted:  1535 \t % Deleted:  0.5041050903119869\n",
      "Finished Iteration:  5\n",
      "Total Deleted:  1822 \t % Deleted:  0.5983579638752052\n",
      "Finished Iteration:  6\n",
      "Total Deleted:  2133 \t % Deleted:  0.7004926108374384\n",
      "Finished Iteration:  7\n",
      "Total Deleted:  2412 \t % Deleted:  0.7921182266009852\n",
      "Finished Iteration:  8\n",
      "Total Deleted:  2739 \t % Deleted:  0.8995073891625616\n",
      "Finished Iteration:  9\n",
      "Total Deleted:  3025 \t % Deleted:  0.993431855500821\n",
      "Finished Iteration:  10\n",
      "Finished Total Iteration:  4\n",
      "Diff:  -0.00585416666666666\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  0\n",
      "Total Deleted:  287 \t % Deleted:  0.09534883720930233\n",
      "Finished Iteration:  1\n",
      "Total Deleted:  654 \t % Deleted:  0.21727574750830564\n",
      "Finished Iteration:  2\n",
      "Total Deleted:  909 \t % Deleted:  0.30199335548172757\n",
      "Finished Iteration:  3\n",
      "Total Deleted:  1207 \t % Deleted:  0.4009966777408638\n",
      "Finished Iteration:  4\n",
      "Total Deleted:  1504 \t % Deleted:  0.49966777408637875\n",
      "Finished Iteration:  5\n",
      "Total Deleted:  1826 \t % Deleted:  0.6066445182724253\n",
      "Finished Iteration:  6\n",
      "Total Deleted:  2132 \t % Deleted:  0.7083056478405315\n",
      "Finished Iteration:  7\n",
      "Total Deleted:  2404 \t % Deleted:  0.7986710963455149\n",
      "Finished Iteration:  8\n",
      "Total Deleted:  2723 \t % Deleted:  0.9046511627906977\n",
      "Finished Iteration:  9\n",
      "Total Deleted:  2975 \t % Deleted:  0.9883720930232558\n",
      "Finished Iteration:  10\n",
      "Finished Total Iteration:  5\n",
      "Diff:  0.0014583333333333393\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  0\n",
      "Total Deleted:  301 \t % Deleted:  0.1021377672209026\n",
      "Finished Iteration:  1\n",
      "Total Deleted:  587 \t % Deleted:  0.1991856124872752\n",
      "Finished Iteration:  2\n",
      "Total Deleted:  850 \t % Deleted:  0.28842891075670174\n",
      "Finished Iteration:  3\n",
      "Total Deleted:  1144 \t % Deleted:  0.38819138106549034\n",
      "Finished Iteration:  4\n",
      "Total Deleted:  1483 \t % Deleted:  0.503223617237869\n",
      "Finished Iteration:  5\n",
      "Total Deleted:  1729 \t % Deleted:  0.5866983372921615\n",
      "Finished Iteration:  6\n",
      "Total Deleted:  2046 \t % Deleted:  0.6942653545978962\n",
      "Finished Iteration:  7\n",
      "Total Deleted:  2354 \t % Deleted:  0.7987784187309128\n",
      "Finished Iteration:  8\n",
      "Total Deleted:  2666 \t % Deleted:  0.9046487953851374\n",
      "Finished Iteration:  9\n",
      "Total Deleted:  2916 \t % Deleted:  0.989480827960638\n",
      "Finished Iteration:  10\n",
      "Finished Total Iteration:  6\n",
      "Diff:  -0.009749999999999981\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  0\n",
      "Total Deleted:  318 \t % Deleted:  0.1033810143042913\n",
      "Finished Iteration:  1\n",
      "Total Deleted:  601 \t % Deleted:  0.19538361508452537\n",
      "Finished Iteration:  2\n",
      "Total Deleted:  891 \t % Deleted:  0.28966189856957086\n",
      "Finished Iteration:  3\n",
      "Total Deleted:  1264 \t % Deleted:  0.41092327698309494\n",
      "Finished Iteration:  4\n",
      "Total Deleted:  1516 \t % Deleted:  0.4928478543563069\n",
      "Finished Iteration:  5\n",
      "Total Deleted:  1857 \t % Deleted:  0.60370611183355\n",
      "Finished Iteration:  6\n",
      "Total Deleted:  2176 \t % Deleted:  0.7074122236671001\n",
      "Finished Iteration:  7\n",
      "Total Deleted:  2458 \t % Deleted:  0.7990897269180754\n",
      "Finished Iteration:  8\n",
      "Total Deleted:  2772 \t % Deleted:  0.9011703511053316\n",
      "Finished Iteration:  9\n",
      "Total Deleted:  3046 \t % Deleted:  0.9902470741222367\n",
      "Finished Iteration:  10\n",
      "Finished Total Iteration:  7\n",
      "Diff:  0.0009583333333333388\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  0\n",
      "Total Deleted:  311 \t % Deleted:  0.10352862849533954\n",
      "Finished Iteration:  1\n",
      "Total Deleted:  586 \t % Deleted:  0.19507323568575233\n",
      "Finished Iteration:  2\n",
      "Total Deleted:  898 \t % Deleted:  0.2989347536617843\n",
      "Finished Iteration:  3\n",
      "Total Deleted:  1217 \t % Deleted:  0.4051264980026631\n",
      "Finished Iteration:  4\n",
      "Total Deleted:  1525 \t % Deleted:  0.5076564580559254\n",
      "Finished Iteration:  5\n",
      "Total Deleted:  1821 \t % Deleted:  0.6061917443408789\n",
      "Finished Iteration:  6\n",
      "Total Deleted:  2100 \t % Deleted:  0.6990679094540613\n",
      "Finished Iteration:  7\n",
      "Total Deleted:  2394 \t % Deleted:  0.7969374167776299\n",
      "Finished Iteration:  8\n",
      "Total Deleted:  2718 \t % Deleted:  0.9047936085219707\n",
      "Finished Iteration:  9\n",
      "Total Deleted:  2967 \t % Deleted:  0.9876830892143809\n",
      "Finished Iteration:  10\n",
      "Finished Total Iteration:  8\n",
      "Diff:  -0.005312499999999942\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  0\n",
      "Total Deleted:  312 \t % Deleted:  0.1025303976339139\n",
      "Finished Iteration:  1\n",
      "Total Deleted:  603 \t % Deleted:  0.19815971081169897\n",
      "Finished Iteration:  2\n",
      "Total Deleted:  923 \t % Deleted:  0.3033190930003286\n",
      "Finished Iteration:  3\n",
      "Total Deleted:  1178 \t % Deleted:  0.3871179756818929\n",
      "Finished Iteration:  4\n",
      "Total Deleted:  1536 \t % Deleted:  0.5047650345054223\n",
      "Finished Iteration:  5\n",
      "Total Deleted:  1840 \t % Deleted:  0.6046664475846204\n",
      "Finished Iteration:  6\n",
      "Total Deleted:  2122 \t % Deleted:  0.6973381531383503\n",
      "Finished Iteration:  7\n",
      "Total Deleted:  2472 \t % Deleted:  0.812356227407164\n",
      "Finished Iteration:  8\n",
      "Total Deleted:  2750 \t % Deleted:  0.903713440683536\n",
      "Finished Iteration:  9\n",
      "Total Deleted:  3008 \t % Deleted:  0.9884981925731187\n",
      "Finished Iteration:  10\n",
      "Finished Total Iteration:  9\n",
      "Diff:  -0.004708333333333314\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  0\n",
      "Total Deleted:  284 \t % Deleted:  0.09357495881383855\n",
      "Finished Iteration:  1\n",
      "Total Deleted:  586 \t % Deleted:  0.1930807248764415\n",
      "Finished Iteration:  2\n",
      "Total Deleted:  916 \t % Deleted:  0.30181219110378915\n",
      "Finished Iteration:  3\n",
      "Total Deleted:  1199 \t % Deleted:  0.39505766062602965\n",
      "Finished Iteration:  4\n",
      "Total Deleted:  1544 \t % Deleted:  0.5087314662273477\n",
      "Finished Iteration:  5\n",
      "Total Deleted:  1794 \t % Deleted:  0.5911037891268534\n",
      "Finished Iteration:  6\n",
      "Total Deleted:  2131 \t % Deleted:  0.7021416803953872\n",
      "Finished Iteration:  7\n",
      "Total Deleted:  2415 \t % Deleted:  0.7957166392092258\n",
      "Finished Iteration:  8\n",
      "Total Deleted:  2755 \t % Deleted:  0.9077429983525536\n",
      "Finished Iteration:  9\n",
      "Total Deleted:  3003 \t % Deleted:  0.9894563426688633\n",
      "Finished Iteration:  10\n",
      "Finished Total Iteration:  10\n"
     ]
    }
   ],
   "source": [
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "eta = 0.2\n",
    "n = 30000\n",
    "r = 0.2\n",
    "num_iters = 10\n",
    "\n",
    "bias_amts, mean_fidel_maj, mean_fidel_min, mean_fidel, y_err_fidel_maj, y_err_fidel_min, y_err_fidel, df = \\\n",
    "representation(r = r, n = n, apply_fairness=True,verbose=True, num_iters=num_iters, diff_base = False, inter = True)\n",
    "\n",
    "df.to_csv('disp_repr_inter.csv', index = True)\n",
    "df_res = pd.DataFrame(np.array([bias_amts, mean_fidel_maj, mean_fidel_min, mean_fidel, y_err_fidel_maj, y_err_fidel_min, y_err_fidel]).T, columns=['bias_amts', 'mean_fidel_maj', 'mean_fidel_min', 'mean_fidel', 'y_err_fidel_maj', 'y_err_fidel_min', 'y_err_fidel'])\n",
    "df_res.to_csv(f'fid_repr_inter.csv', index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa9b967-bcf5-4dd5-9ee5-4bb1a0528973",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bias_amts = np.divide(list(range(10,-1,-1)),10)\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.errorbar(bias_amts, mean_fidel_maj22, yerr = y_err_fidel_maj22, label = 'Fidelity Majority', color = \"red\")\n",
    "plt.errorbar(bias_amts, mean_fidel_min22, yerr = y_err_fidel_min22, label = 'Fidelity Minority', color = \"green\")\n",
    "plt.xlabel(\"Beta Value for Minority\")\n",
    "plt.ylabel(\"Fidelity\")\n",
    "plt.xlim(1.05, -0.05)\n",
    "plt.ylim(0.92, 1)\n",
    "plt.legend()\n",
    "plt.title(\"Fidelity Plot\")\n",
    "plt.savefig('repr_inter.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3f8590be-6740-467c-8a4c-2232d7918caa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diff:  0.2084166666666667\n",
      "Beta:  1.0 \n",
      "\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  0\n",
      "Beta:  0.9 \n",
      "\n",
      "Total Deleted:  296 \t % Deleted:  0.10088616223585549\n",
      "Finished Iteration:  1\n",
      "Beta:  0.8 \n",
      "\n",
      "Total Deleted:  576 \t % Deleted:  0.19631901840490798\n",
      "Finished Iteration:  2\n",
      "Beta:  0.7 \n",
      "\n",
      "Total Deleted:  878 \t % Deleted:  0.2992501704158146\n",
      "Finished Iteration:  3\n",
      "Beta:  0.6 \n",
      "\n",
      "Total Deleted:  1187 \t % Deleted:  0.4045671438309475\n",
      "Finished Iteration:  4\n",
      "Beta:  0.5 \n",
      "\n",
      "Total Deleted:  1481 \t % Deleted:  0.5047716428084527\n",
      "Finished Iteration:  5\n",
      "Beta:  0.4 \n",
      "\n",
      "Total Deleted:  1786 \t % Deleted:  0.6087252897068848\n",
      "Finished Iteration:  6\n",
      "Beta:  0.3 \n",
      "\n",
      "Total Deleted:  2118 \t % Deleted:  0.721881390593047\n",
      "Finished Iteration:  7\n",
      "Beta:  0.2 \n",
      "\n",
      "Total Deleted:  2338 \t % Deleted:  0.7968643490115883\n",
      "Finished Iteration:  8\n",
      "Beta:  0.1 \n",
      "\n",
      "Total Deleted:  2634 \t % Deleted:  0.8977505112474438\n",
      "Finished Iteration:  9\n",
      "Beta:  0.01 \n",
      "\n",
      "Total Deleted:  2901 \t % Deleted:  0.9887525562372188\n",
      "Finished Iteration:  10\n",
      "Finished Total Iteration:  1\n",
      "Diff:  0.21193750000000006\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  0\n",
      "Total Deleted:  301 \t % Deleted:  0.09976798143851508\n",
      "Finished Iteration:  1\n",
      "Total Deleted:  640 \t % Deleted:  0.2121312562147829\n",
      "Finished Iteration:  2\n",
      "Total Deleted:  927 \t % Deleted:  0.3072588664235996\n",
      "Finished Iteration:  3\n",
      "Total Deleted:  1212 \t % Deleted:  0.4017235664567451\n",
      "Finished Iteration:  4\n",
      "Total Deleted:  1515 \t % Deleted:  0.5021544580709314\n",
      "Finished Iteration:  5\n",
      "Total Deleted:  1779 \t % Deleted:  0.5896586012595293\n",
      "Finished Iteration:  6\n",
      "Total Deleted:  2191 \t % Deleted:  0.7262180974477959\n",
      "Finished Iteration:  7\n",
      "Total Deleted:  2423 \t % Deleted:  0.8031156778256546\n",
      "Finished Iteration:  8\n",
      "Total Deleted:  2719 \t % Deleted:  0.9012263838249918\n",
      "Finished Iteration:  9\n",
      "Total Deleted:  2991 \t % Deleted:  0.9913821677162744\n",
      "Finished Iteration:  10\n",
      "Finished Total Iteration:  2\n",
      "Diff:  0.20008333333333328\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  0\n",
      "Total Deleted:  278 \t % Deleted:  0.0929144385026738\n",
      "Finished Iteration:  1\n",
      "Total Deleted:  640 \t % Deleted:  0.21390374331550802\n",
      "Finished Iteration:  2\n",
      "Total Deleted:  908 \t % Deleted:  0.303475935828877\n",
      "Finished Iteration:  3\n",
      "Total Deleted:  1140 \t % Deleted:  0.3810160427807487\n",
      "Finished Iteration:  4\n",
      "Total Deleted:  1511 \t % Deleted:  0.5050133689839572\n",
      "Finished Iteration:  5\n",
      "Total Deleted:  1809 \t % Deleted:  0.6046122994652406\n",
      "Finished Iteration:  6\n",
      "Total Deleted:  2098 \t % Deleted:  0.7012032085561497\n",
      "Finished Iteration:  7\n",
      "Total Deleted:  2405 \t % Deleted:  0.8038101604278075\n",
      "Finished Iteration:  8\n",
      "Total Deleted:  2672 \t % Deleted:  0.893048128342246\n",
      "Finished Iteration:  9\n",
      "Total Deleted:  2959 \t % Deleted:  0.9889705882352942\n",
      "Finished Iteration:  10\n",
      "Finished Total Iteration:  3\n",
      "Diff:  0.19435416666666672\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  0\n",
      "Total Deleted:  307 \t % Deleted:  0.10082101806239738\n",
      "Finished Iteration:  1\n",
      "Total Deleted:  609 \t % Deleted:  0.2\n",
      "Finished Iteration:  2\n",
      "Total Deleted:  903 \t % Deleted:  0.296551724137931\n",
      "Finished Iteration:  3\n",
      "Total Deleted:  1227 \t % Deleted:  0.40295566502463054\n",
      "Finished Iteration:  4\n",
      "Total Deleted:  1535 \t % Deleted:  0.5041050903119869\n",
      "Finished Iteration:  5\n",
      "Total Deleted:  1822 \t % Deleted:  0.5983579638752052\n",
      "Finished Iteration:  6\n",
      "Total Deleted:  2133 \t % Deleted:  0.7004926108374384\n",
      "Finished Iteration:  7\n",
      "Total Deleted:  2412 \t % Deleted:  0.7921182266009852\n",
      "Finished Iteration:  8\n",
      "Total Deleted:  2739 \t % Deleted:  0.8995073891625616\n",
      "Finished Iteration:  9\n",
      "Total Deleted:  3025 \t % Deleted:  0.993431855500821\n",
      "Finished Iteration:  10\n",
      "Finished Total Iteration:  4\n",
      "Diff:  0.19643750000000004\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  0\n",
      "Total Deleted:  287 \t % Deleted:  0.09534883720930233\n",
      "Finished Iteration:  1\n",
      "Total Deleted:  654 \t % Deleted:  0.21727574750830564\n",
      "Finished Iteration:  2\n",
      "Total Deleted:  909 \t % Deleted:  0.30199335548172757\n",
      "Finished Iteration:  3\n",
      "Total Deleted:  1207 \t % Deleted:  0.4009966777408638\n",
      "Finished Iteration:  4\n",
      "Total Deleted:  1504 \t % Deleted:  0.49966777408637875\n",
      "Finished Iteration:  5\n",
      "Total Deleted:  1826 \t % Deleted:  0.6066445182724253\n",
      "Finished Iteration:  6\n",
      "Total Deleted:  2132 \t % Deleted:  0.7083056478405315\n",
      "Finished Iteration:  7\n",
      "Total Deleted:  2404 \t % Deleted:  0.7986710963455149\n",
      "Finished Iteration:  8\n",
      "Total Deleted:  2723 \t % Deleted:  0.9046511627906977\n",
      "Finished Iteration:  9\n",
      "Total Deleted:  2975 \t % Deleted:  0.9883720930232558\n",
      "Finished Iteration:  10\n",
      "Finished Total Iteration:  5\n",
      "Diff:  0.20560416666666664\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  0\n",
      "Total Deleted:  301 \t % Deleted:  0.1021377672209026\n",
      "Finished Iteration:  1\n",
      "Total Deleted:  587 \t % Deleted:  0.1991856124872752\n",
      "Finished Iteration:  2\n",
      "Total Deleted:  850 \t % Deleted:  0.28842891075670174\n",
      "Finished Iteration:  3\n",
      "Total Deleted:  1144 \t % Deleted:  0.38819138106549034\n",
      "Finished Iteration:  4\n",
      "Total Deleted:  1483 \t % Deleted:  0.503223617237869\n",
      "Finished Iteration:  5\n",
      "Total Deleted:  1729 \t % Deleted:  0.5866983372921615\n",
      "Finished Iteration:  6\n",
      "Total Deleted:  2046 \t % Deleted:  0.6942653545978962\n",
      "Finished Iteration:  7\n",
      "Total Deleted:  2354 \t % Deleted:  0.7987784187309128\n",
      "Finished Iteration:  8\n",
      "Total Deleted:  2666 \t % Deleted:  0.9046487953851374\n",
      "Finished Iteration:  9\n",
      "Total Deleted:  2916 \t % Deleted:  0.989480827960638\n",
      "Finished Iteration:  10\n",
      "Finished Total Iteration:  6\n",
      "Diff:  0.19258333333333333\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  0\n",
      "Total Deleted:  318 \t % Deleted:  0.1033810143042913\n",
      "Finished Iteration:  1\n",
      "Total Deleted:  601 \t % Deleted:  0.19538361508452537\n",
      "Finished Iteration:  2\n",
      "Total Deleted:  891 \t % Deleted:  0.28966189856957086\n",
      "Finished Iteration:  3\n",
      "Total Deleted:  1264 \t % Deleted:  0.41092327698309494\n",
      "Finished Iteration:  4\n",
      "Total Deleted:  1516 \t % Deleted:  0.4928478543563069\n",
      "Finished Iteration:  5\n",
      "Total Deleted:  1857 \t % Deleted:  0.60370611183355\n",
      "Finished Iteration:  6\n",
      "Total Deleted:  2176 \t % Deleted:  0.7074122236671001\n",
      "Finished Iteration:  7\n",
      "Total Deleted:  2458 \t % Deleted:  0.7990897269180754\n",
      "Finished Iteration:  8\n",
      "Total Deleted:  2772 \t % Deleted:  0.9011703511053316\n",
      "Finished Iteration:  9\n",
      "Total Deleted:  3046 \t % Deleted:  0.9902470741222367\n",
      "Finished Iteration:  10\n",
      "Finished Total Iteration:  7\n",
      "Diff:  0.1993333333333333\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  0\n",
      "Total Deleted:  311 \t % Deleted:  0.10352862849533954\n",
      "Finished Iteration:  1\n",
      "Total Deleted:  586 \t % Deleted:  0.19507323568575233\n",
      "Finished Iteration:  2\n",
      "Total Deleted:  898 \t % Deleted:  0.2989347536617843\n",
      "Finished Iteration:  3\n",
      "Total Deleted:  1217 \t % Deleted:  0.4051264980026631\n",
      "Finished Iteration:  4\n",
      "Total Deleted:  1525 \t % Deleted:  0.5076564580559254\n",
      "Finished Iteration:  5\n",
      "Total Deleted:  1821 \t % Deleted:  0.6061917443408789\n",
      "Finished Iteration:  6\n",
      "Total Deleted:  2100 \t % Deleted:  0.6990679094540613\n",
      "Finished Iteration:  7\n",
      "Total Deleted:  2394 \t % Deleted:  0.7969374167776299\n",
      "Finished Iteration:  8\n",
      "Total Deleted:  2718 \t % Deleted:  0.9047936085219707\n",
      "Finished Iteration:  9\n",
      "Total Deleted:  2967 \t % Deleted:  0.9876830892143809\n",
      "Finished Iteration:  10\n",
      "Finished Total Iteration:  8\n",
      "Diff:  0.19466666666666677\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  0\n",
      "Total Deleted:  312 \t % Deleted:  0.1025303976339139\n",
      "Finished Iteration:  1\n",
      "Total Deleted:  603 \t % Deleted:  0.19815971081169897\n",
      "Finished Iteration:  2\n",
      "Total Deleted:  923 \t % Deleted:  0.3033190930003286\n",
      "Finished Iteration:  3\n",
      "Total Deleted:  1178 \t % Deleted:  0.3871179756818929\n",
      "Finished Iteration:  4\n",
      "Total Deleted:  1536 \t % Deleted:  0.5047650345054223\n",
      "Finished Iteration:  5\n",
      "Total Deleted:  1840 \t % Deleted:  0.6046664475846204\n",
      "Finished Iteration:  6\n",
      "Total Deleted:  2122 \t % Deleted:  0.6973381531383503\n",
      "Finished Iteration:  7\n",
      "Total Deleted:  2472 \t % Deleted:  0.812356227407164\n",
      "Finished Iteration:  8\n",
      "Total Deleted:  2750 \t % Deleted:  0.903713440683536\n",
      "Finished Iteration:  9\n",
      "Total Deleted:  3008 \t % Deleted:  0.9884981925731187\n",
      "Finished Iteration:  10\n",
      "Finished Total Iteration:  9\n",
      "Diff:  0.19658333333333333\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  0\n",
      "Total Deleted:  284 \t % Deleted:  0.09357495881383855\n",
      "Finished Iteration:  1\n",
      "Total Deleted:  586 \t % Deleted:  0.1930807248764415\n",
      "Finished Iteration:  2\n",
      "Total Deleted:  916 \t % Deleted:  0.30181219110378915\n",
      "Finished Iteration:  3\n",
      "Total Deleted:  1199 \t % Deleted:  0.39505766062602965\n",
      "Finished Iteration:  4\n",
      "Total Deleted:  1544 \t % Deleted:  0.5087314662273477\n",
      "Finished Iteration:  5\n",
      "Total Deleted:  1794 \t % Deleted:  0.5911037891268534\n",
      "Finished Iteration:  6\n",
      "Total Deleted:  2131 \t % Deleted:  0.7021416803953872\n",
      "Finished Iteration:  7\n",
      "Total Deleted:  2415 \t % Deleted:  0.7957166392092258\n",
      "Finished Iteration:  8\n",
      "Total Deleted:  2755 \t % Deleted:  0.9077429983525536\n",
      "Finished Iteration:  9\n",
      "Total Deleted:  3003 \t % Deleted:  0.9894563426688633\n",
      "Finished Iteration:  10\n",
      "Finished Total Iteration:  10\n"
     ]
    }
   ],
   "source": [
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "eta = 0.2\n",
    "n = 30000\n",
    "r = 0.2\n",
    "num_iters = 10\n",
    "\n",
    "bias_amts, mean_fidel_maj, mean_fidel_min, mean_fidel, y_err_fidel_maj, y_err_fidel_min, y_err_fidel, df = \\\n",
    "representation(r = r, n = n, apply_fairness=True,verbose=True, num_iters=num_iters, diff_base = True, inter = True)\n",
    "\n",
    "df.to_csv('disp_repr_diff_base.csv', index = True)\n",
    "df_res = pd.DataFrame(np.array([bias_amts, mean_fidel_maj, mean_fidel_min, mean_fidel, y_err_fidel_maj, y_err_fidel_min, y_err_fidel]).T, columns=['bias_amts', 'mean_fidel_maj', 'mean_fidel_min', 'mean_fidel', 'y_err_fidel_maj', 'y_err_fidel_min', 'y_err_fidel'])\n",
    "df_res.to_csv(f'fid_repr_diff_base.csv', index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db45ad24-7410-4508-80e8-1a603a247728",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bias_amts = np.divide(list(range(10,-1,-1)),10)\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.errorbar(bias_amts, mean_fidel_maj23, yerr = y_err_fidel_maj23, label = 'Fidelity Majority', color = \"red\")\n",
    "plt.errorbar(bias_amts, mean_fidel_min23, yerr = y_err_fidel_min23, label = 'Fidelity Minority', color = \"green\")\n",
    "plt.xlabel(\"Beta Value for Minority\")\n",
    "plt.ylabel(\"Fidelity\")\n",
    "plt.xlim(1.05, -0.05)\n",
    "plt.ylim(0.92, 1)\n",
    "plt.legend()\n",
    "plt.title(\"Fidelity Plot\")\n",
    "plt.savefig('repr_diff_base.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83012064-1e3e-47c5-83b9-e49c38db2e90",
   "metadata": {},
   "source": [
    "## Label Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "83663e42-1cb4-4ecb-991e-bd4d83ed3d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flip(df_synthetic, eta):\n",
    "    labels = df_synthetic['outcome'].values\n",
    "    #print('Before:', df_synthetic['outcome'].value_counts())\n",
    "    num_flipped = 0\n",
    "    for i in range(len(labels)):\n",
    "        if random.uniform(0,1) <= eta:\n",
    "            labels[i] = 1 if labels[i] == 0 else 0\n",
    "            num_flipped += 1\n",
    "    df_synthetic['outcome'] = labels\n",
    "    #print('After:', df_synthetic['outcome'].value_counts())\n",
    "    print('Bias Num Flipped: ', num_flipped, \"\\tRate: \", num_flipped / len(df_synthetic))\n",
    "    return df_synthetic\n",
    "\n",
    "def get_noise(df, beta, inter = False):\n",
    "    df_majority = df[df['cat'] == 1]\n",
    "    df_minority = df[df['cat'] == 0]\n",
    "    \n",
    "    if inter:\n",
    "        # unfavored group with negative label\n",
    "        df_minority_negative = df_minority[df_minority['outcome'] == 0.0]\n",
    "\n",
    "        # unfavored group with positive label (preferred)\n",
    "        df_minority_positive = df_minority[df_minority['outcome'] == 1.0]\n",
    "\n",
    "        # data frame without positively labeled examples from minority class\n",
    "        df_total = pd.concat([df_majority, df_minority_negative])\n",
    "        \n",
    "        df_undersampled = flip(df_minority_positive, beta)\n",
    "\n",
    "        # combine undersampled and original favored class to create dataset\n",
    "        df_concat = pd.concat([df_total,df_undersampled])\n",
    "\n",
    "        return df_concat.sample(frac=1, random_state = 42) # permute data\n",
    "    \n",
    "    else:\n",
    "        df_undersampled = flip(df_minority, beta)\n",
    "\n",
    "        # combine undersampled and original favored class to create dataset\n",
    "        df_concat = pd.concat([df_majority,df_undersampled])\n",
    "\n",
    "        return df_concat.sample(frac=1, random_state = 42) # permute data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a92a2670-6214-46a0-8845-1a19baeeee82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_noise(r, n, apply_fairness = True, verbose = False, num_iters = 10, inter = False, diff_base = False):\n",
    "    \n",
    "    total_fidel_maj = []\n",
    "    total_fidel_min = []\n",
    "    total_fidel = []\n",
    "    \n",
    "    total_disp_bias_train = []\n",
    "    total_disp_bo_train = []\n",
    "    total_disp_mitigated_train = []\n",
    "    \n",
    "    total_disp_bias_test = []\n",
    "    total_disp_bo_test = []\n",
    "    total_disp_mitigated_test = []\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "    \n",
    "        # 1 to 0 in increments of 0.1\n",
    "        bias_amts = np.divide(list(range(1,11,1)),22)\n",
    "\n",
    "        test_maj = []\n",
    "        test_min = []\n",
    "        total = []\n",
    "        \n",
    "        disp_bias_train = []\n",
    "        disp_bo_train = []\n",
    "        disp_mitigated_train = []\n",
    "        \n",
    "        disp_bias_test = []\n",
    "        disp_bo_test = []\n",
    "        disp_mitigated_test = []\n",
    "        \n",
    "        count = 0\n",
    "        \n",
    "        if diff_base: \n",
    "            outcome_continuous, df_synthetic = true_label_generation(r=r, eta=eta, n=2*n, maj_means =[0.7,0.7,0.7])\n",
    "        else: outcome_continuous, df_synthetic = true_label_generation(r=r, eta=eta, n=2*n)        \n",
    "        \n",
    "        threshold = 0.5\n",
    "        exact_bo_labels = np.where(outcome_continuous < threshold, 0, 1)\n",
    "        exact_bo_labels_train = np.array(exact_bo_labels[range(0,n)])\n",
    "        exact_bo_labels_test = np.array(exact_bo_labels[range(n,len(df_synthetic))])\n",
    "        \n",
    "        # split into train and test\n",
    "        df_train = df_synthetic.loc[range(0,n), :]\n",
    "        \n",
    "        df_train_transf, maj_list, min_list = transform(df_train, True, True)\n",
    "        \n",
    "        df_test = df_synthetic.loc[range(n, len(df_synthetic)),:]\n",
    "        df_test_transf, maj_list, min_list = transform(df_test, True, False)\n",
    "        \n",
    "        df_test_maj = df_test_transf.loc[maj_list]\n",
    "        df_test_min = df_test_transf.loc[min_list]\n",
    "\n",
    "        # format training data\n",
    "        X_true = df_train_transf.iloc[:, :-1].values\n",
    "        y_true = df_train_transf.iloc[:, -1].values\n",
    "        \n",
    "        # format test data\n",
    "        X_test = df_test_transf.iloc[:, :-1].values\n",
    "        X_test_maj = df_test_maj.iloc[:, :-1].values\n",
    "        X_test_min = df_test_min.iloc[:, :-1].values\n",
    "        y_test = df_test_transf.iloc[:, -1].values\n",
    "        y_test_maj = df_test_maj.iloc[:, -1].values\n",
    "        y_test_min = df_test_min.iloc[:, -1].values\n",
    "        \n",
    "        sens_attr_test = df_test['cat']\n",
    "        \n",
    "        for beta in bias_amts:\n",
    "            \n",
    "            if i == 0: print(\"Beta: \", beta, '\\n')\n",
    "\n",
    "            df_train_copy = df_train.copy()\n",
    "\n",
    "            df_majority = df_train_copy[df_train_copy['cat'] == 1]\n",
    "            df_minority = df_train_copy[df_train_copy['cat'] == 0]\n",
    "\n",
    "            # unfavored group with negative label\n",
    "            df_minority_negative = df_minority[df_minority['outcome'] == 0.0]\n",
    "\n",
    "            # unfavored group with positive label (preferred)\n",
    "            df_minority_positive = df_minority[df_minority['outcome'] == 1.0]\n",
    "\n",
    "            # data frame without positively labeled examples from minority class\n",
    "            df_total = pd.concat([df_majority, df_minority_negative])\n",
    "\n",
    "            # under-sampling process\n",
    "            df_undersampled = under(df_minority_positive, 1)\n",
    "\n",
    "            # combine undersampled and original favored class to create dataset\n",
    "            df_concat1 = pd.concat([df_total,df_undersampled])\n",
    "            \n",
    "            df_concat = get_noise(df_concat1, beta, inter) # group-dependent label noise\n",
    "\n",
    "            # format data\n",
    "            X_bias_true, y_bias_true, df_sens = transform(df_concat)\n",
    "            \n",
    "            # model trained on biased data\n",
    "            classifier = LogisticRegression(solver = 'lbfgs', random_state = 42, penalty = 'none')\n",
    "            classifier_bias = classifier.fit(X_bias_true, y_bias_true)\n",
    "            \n",
    "            acc = accuracy_score(y_test,classifier_bias.predict(X_test))\n",
    "            #print(f'Biased classifier:')\n",
    "            #print(f'     Test accuracy = {acc}')\n",
    "            \n",
    "            m = classifier_bias\n",
    "            def classify(X): return m.predict(X)\n",
    "            error = ErrorRate()\n",
    "            error.load_data(X_bias_true, pd.Series(y_bias_true), sensitive_features=df_sens)\n",
    "            disparity = EqualizedOdds()\n",
    "            disparity.load_data(X_bias_true, pd.Series(y_bias_true), sensitive_features=df_sens)\n",
    "            #print(f'     Train error = {error.gamma(classify)[0]}')\n",
    "            #print(f'     Train disparity = {disparity.gamma(classify).max()}')\n",
    "            disp_bias_train += [disparity.gamma(classify).max()]\n",
    "\n",
    "            m = classifier_bias\n",
    "            def classify(X): return m.predict(X)\n",
    "            error = ErrorRate()\n",
    "            error.load_data(X_test, pd.Series(y_test), sensitive_features=sens_attr_test)\n",
    "            disparity = EqualizedOdds()\n",
    "            disparity.load_data(X_test, pd.Series(y_test), sensitive_features=sens_attr_test)\n",
    "            #print(f'     Test error = {error.gamma(classify)[0]}')\n",
    "            #print(f'     Test disparity = {disparity.gamma(classify).max()}')\n",
    "            disp_bias_test += [disparity.gamma(classify).max()]\n",
    "            \n",
    "            # Learned bayes optimal classifier\n",
    "            classifier = LogisticRegression(solver = 'lbfgs', random_state = 42, penalty = 'none')\n",
    "            classifier_b = classifier.fit(X_true, y_true)                \n",
    "            #classifier_b = clone(classifier).fit(X_true, y_true)\n",
    "            acc = accuracy_score(y_test,classifier_b.predict(X_test))\n",
    "            #print(f'Learned BO classifier:')\n",
    "            #print(f'     Test accuracy = {acc}')\n",
    "            \n",
    "            m = classifier_b\n",
    "            def classify(X): return m.predict(X)\n",
    "            error = ErrorRate()\n",
    "            error.load_data(X_bias_true, pd.Series(y_bias_true), sensitive_features=df_sens)\n",
    "            disparity = EqualizedOdds()\n",
    "            disparity.load_data(X_bias_true, pd.Series(y_bias_true), sensitive_features=df_sens)\n",
    "            #print(f'     Train error = {error.gamma(classify)[0]}')\n",
    "            #print(f'     Train disparity = {disparity.gamma(classify).max()}')\n",
    "            disp_bo_train += [disparity.gamma(classify).max()]\n",
    "\n",
    "            m = classifier_b\n",
    "            def classify(X): return m.predict(X)\n",
    "            error = ErrorRate()\n",
    "            error.load_data(X_test, pd.Series(y_test), sensitive_features=sens_attr_test)\n",
    "            disparity = EqualizedOdds()\n",
    "            disparity.load_data(X_test, pd.Series(y_test), sensitive_features=sens_attr_test)\n",
    "            #print(f'     Test error = {error.gamma(classify)[0]}')\n",
    "            #print(f'     Test disparity = {disparity.gamma(classify).max()}')\n",
    "            disp_bo_test += [disparity.gamma(classify).max()]\n",
    "            \n",
    "            \n",
    "            # Exact BO optimal classifier\n",
    "            #print(f'Exact BO classifier:')\n",
    "            acc = accuracy_score(y_test,exact_bo_labels_test)\n",
    "            #print(f'     Test accuracy = {acc}')\n",
    "            acc = accuracy_score(y_true,exact_bo_labels_train)\n",
    "            #print(f'     Train accuracy = {acc}')\n",
    "\n",
    "            if apply_fairness:\n",
    "                constraint = EqualizedOdds()\n",
    "                classifier = LogisticRegression(solver = 'lbfgs', random_state = 42, penalty = 'none', fit_intercept = False, max_iter = 200)   \n",
    "\n",
    "                classifier_mitigated_bias = GridSearch(estimator=classifier,\n",
    "                                                       constraints=constraint,\n",
    "                                                       selection_rule='tradeoff_optimization',\n",
    "                                                       constraint_weight=0.5,\n",
    "                                                       grid_size=10,\n",
    "                                                       grid_limit=2.0,\n",
    "                                                       grid_offset=None,\n",
    "                                                       grid=None,\n",
    "                                                       sample_weight_name='sample_weight')\n",
    "                                                       \n",
    "                classifier_mitigated_bias.fit(X_bias_true, y_bias_true, sensitive_features = df_sens)\n",
    "                \n",
    "                acc = accuracy_score(y_test,classifier_mitigated_bias.predict(X_test))\n",
    "                #print(f'Mitigated bias classifier:')\n",
    "                #print(f'     Test accuracy = {acc}')\n",
    "                \n",
    "                m = classifier_mitigated_bias\n",
    "                def classify(X): return m.predict(X)\n",
    "                error = ErrorRate()\n",
    "                error.load_data(X_bias_true, pd.Series(y_bias_true), sensitive_features=df_sens)\n",
    "                disparity = EqualizedOdds()\n",
    "                disparity.load_data(X_bias_true, pd.Series(y_bias_true), sensitive_features=df_sens)\n",
    "                #print(f'     Train error = {error.gamma(classify)[0]}')\n",
    "                #print(f'     Train disparity = {disparity.gamma(classify).max()}')\n",
    "                disp_mitigated_train += [disparity.gamma(classify).max()]\n",
    "                \n",
    "                m = classifier_mitigated_bias\n",
    "                def classify(X): return m.predict(X)\n",
    "                error = ErrorRate()\n",
    "                error.load_data(X_test, pd.Series(y_test), sensitive_features=sens_attr_test)\n",
    "                disparity = EqualizedOdds()\n",
    "                disparity.load_data(X_test, pd.Series(y_test), sensitive_features=sens_attr_test)\n",
    "                #print(f'     Test error = {error.gamma(classify)[0]}')\n",
    "                #print(f'     Test disparity = {disparity.gamma(classify).max()}')\n",
    "                disp_mitigated_test += [disparity.gamma(classify).max()]\n",
    "                \n",
    "                # Alternative fidelity of intervention model to no intervention model\n",
    "                alt_fid_train = accuracy_score(classifier_mitigated_bias.predict(X_bias_true),classifier_bias.predict(X_bias_true))\n",
    "                alt_fid_test = accuracy_score(classifier_mitigated_bias.predict(X_test),classifier_bias.predict(X_test))\n",
    "                #print(f'Alternative fidelity of intervention model to no intervention model: train = {alt_fid_train}, test = {alt_fid_test}')\n",
    "                \n",
    "\n",
    "            else:\n",
    "                classifier_mitigated_bias = clone(classifier_bias)\n",
    "                classifier_mitigated_bias.fit(X_bias_true, y_bias_true)\n",
    "                \n",
    "                # NOTE: disparities are the same as for classifier_bias\n",
    "\n",
    "                \n",
    "            # Fidelity in this step\n",
    "            fid = accuracy_score(classifier_mitigated_bias.predict(X_test),classifier_b.predict(X_test))\n",
    "            #print(f'Test set fidelity of learned BO classifier and mitigated_bias_classifier: {fid}')\n",
    "            fid = accuracy_score(classifier_mitigated_bias.predict(X_test),exact_bo_labels_test)\n",
    "            #print(f'Test set fidelity of exact BO classifier and mitigated biased classifier: {fid}')\n",
    "            fid = accuracy_score(classifier_bias.predict(X_test),classifier_b.predict(X_test))\n",
    "            #print(f'Test set fidelity of learned BO classifier and biased classifier: {fid}')\n",
    "            \n",
    "            # fidelity check\n",
    "            test_maj += [accuracy_score(classifier_mitigated_bias.predict(X_test_maj), classifier_b.predict(X_test_maj))]\n",
    "            test_min += [accuracy_score(classifier_mitigated_bias.predict(X_test_min), classifier_b.predict(X_test_min))]\n",
    "            total += [accuracy_score(classifier_mitigated_bias.predict(X_test), classifier_b.predict(X_test))]\n",
    "                \n",
    "            if verbose:\n",
    "                print(\"Finished Iteration: \", count)\n",
    "                count +=1\n",
    "        \n",
    "        #print(f'Fidelity test maj: {test_maj}')\n",
    "        #print(f'Fidelity test min: {test_maj}')\n",
    "        \n",
    "        total_fidel_maj.append(test_maj)\n",
    "        total_fidel_min.append(test_min)\n",
    "        total_fidel.append(total)\n",
    "        \n",
    "        total_disp_bias_train.append(disp_bias_train)\n",
    "        total_disp_bo_train.append(disp_bo_train)\n",
    "        total_disp_mitigated_train.append(disp_mitigated_train)\n",
    "\n",
    "        total_disp_bias_test.append(disp_bias_test)\n",
    "        total_disp_bo_test.append(disp_bo_test)\n",
    "        total_disp_mitigated_test.append(disp_mitigated_test)\n",
    "        \n",
    "        if verbose:\n",
    "                print(\"Finished Total Iteration: \", i+1)\n",
    "    \n",
    "    mean_fidel_maj = np.mean(total_fidel_maj, axis = 0)\n",
    "    mean_fidel_min = np.mean(total_fidel_min, axis = 0)\n",
    "    mean_fidel = np.mean(total_fidel, axis = 0)\n",
    "    \n",
    "    mean_disp_bias_train = np.mean(total_disp_bias_train, axis = 0)\n",
    "    mean_disp_bo_train = np.mean(total_disp_bo_train, axis = 0)\n",
    "    mean_disp_mitigated_train = np.mean(total_disp_mitigated_train, axis = 0)\n",
    "    \n",
    "    mean_disp_bias_test = np.mean(total_disp_bias_test, axis = 0)\n",
    "    mean_disp_bo_test = np.mean(total_disp_bo_test, axis = 0)\n",
    "    mean_disp_mitigated_test = np.mean(total_disp_mitigated_test, axis = 0)\n",
    "    \n",
    "    y_err_fidel_maj = np.std(total_fidel_maj, axis = 0)\n",
    "    y_err_fidel_min = np.std(total_fidel_min, axis = 0)\n",
    "    y_err_fidel = np.std(total_fidel, axis = 0)\n",
    "    \n",
    "    df = pd.DataFrame({\"Biased Train\" : mean_disp_bias_train,\n",
    "                       \"BO Train\" : mean_disp_bo_train,\n",
    "                       \"Mitigated Train\" : mean_disp_mitigated_train,\n",
    "                       \"Biased Test\" : mean_disp_bias_test,\n",
    "                       \"BO Test\" : mean_disp_bo_test,\n",
    "                       \"Mitigated Test\" : mean_disp_mitigated_test})\n",
    "   \n",
    "    return bias_amts, mean_fidel_maj, mean_fidel_min, mean_fidel, y_err_fidel_maj, y_err_fidel_min, y_err_fidel, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "be596a58-8b4f-4adb-b9b9-01f13a984f11",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diff:  0.008458333333333345\n",
      "Beta:  0.045454545454545456 \n",
      "\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  273 \tRate:  0.046083727211343684\n",
      "Finished Iteration:  0\n",
      "Beta:  0.09090909090909091 \n",
      "\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  566 \tRate:  0.09554355165428764\n",
      "Finished Iteration:  1\n",
      "Beta:  0.13636363636363635 \n",
      "\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  800 \tRate:  0.1350438892640108\n",
      "Finished Iteration:  2\n",
      "Beta:  0.18181818181818182 \n",
      "\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  1073 \tRate:  0.1811276164753545\n",
      "Finished Iteration:  3\n",
      "Beta:  0.22727272727272727 \n",
      "\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  1386 \tRate:  0.2339635381498987\n",
      "Finished Iteration:  4\n",
      "Beta:  0.2727272727272727 \n",
      "\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  1634 \tRate:  0.27582714382174206\n",
      "Finished Iteration:  5\n",
      "Beta:  0.3181818181818182 \n",
      "\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  1875 \tRate:  0.3165091154625253\n",
      "Finished Iteration:  6\n",
      "Beta:  0.36363636363636365 \n",
      "\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  2148 \tRate:  0.362592842673869\n",
      "Finished Iteration:  7\n",
      "Beta:  0.4090909090909091 \n",
      "\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  2379 \tRate:  0.4015867656988521\n",
      "Finished Iteration:  8\n",
      "Beta:  0.45454545454545453 \n",
      "\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  2629 \tRate:  0.4437879810938555\n",
      "Finished Iteration:  9\n",
      "Finished Total Iteration:  1\n",
      "Diff:  0.010937499999999989\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  262 \tRate:  0.04372496662216288\n",
      "Finished Iteration:  0\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  566 \tRate:  0.09445927903871829\n",
      "Finished Iteration:  1\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  858 \tRate:  0.1431909212283044\n",
      "Finished Iteration:  2\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  1086 \tRate:  0.18124165554072097\n",
      "Finished Iteration:  3\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  1304 \tRate:  0.21762349799732977\n",
      "Finished Iteration:  4\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  1621 \tRate:  0.27052736982643527\n",
      "Finished Iteration:  5\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  1904 \tRate:  0.3177570093457944\n",
      "Finished Iteration:  6\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  2148 \tRate:  0.35847797062750336\n",
      "Finished Iteration:  7\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  2437 \tRate:  0.4067089452603471\n",
      "Finished Iteration:  8\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  2830 \tRate:  0.47229639519359146\n",
      "Finished Iteration:  9\n",
      "Finished Total Iteration:  2\n",
      "Diff:  8.333333333332416e-05\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  258 \tRate:  0.04305022526280661\n",
      "Finished Iteration:  0\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  538 \tRate:  0.08977139996662774\n",
      "Finished Iteration:  1\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  809 \tRate:  0.13499082262639747\n",
      "Finished Iteration:  2\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  1141 \tRate:  0.1903887869180711\n",
      "Finished Iteration:  3\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  1388 \tRate:  0.23160353746037043\n",
      "Finished Iteration:  4\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  1643 \tRate:  0.27415317870849326\n",
      "Finished Iteration:  5\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  1893 \tRate:  0.3158685132654764\n",
      "Finished Iteration:  6\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  2167 \tRate:  0.3615885199399299\n",
      "Finished Iteration:  7\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  2449 \tRate:  0.4086434173202069\n",
      "Finished Iteration:  8\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  2742 \tRate:  0.45753378942099115\n",
      "Finished Iteration:  9\n",
      "Finished Total Iteration:  3\n",
      "Diff:  -0.00520833333333337\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  243 \tRate:  0.04021847070506455\n",
      "Finished Iteration:  0\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  544 \tRate:  0.09003641178417743\n",
      "Finished Iteration:  1\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  808 \tRate:  0.13373055279708707\n",
      "Finished Iteration:  2\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  1154 \tRate:  0.19099635882158225\n",
      "Finished Iteration:  3\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  1348 \tRate:  0.22310493214167496\n",
      "Finished Iteration:  4\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  1642 \tRate:  0.2717643164515061\n",
      "Finished Iteration:  5\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  1930 \tRate:  0.319430652101953\n",
      "Finished Iteration:  6\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  2185 \tRate:  0.36163522012578614\n",
      "Finished Iteration:  7\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  2489 \tRate:  0.4119496855345912\n",
      "Finished Iteration:  8\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  2721 \tRate:  0.4503475670307845\n",
      "Finished Iteration:  9\n",
      "Finished Total Iteration:  4\n",
      "Diff:  -0.00585416666666666\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  270 \tRate:  0.045105245573003674\n",
      "Finished Iteration:  0\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  566 \tRate:  0.09455395923822252\n",
      "Finished Iteration:  1\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  848 \tRate:  0.14166388239224859\n",
      "Finished Iteration:  2\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  1105 \tRate:  0.18459739391914468\n",
      "Finished Iteration:  3\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  1368 \tRate:  0.22853324423655194\n",
      "Finished Iteration:  4\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  1595 \tRate:  0.26645506181089207\n",
      "Finished Iteration:  5\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  1890 \tRate:  0.31573671901102573\n",
      "Finished Iteration:  6\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  2139 \tRate:  0.3573337788172402\n",
      "Finished Iteration:  7\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  2434 \tRate:  0.4066154360173739\n",
      "Finished Iteration:  8\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  2650 \tRate:  0.4426996324757768\n",
      "Finished Iteration:  9\n",
      "Finished Total Iteration:  5\n",
      "Diff:  0.0014583333333333393\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  265 \tRate:  0.04402724705100515\n",
      "Finished Iteration:  0\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  564 \tRate:  0.09370327296893172\n",
      "Finished Iteration:  1\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  831 \tRate:  0.13806280112975577\n",
      "Finished Iteration:  2\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  1109 \tRate:  0.18424987539458382\n",
      "Finished Iteration:  3\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  1377 \tRate:  0.22877554411031734\n",
      "Finished Iteration:  4\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  1691 \tRate:  0.2809436783518857\n",
      "Finished Iteration:  5\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  1963 \tRate:  0.326133909287257\n",
      "Finished Iteration:  6\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  2209 \tRate:  0.36700448579498257\n",
      "Finished Iteration:  7\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  2520 \tRate:  0.41867419837182257\n",
      "Finished Iteration:  8\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  2762 \tRate:  0.4588802126599103\n",
      "Finished Iteration:  9\n",
      "Finished Total Iteration:  6\n",
      "Diff:  -0.009749999999999981\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  282 \tRate:  0.04695304695304695\n",
      "Finished Iteration:  0\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  521 \tRate:  0.08674658674658675\n",
      "Finished Iteration:  1\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  813 \tRate:  0.13536463536463536\n",
      "Finished Iteration:  2\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  1058 \tRate:  0.17615717615717616\n",
      "Finished Iteration:  3\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  1370 \tRate:  0.2281052281052281\n",
      "Finished Iteration:  4\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  1705 \tRate:  0.2838827838827839\n",
      "Finished Iteration:  5\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  1910 \tRate:  0.318015318015318\n",
      "Finished Iteration:  6\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  2215 \tRate:  0.3687978687978688\n",
      "Finished Iteration:  7\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  2382 \tRate:  0.3966033966033966\n",
      "Finished Iteration:  8\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  2761 \tRate:  0.4597069597069597\n",
      "Finished Iteration:  9\n",
      "Finished Total Iteration:  7\n",
      "Diff:  0.0009583333333333388\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  295 \tRate:  0.04947174241153782\n",
      "Finished Iteration:  0\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  560 \tRate:  0.09391246017105484\n",
      "Finished Iteration:  1\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  860 \tRate:  0.14422270669126278\n",
      "Finished Iteration:  2\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  1078 \tRate:  0.18078148582928055\n",
      "Finished Iteration:  3\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  1344 \tRate:  0.2253899044105316\n",
      "Finished Iteration:  4\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  1582 \tRate:  0.2653026999832299\n",
      "Finished Iteration:  5\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  1877 \tRate:  0.31477444239476776\n",
      "Finished Iteration:  6\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  2080 \tRate:  0.34881770920677513\n",
      "Finished Iteration:  7\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  2417 \tRate:  0.40533288613114205\n",
      "Finished Iteration:  8\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  2741 \tRate:  0.45966795237296665\n",
      "Finished Iteration:  9\n",
      "Finished Total Iteration:  8\n",
      "Diff:  -0.005312499999999942\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  291 \tRate:  0.04825870646766169\n",
      "Finished Iteration:  0\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  550 \tRate:  0.0912106135986733\n",
      "Finished Iteration:  1\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  836 \tRate:  0.13864013266998343\n",
      "Finished Iteration:  2\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  1110 \tRate:  0.18407960199004975\n",
      "Finished Iteration:  3\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  1417 \tRate:  0.23499170812603648\n",
      "Finished Iteration:  4\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  1570 \tRate:  0.2603648424543947\n",
      "Finished Iteration:  5\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  1892 \tRate:  0.3137645107794362\n",
      "Finished Iteration:  6\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  2217 \tRate:  0.36766169154228856\n",
      "Finished Iteration:  7\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  2511 \tRate:  0.4164179104477612\n",
      "Finished Iteration:  8\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  2746 \tRate:  0.45538971807628525\n",
      "Finished Iteration:  9\n",
      "Finished Total Iteration:  9\n",
      "Diff:  -0.004708333333333314\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  301 \tRate:  0.050058207217694994\n",
      "Finished Iteration:  0\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  530 \tRate:  0.08814235822384833\n",
      "Finished Iteration:  1\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  833 \tRate:  0.13853317811408614\n",
      "Finished Iteration:  2\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  1057 \tRate:  0.17578579743888242\n",
      "Finished Iteration:  3\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  1382 \tRate:  0.2298353567270913\n",
      "Finished Iteration:  4\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  1573 \tRate:  0.261599866954931\n",
      "Finished Iteration:  5\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  1917 \tRate:  0.3188092466322967\n",
      "Finished Iteration:  6\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  2233 \tRate:  0.3713620488940629\n",
      "Finished Iteration:  7\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  2457 \tRate:  0.4086146682188591\n",
      "Finished Iteration:  8\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  2776 \tRate:  0.46166638948943955\n",
      "Finished Iteration:  9\n",
      "Finished Total Iteration:  10\n"
     ]
    }
   ],
   "source": [
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "eta = 0.2\n",
    "n = 30000\n",
    "r = 0.2\n",
    "num_iters = 10\n",
    "\n",
    "bias_amts, mean_fidel_maj, mean_fidel_min, mean_fidel, y_err_fidel_maj, y_err_fidel_min, y_err_fidel, df = \\\n",
    "label_noise(r = r, n = n, apply_fairness=True,verbose=True, num_iters=num_iters, diff_base = False, inter = False)\n",
    "\n",
    "df.to_csv('disp_label_noise_no_inter.csv', index = True)\n",
    "df_res = pd.DataFrame(np.array([bias_amts, mean_fidel_maj, mean_fidel_min, mean_fidel, y_err_fidel_maj, y_err_fidel_min, y_err_fidel]).T, columns=['bias_amts', 'mean_fidel_maj', 'mean_fidel_min', 'mean_fidel', 'y_err_fidel_maj', 'y_err_fidel_min', 'y_err_fidel'])\n",
    "df_res.to_csv(f'fid_label_noise_no_inter.csv', index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a85d73b-c547-44a1-9fbe-a2e155a35249",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bias_amts = np.divide(list(range(1,11,1)),22)\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.errorbar(bias_amts, mean_fidel_maj24, yerr = y_err_fidel_maj24, label = 'Fidelity Majority', color = \"red\")\n",
    "plt.errorbar(bias_amts, mean_fidel_min24, yerr = y_err_fidel_min24, label = 'Fidelity Minority', color = \"green\")\n",
    "plt.xlabel(\"Eta Minority Value\")\n",
    "plt.ylabel(\"Fidelity\")\n",
    "plt.ylim(0.85,1)\n",
    "plt.legend()\n",
    "plt.title(\"Fidelity Plot\")\n",
    "plt.savefig('label_noise_no_inter.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "84b4cf46-bd7e-4c96-af0b-d10298355ab9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diff:  0.008458333333333345\n",
      "Beta:  0.045454545454545456 \n",
      "\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  139 \tRate:  0.047375596455351054\n",
      "Finished Iteration:  0\n",
      "Beta:  0.09090909090909091 \n",
      "\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  274 \tRate:  0.09338786639400136\n",
      "Finished Iteration:  1\n",
      "Beta:  0.13636363636363635 \n",
      "\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  409 \tRate:  0.13940013633265166\n",
      "Finished Iteration:  2\n",
      "Beta:  0.18181818181818182 \n",
      "\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  488 \tRate:  0.1663258350374915\n",
      "Finished Iteration:  3\n",
      "Beta:  0.22727272727272727 \n",
      "\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  656 \tRate:  0.22358554873892297\n",
      "Finished Iteration:  4\n",
      "Beta:  0.2727272727272727 \n",
      "\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  809 \tRate:  0.27573278800272666\n",
      "Finished Iteration:  5\n",
      "Beta:  0.3181818181818182 \n",
      "\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  949 \tRate:  0.3234492160872529\n",
      "Finished Iteration:  6\n",
      "Beta:  0.36363636363636365 \n",
      "\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  1088 \tRate:  0.37082481254260397\n",
      "Finished Iteration:  7\n",
      "Beta:  0.4090909090909091 \n",
      "\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  1208 \tRate:  0.4117246080436264\n",
      "Finished Iteration:  8\n",
      "Beta:  0.45454545454545453 \n",
      "\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  1342 \tRate:  0.45739604635310155\n",
      "Finished Iteration:  9\n",
      "Finished Total Iteration:  1\n",
      "Diff:  0.010937499999999989\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  138 \tRate:  0.046875\n",
      "Finished Iteration:  0\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  273 \tRate:  0.09273097826086957\n",
      "Finished Iteration:  1\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  410 \tRate:  0.13926630434782608\n",
      "Finished Iteration:  2\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  554 \tRate:  0.18817934782608695\n",
      "Finished Iteration:  3\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  673 \tRate:  0.22860054347826086\n",
      "Finished Iteration:  4\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  765 \tRate:  0.25985054347826086\n",
      "Finished Iteration:  5\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  977 \tRate:  0.33186141304347827\n",
      "Finished Iteration:  6\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  1086 \tRate:  0.3688858695652174\n",
      "Finished Iteration:  7\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  1219 \tRate:  0.4140625\n",
      "Finished Iteration:  8\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  1334 \tRate:  0.453125\n",
      "Finished Iteration:  9\n",
      "Finished Total Iteration:  2\n",
      "Diff:  8.333333333332416e-05\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  140 \tRate:  0.04638833664678595\n",
      "Finished Iteration:  0\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  293 \tRate:  0.09708416169648774\n",
      "Finished Iteration:  1\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  418 \tRate:  0.13850231941683233\n",
      "Finished Iteration:  2\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  525 \tRate:  0.17395626242544732\n",
      "Finished Iteration:  3\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  649 \tRate:  0.21504307488402916\n",
      "Finished Iteration:  4\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  829 \tRate:  0.2746852220013254\n",
      "Finished Iteration:  5\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  971 \tRate:  0.32173624917163685\n",
      "Finished Iteration:  6\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  1094 \tRate:  0.36249171636845595\n",
      "Finished Iteration:  7\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  1269 \tRate:  0.42047713717693835\n",
      "Finished Iteration:  8\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  1364 \tRate:  0.45195493704440026\n",
      "Finished Iteration:  9\n",
      "Finished Total Iteration:  3\n",
      "Diff:  -0.00520833333333337\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  139 \tRate:  0.044225262488068726\n",
      "Finished Iteration:  0\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  283 \tRate:  0.0900413617562838\n",
      "Finished Iteration:  1\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  393 \tRate:  0.12503977091950366\n",
      "Finished Iteration:  2\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  534 \tRate:  0.1699013681196309\n",
      "Finished Iteration:  3\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  694 \tRate:  0.22080814508431434\n",
      "Finished Iteration:  4\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  861 \tRate:  0.2739420935412027\n",
      "Finished Iteration:  5\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  996 \tRate:  0.3168946866051543\n",
      "Finished Iteration:  6\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  1128 \tRate:  0.3588927776010181\n",
      "Finished Iteration:  7\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  1279 \tRate:  0.4069360483614381\n",
      "Finished Iteration:  8\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  1425 \tRate:  0.45338848234171175\n",
      "Finished Iteration:  9\n",
      "Finished Total Iteration:  4\n",
      "Diff:  -0.00585416666666666\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  149 \tRate:  0.04976619906479626\n",
      "Finished Iteration:  0\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  247 \tRate:  0.08249832999331998\n",
      "Finished Iteration:  1\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  389 \tRate:  0.12992651970607882\n",
      "Finished Iteration:  2\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  556 \tRate:  0.18570474281897129\n",
      "Finished Iteration:  3\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  668 \tRate:  0.22311289245156982\n",
      "Finished Iteration:  4\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  810 \tRate:  0.27054108216432865\n",
      "Finished Iteration:  5\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  966 \tRate:  0.3226452905811623\n",
      "Finished Iteration:  6\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  1115 \tRate:  0.3724114896459586\n",
      "Finished Iteration:  7\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  1247 \tRate:  0.416499665998664\n",
      "Finished Iteration:  8\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  1343 \tRate:  0.448563794255177\n",
      "Finished Iteration:  9\n",
      "Finished Total Iteration:  5\n",
      "Diff:  0.0014583333333333393\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  127 \tRate:  0.043182590955457324\n",
      "Finished Iteration:  0\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  276 \tRate:  0.09384563073784427\n",
      "Finished Iteration:  1\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  396 \tRate:  0.1346480788847331\n",
      "Finished Iteration:  2\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  576 \tRate:  0.1958517511050663\n",
      "Finished Iteration:  3\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  643 \tRate:  0.2186331179870792\n",
      "Finished Iteration:  4\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  796 \tRate:  0.27065623937436245\n",
      "Finished Iteration:  5\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  923 \tRate:  0.3138388303298198\n",
      "Finished Iteration:  6\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  1050 \tRate:  0.35702142128527714\n",
      "Finished Iteration:  7\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  1183 \tRate:  0.4022441346480789\n",
      "Finished Iteration:  8\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  1381 \tRate:  0.46956817409044543\n",
      "Finished Iteration:  9\n",
      "Finished Total Iteration:  6\n",
      "Diff:  -0.009749999999999981\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  123 \tRate:  0.040594059405940595\n",
      "Finished Iteration:  0\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  285 \tRate:  0.09405940594059406\n",
      "Finished Iteration:  1\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  405 \tRate:  0.13366336633663367\n",
      "Finished Iteration:  2\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  557 \tRate:  0.18382838283828382\n",
      "Finished Iteration:  3\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  702 \tRate:  0.2316831683168317\n",
      "Finished Iteration:  4\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  808 \tRate:  0.26666666666666666\n",
      "Finished Iteration:  5\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  953 \tRate:  0.31452145214521454\n",
      "Finished Iteration:  6\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  1112 \tRate:  0.366996699669967\n",
      "Finished Iteration:  7\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  1195 \tRate:  0.3943894389438944\n",
      "Finished Iteration:  8\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  1405 \tRate:  0.4636963696369637\n",
      "Finished Iteration:  9\n",
      "Finished Total Iteration:  7\n",
      "Diff:  0.0009583333333333388\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  135 \tRate:  0.04536290322580645\n",
      "Finished Iteration:  0\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  270 \tRate:  0.0907258064516129\n",
      "Finished Iteration:  1\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  385 \tRate:  0.12936827956989247\n",
      "Finished Iteration:  2\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  538 \tRate:  0.18077956989247312\n",
      "Finished Iteration:  3\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  631 \tRate:  0.21202956989247312\n",
      "Finished Iteration:  4\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  784 \tRate:  0.26344086021505375\n",
      "Finished Iteration:  5\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  904 \tRate:  0.30376344086021506\n",
      "Finished Iteration:  6\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  1109 \tRate:  0.37264784946236557\n",
      "Finished Iteration:  7\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  1206 \tRate:  0.40524193548387094\n",
      "Finished Iteration:  8\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  1339 \tRate:  0.44993279569892475\n",
      "Finished Iteration:  9\n",
      "Finished Total Iteration:  8\n",
      "Diff:  -0.005312499999999942\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  153 \tRate:  0.0504950495049505\n",
      "Finished Iteration:  0\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  278 \tRate:  0.09174917491749175\n",
      "Finished Iteration:  1\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  426 \tRate:  0.1405940594059406\n",
      "Finished Iteration:  2\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  560 \tRate:  0.1848184818481848\n",
      "Finished Iteration:  3\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  705 \tRate:  0.23267326732673269\n",
      "Finished Iteration:  4\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  826 \tRate:  0.2726072607260726\n",
      "Finished Iteration:  5\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  957 \tRate:  0.31584158415841584\n",
      "Finished Iteration:  6\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  1067 \tRate:  0.35214521452145214\n",
      "Finished Iteration:  7\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  1250 \tRate:  0.41254125412541254\n",
      "Finished Iteration:  8\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  1460 \tRate:  0.48184818481848185\n",
      "Finished Iteration:  9\n",
      "Finished Total Iteration:  9\n",
      "Diff:  -0.004708333333333314\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  118 \tRate:  0.03926788685524126\n",
      "Finished Iteration:  0\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  273 \tRate:  0.0908485856905158\n",
      "Finished Iteration:  1\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  425 \tRate:  0.14143094841930118\n",
      "Finished Iteration:  2\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  534 \tRate:  0.17770382695507486\n",
      "Finished Iteration:  3\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  708 \tRate:  0.23560732113144758\n",
      "Finished Iteration:  4\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  793 \tRate:  0.2638935108153078\n",
      "Finished Iteration:  5\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  904 \tRate:  0.30083194675540764\n",
      "Finished Iteration:  6\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  1085 \tRate:  0.3610648918469218\n",
      "Finished Iteration:  7\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  1243 \tRate:  0.4136439267886855\n",
      "Finished Iteration:  8\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  1391 \tRate:  0.46289517470881864\n",
      "Finished Iteration:  9\n",
      "Finished Total Iteration:  10\n"
     ]
    }
   ],
   "source": [
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "eta = 0.2\n",
    "n = 30000\n",
    "r = 0.2\n",
    "num_iters = 10\n",
    "\n",
    "bias_amts, mean_fidel_maj, mean_fidel_min, mean_fidel, y_err_fidel_maj, y_err_fidel_min, y_err_fidel, df = \\\n",
    "label_noise(r = r, n = n, apply_fairness=True,verbose=True, num_iters=num_iters, diff_base = False, inter = True)\n",
    "\n",
    "df.to_csv('disp_label_noise_inter.csv', index = True)\n",
    "df_res = pd.DataFrame(np.array([bias_amts, mean_fidel_maj, mean_fidel_min, mean_fidel, y_err_fidel_maj, y_err_fidel_min, y_err_fidel]).T, columns=['bias_amts', 'mean_fidel_maj', 'mean_fidel_min', 'mean_fidel', 'y_err_fidel_maj', 'y_err_fidel_min', 'y_err_fidel'])\n",
    "df_res.to_csv(f'fid_label_noise_inter.csv', index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569c31a0-a07e-478b-8738-6f068836bd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_amts = np.divide(list(range(1,11,1)),22)\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.errorbar(bias_amts, mean_fidel_maj25, yerr = y_err_fidel_maj25, label = 'Fidelity Majority', color = \"red\")\n",
    "plt.errorbar(bias_amts, mean_fidel_min25, yerr = y_err_fidel_min25, label = 'Fidelity Minority', color = \"green\")\n",
    "plt.xlabel(\"Eta Minority Value\")\n",
    "plt.ylabel(\"Fidelity\")\n",
    "plt.ylim(0.85,1)\n",
    "plt.legend()\n",
    "plt.title(\"Fidelity Plot\")\n",
    "plt.savefig('label_noise_inter.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0392aa7e-446f-406f-8bf1-44b8ad247ffb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diff:  0.2084166666666667\n",
      "Beta:  0.045454545454545456 \n",
      "\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  139 \tRate:  0.047375596455351054\n",
      "Finished Iteration:  0\n",
      "Beta:  0.09090909090909091 \n",
      "\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  274 \tRate:  0.09338786639400136\n",
      "Finished Iteration:  1\n",
      "Beta:  0.13636363636363635 \n",
      "\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  409 \tRate:  0.13940013633265166\n",
      "Finished Iteration:  2\n",
      "Beta:  0.18181818181818182 \n",
      "\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  488 \tRate:  0.1663258350374915\n",
      "Finished Iteration:  3\n",
      "Beta:  0.22727272727272727 \n",
      "\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  656 \tRate:  0.22358554873892297\n",
      "Finished Iteration:  4\n",
      "Beta:  0.2727272727272727 \n",
      "\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  809 \tRate:  0.27573278800272666\n",
      "Finished Iteration:  5\n",
      "Beta:  0.3181818181818182 \n",
      "\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  949 \tRate:  0.3234492160872529\n",
      "Finished Iteration:  6\n",
      "Beta:  0.36363636363636365 \n",
      "\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  1088 \tRate:  0.37082481254260397\n",
      "Finished Iteration:  7\n",
      "Beta:  0.4090909090909091 \n",
      "\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  1208 \tRate:  0.4117246080436264\n",
      "Finished Iteration:  8\n",
      "Beta:  0.45454545454545453 \n",
      "\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  1342 \tRate:  0.45739604635310155\n",
      "Finished Iteration:  9\n",
      "Finished Total Iteration:  1\n",
      "Diff:  0.21193750000000006\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  138 \tRate:  0.046875\n",
      "Finished Iteration:  0\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  273 \tRate:  0.09273097826086957\n",
      "Finished Iteration:  1\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  410 \tRate:  0.13926630434782608\n",
      "Finished Iteration:  2\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  554 \tRate:  0.18817934782608695\n",
      "Finished Iteration:  3\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  673 \tRate:  0.22860054347826086\n",
      "Finished Iteration:  4\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  765 \tRate:  0.25985054347826086\n",
      "Finished Iteration:  5\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  977 \tRate:  0.33186141304347827\n",
      "Finished Iteration:  6\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  1086 \tRate:  0.3688858695652174\n",
      "Finished Iteration:  7\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  1219 \tRate:  0.4140625\n",
      "Finished Iteration:  8\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  1334 \tRate:  0.453125\n",
      "Finished Iteration:  9\n",
      "Finished Total Iteration:  2\n",
      "Diff:  0.20008333333333328\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  140 \tRate:  0.04638833664678595\n",
      "Finished Iteration:  0\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  293 \tRate:  0.09708416169648774\n",
      "Finished Iteration:  1\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  418 \tRate:  0.13850231941683233\n",
      "Finished Iteration:  2\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  525 \tRate:  0.17395626242544732\n",
      "Finished Iteration:  3\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  649 \tRate:  0.21504307488402916\n",
      "Finished Iteration:  4\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  829 \tRate:  0.2746852220013254\n",
      "Finished Iteration:  5\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  971 \tRate:  0.32173624917163685\n",
      "Finished Iteration:  6\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  1094 \tRate:  0.36249171636845595\n",
      "Finished Iteration:  7\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  1269 \tRate:  0.42047713717693835\n",
      "Finished Iteration:  8\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  1364 \tRate:  0.45195493704440026\n",
      "Finished Iteration:  9\n",
      "Finished Total Iteration:  3\n",
      "Diff:  0.19435416666666672\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  139 \tRate:  0.044225262488068726\n",
      "Finished Iteration:  0\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  283 \tRate:  0.0900413617562838\n",
      "Finished Iteration:  1\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  393 \tRate:  0.12503977091950366\n",
      "Finished Iteration:  2\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  534 \tRate:  0.1699013681196309\n",
      "Finished Iteration:  3\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  694 \tRate:  0.22080814508431434\n",
      "Finished Iteration:  4\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  861 \tRate:  0.2739420935412027\n",
      "Finished Iteration:  5\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  996 \tRate:  0.3168946866051543\n",
      "Finished Iteration:  6\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  1128 \tRate:  0.3588927776010181\n",
      "Finished Iteration:  7\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  1279 \tRate:  0.4069360483614381\n",
      "Finished Iteration:  8\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  1425 \tRate:  0.45338848234171175\n",
      "Finished Iteration:  9\n",
      "Finished Total Iteration:  4\n",
      "Diff:  0.19643750000000004\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  149 \tRate:  0.04976619906479626\n",
      "Finished Iteration:  0\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  247 \tRate:  0.08249832999331998\n",
      "Finished Iteration:  1\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  389 \tRate:  0.12992651970607882\n",
      "Finished Iteration:  2\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  556 \tRate:  0.18570474281897129\n",
      "Finished Iteration:  3\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  668 \tRate:  0.22311289245156982\n",
      "Finished Iteration:  4\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  810 \tRate:  0.27054108216432865\n",
      "Finished Iteration:  5\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  966 \tRate:  0.3226452905811623\n",
      "Finished Iteration:  6\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  1115 \tRate:  0.3724114896459586\n",
      "Finished Iteration:  7\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  1247 \tRate:  0.416499665998664\n",
      "Finished Iteration:  8\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  1343 \tRate:  0.448563794255177\n",
      "Finished Iteration:  9\n",
      "Finished Total Iteration:  5\n",
      "Diff:  0.20560416666666664\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  127 \tRate:  0.043182590955457324\n",
      "Finished Iteration:  0\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  276 \tRate:  0.09384563073784427\n",
      "Finished Iteration:  1\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  396 \tRate:  0.1346480788847331\n",
      "Finished Iteration:  2\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  576 \tRate:  0.1958517511050663\n",
      "Finished Iteration:  3\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  643 \tRate:  0.2186331179870792\n",
      "Finished Iteration:  4\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  796 \tRate:  0.27065623937436245\n",
      "Finished Iteration:  5\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  923 \tRate:  0.3138388303298198\n",
      "Finished Iteration:  6\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  1050 \tRate:  0.35702142128527714\n",
      "Finished Iteration:  7\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  1183 \tRate:  0.4022441346480789\n",
      "Finished Iteration:  8\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  1381 \tRate:  0.46956817409044543\n",
      "Finished Iteration:  9\n",
      "Finished Total Iteration:  6\n",
      "Diff:  0.19258333333333333\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  123 \tRate:  0.040594059405940595\n",
      "Finished Iteration:  0\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  285 \tRate:  0.09405940594059406\n",
      "Finished Iteration:  1\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  405 \tRate:  0.13366336633663367\n",
      "Finished Iteration:  2\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  557 \tRate:  0.18382838283828382\n",
      "Finished Iteration:  3\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  702 \tRate:  0.2316831683168317\n",
      "Finished Iteration:  4\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  808 \tRate:  0.26666666666666666\n",
      "Finished Iteration:  5\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  953 \tRate:  0.31452145214521454\n",
      "Finished Iteration:  6\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  1112 \tRate:  0.366996699669967\n",
      "Finished Iteration:  7\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  1195 \tRate:  0.3943894389438944\n",
      "Finished Iteration:  8\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  1405 \tRate:  0.4636963696369637\n",
      "Finished Iteration:  9\n",
      "Finished Total Iteration:  7\n",
      "Diff:  0.1993333333333333\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  135 \tRate:  0.04536290322580645\n",
      "Finished Iteration:  0\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  270 \tRate:  0.0907258064516129\n",
      "Finished Iteration:  1\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  385 \tRate:  0.12936827956989247\n",
      "Finished Iteration:  2\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  538 \tRate:  0.18077956989247312\n",
      "Finished Iteration:  3\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  631 \tRate:  0.21202956989247312\n",
      "Finished Iteration:  4\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  784 \tRate:  0.26344086021505375\n",
      "Finished Iteration:  5\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  904 \tRate:  0.30376344086021506\n",
      "Finished Iteration:  6\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  1109 \tRate:  0.37264784946236557\n",
      "Finished Iteration:  7\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  1206 \tRate:  0.40524193548387094\n",
      "Finished Iteration:  8\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  1339 \tRate:  0.44993279569892475\n",
      "Finished Iteration:  9\n",
      "Finished Total Iteration:  8\n",
      "Diff:  0.19466666666666677\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  153 \tRate:  0.0504950495049505\n",
      "Finished Iteration:  0\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  278 \tRate:  0.09174917491749175\n",
      "Finished Iteration:  1\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  426 \tRate:  0.1405940594059406\n",
      "Finished Iteration:  2\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  560 \tRate:  0.1848184818481848\n",
      "Finished Iteration:  3\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  705 \tRate:  0.23267326732673269\n",
      "Finished Iteration:  4\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  826 \tRate:  0.2726072607260726\n",
      "Finished Iteration:  5\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  957 \tRate:  0.31584158415841584\n",
      "Finished Iteration:  6\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  1067 \tRate:  0.35214521452145214\n",
      "Finished Iteration:  7\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  1250 \tRate:  0.41254125412541254\n",
      "Finished Iteration:  8\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  1460 \tRate:  0.48184818481848185\n",
      "Finished Iteration:  9\n",
      "Finished Total Iteration:  9\n",
      "Diff:  0.19658333333333333\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  118 \tRate:  0.03926788685524126\n",
      "Finished Iteration:  0\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  273 \tRate:  0.0908485856905158\n",
      "Finished Iteration:  1\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  425 \tRate:  0.14143094841930118\n",
      "Finished Iteration:  2\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  534 \tRate:  0.17770382695507486\n",
      "Finished Iteration:  3\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  708 \tRate:  0.23560732113144758\n",
      "Finished Iteration:  4\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  793 \tRate:  0.2638935108153078\n",
      "Finished Iteration:  5\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  904 \tRate:  0.30083194675540764\n",
      "Finished Iteration:  6\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  1085 \tRate:  0.3610648918469218\n",
      "Finished Iteration:  7\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  1243 \tRate:  0.4136439267886855\n",
      "Finished Iteration:  8\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Bias Num Flipped:  1391 \tRate:  0.46289517470881864\n",
      "Finished Iteration:  9\n",
      "Finished Total Iteration:  10\n"
     ]
    }
   ],
   "source": [
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "eta = 0.2\n",
    "n = 30000\n",
    "r = 0.2\n",
    "num_iters = 10\n",
    "\n",
    "bias_amts, mean_fidel_maj, mean_fidel_min, mean_fidel, y_err_fidel_maj, y_err_fidel_min, y_err_fidel, df = \\\n",
    "label_noise(r = r, n = n, apply_fairness=True,verbose=True, num_iters=num_iters, diff_base = True, inter = True)\n",
    "\n",
    "df.to_csv('disp_label_noise_diff_base.csv', index = True)\n",
    "df_res = pd.DataFrame(np.array([bias_amts, mean_fidel_maj, mean_fidel_min, mean_fidel, y_err_fidel_maj, y_err_fidel_min, y_err_fidel]).T, columns=['bias_amts', 'mean_fidel_maj', 'mean_fidel_min', 'mean_fidel', 'y_err_fidel_maj', 'y_err_fidel_min', 'y_err_fidel'])\n",
    "df_res.to_csv(f'fid_label_noise_diff_base.csv', index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012893d8-819b-4e3e-b724-36bf6e91fdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_amts = np.divide(list(range(1,11,1)),22)\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.errorbar(bias_amts, mean_fidel_maj26, yerr = y_err_fidel_maj26, label = 'Fidelity Majority', color = \"red\")\n",
    "plt.errorbar(bias_amts, mean_fidel_min26, yerr = y_err_fidel_min26, label = 'Fidelity Minority', color = \"green\")\n",
    "plt.xlabel(\"Eta Minority Value\")\n",
    "plt.ylabel(\"Fidelity\")\n",
    "plt.ylim(0.85,1)\n",
    "plt.legend()\n",
    "plt.title(\"Fidelity Plot\")\n",
    "plt.savefig('label_noise_diff_base.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e5dce5-46b4-4d5c-a267-772edce57f04",
   "metadata": {},
   "source": [
    "## Feature Noise/Missingness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "de841c3f-270f-44f2-ae25-a1e8c7761b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inject_noise_num(df, feature, eta, eps = 1):\n",
    "    feats = df[feature].values\n",
    "    #print('Before:', df_synthetic['outcome'].value_counts())\n",
    "    num_flipped = 0\n",
    "    for i in range(len(feats)):\n",
    "        if random.uniform(0,1) <= eta:\n",
    "            feats[i] = 0\n",
    "            num_flipped += 1\n",
    "    df[feature] = feats\n",
    "    print('Num Flipped: ', num_flipped, \"\\tRate: \", num_flipped / len(df))\n",
    "    return df\n",
    "\n",
    "# measurement bias\n",
    "def get_biased_data(df, eps, inter):\n",
    "    df_majority = df[df['cat'] == 1]\n",
    "    df_minority = df[df['cat'] == 0]\n",
    "    \n",
    "    if inter:\n",
    "    \n",
    "        # unfavored group with negative label\n",
    "        df_minority_negative = df_minority[df_minority['outcome'] == 0.0]\n",
    "\n",
    "        # unfavored group with positive label (preferred)\n",
    "        df_minority_positive = df_minority[df_minority['outcome'] == 1.0]\n",
    "\n",
    "        # data frame without positively labeled examples from minority class\n",
    "        df_total = pd.concat([df_majority, df_minority_negative])\n",
    "\n",
    "        # under-sampling process\n",
    "        df_bias = inject_noise_num(df_minority_positive, 'num1', eps)\n",
    "\n",
    "        # combine undersampled and original favored class to create dataset\n",
    "        df_concat = pd.concat([df_total,df_bias])\n",
    "    \n",
    "    else:\n",
    "        # under-sampling process\n",
    "        df_bias = inject_noise_num(df_minority, 'num1', eps)\n",
    "        # combine undersampled and original favored class to create dataset\n",
    "        df_concat = pd.concat([df_majority,df_bias])\n",
    "    \n",
    "    return df_concat.sample(frac=1, random_state = 42) # permute data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8940a097-802a-41ea-a450-e3dd7bf94dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measurement(r, n, apply_fairness = True, verbose = False, num_iters = 10, inter = False, diff_base = False):\n",
    "    \n",
    "    total_fidel_maj = []\n",
    "    total_fidel_min = []\n",
    "    total_fidel = []\n",
    "    \n",
    "    total_disp_bias_train = []\n",
    "    total_disp_bo_train = []\n",
    "    total_disp_mitigated_train = []\n",
    "    \n",
    "    total_disp_bias_test = []\n",
    "    total_disp_bo_test = []\n",
    "    total_disp_mitigated_test = []\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "    \n",
    "        # 1 to 0 in increments of 0.1\n",
    "        bias_amts = np.divide(list(range(0,11,1)),10)\n",
    "        bias_amts[0] = 0.01 # doesn't work with 0\n",
    "\n",
    "        test_maj = []\n",
    "        test_min = []\n",
    "        total = []\n",
    "        \n",
    "        disp_bias_train = []\n",
    "        disp_bo_train = []\n",
    "        disp_mitigated_train = []\n",
    "        \n",
    "        disp_bias_test = []\n",
    "        disp_bo_test = []\n",
    "        disp_mitigated_test = []\n",
    "        \n",
    "        count = 0\n",
    "        \n",
    "        if diff_base: \n",
    "            outcome_continuous, df_synthetic = true_label_generation(r=r, eta=eta, n=2*n, maj_means =[0.7,0.7,0.7])\n",
    "        else: outcome_continuous, df_synthetic = true_label_generation(r=r, eta=eta, n=2*n)        \n",
    "        \n",
    "        threshold = 0.5\n",
    "        exact_bo_labels = np.where(outcome_continuous < threshold, 0, 1)\n",
    "        exact_bo_labels_train = np.array(exact_bo_labels[range(0,n)])\n",
    "        exact_bo_labels_test = np.array(exact_bo_labels[range(n,len(df_synthetic))])\n",
    "        \n",
    "        # split into train and test\n",
    "        df_train = df_synthetic.loc[range(0,n), :]\n",
    "        \n",
    "        df_train_transf, maj_list, min_list = transform(df_train, True, True)\n",
    "        \n",
    "        df_test = df_synthetic.loc[range(n, len(df_synthetic)),:]\n",
    "        df_test_transf, maj_list, min_list = transform(df_test, True, False)\n",
    "        \n",
    "        df_test_maj = df_test_transf.loc[maj_list]\n",
    "        df_test_min = df_test_transf.loc[min_list]\n",
    "\n",
    "        # format training data\n",
    "        X_true = df_train_transf.iloc[:, :-1].values\n",
    "        y_true = df_train_transf.iloc[:, -1].values\n",
    "        \n",
    "        # format test data\n",
    "        X_test = df_test_transf.iloc[:, :-1].values\n",
    "        X_test_maj = df_test_maj.iloc[:, :-1].values\n",
    "        X_test_min = df_test_min.iloc[:, :-1].values\n",
    "        y_test = df_test_transf.iloc[:, -1].values\n",
    "        y_test_maj = df_test_maj.iloc[:, -1].values\n",
    "        y_test_min = df_test_min.iloc[:, -1].values\n",
    "        \n",
    "        sens_attr_test = df_test['cat']\n",
    "        \n",
    "        for beta in bias_amts:\n",
    "            \n",
    "            if i == 0: print(\"Beta: \", beta, '\\n')\n",
    "\n",
    "            df_train_copy = df_train.copy()\n",
    "\n",
    "            df_majority = df_train_copy[df_train_copy['cat'] == 1]\n",
    "            df_minority = df_train_copy[df_train_copy['cat'] == 0]\n",
    "\n",
    "            # unfavored group with negative label\n",
    "            df_minority_negative = df_minority[df_minority['outcome'] == 0.0]\n",
    "\n",
    "            # unfavored group with positive label (preferred)\n",
    "            df_minority_positive = df_minority[df_minority['outcome'] == 1.0]\n",
    "\n",
    "            # data frame without positively labeled examples from minority class\n",
    "            df_total = pd.concat([df_majority, df_minority_negative])\n",
    "\n",
    "            # under-sampling process\n",
    "            df_undersampled = under(df_minority_positive, 1)\n",
    "\n",
    "            # combine undersampled and original favored class to create dataset\n",
    "            df_concat1 = pd.concat([df_total,df_undersampled])\n",
    "            \n",
    "            df_concat = get_biased_data(df_concat1, beta, inter)\n",
    "\n",
    "            # format data\n",
    "            X_bias_true, y_bias_true, df_sens = transform(df_concat)\n",
    "            \n",
    "            # model trained on biased data\n",
    "            classifier = LogisticRegression(solver = 'lbfgs', random_state = 42, penalty = 'none')\n",
    "            classifier_bias = classifier.fit(X_bias_true, y_bias_true)\n",
    "            \n",
    "            acc = accuracy_score(y_test,classifier_bias.predict(X_test))\n",
    "            #print(f'Biased classifier:')\n",
    "            #print(f'     Test accuracy = {acc}')\n",
    "            \n",
    "            m = classifier_bias\n",
    "            def classify(X): return m.predict(X)\n",
    "            error = ErrorRate()\n",
    "            error.load_data(X_bias_true, pd.Series(y_bias_true), sensitive_features=df_sens)\n",
    "            disparity = EqualizedOdds()\n",
    "            disparity.load_data(X_bias_true, pd.Series(y_bias_true), sensitive_features=df_sens)\n",
    "            #print(f'     Train error = {error.gamma(classify)[0]}')\n",
    "            #print(f'     Train disparity = {disparity.gamma(classify).max()}')\n",
    "            disp_bias_train += [disparity.gamma(classify).max()]\n",
    "\n",
    "            m = classifier_bias\n",
    "            def classify(X): return m.predict(X)\n",
    "            error = ErrorRate()\n",
    "            error.load_data(X_test, pd.Series(y_test), sensitive_features=sens_attr_test)\n",
    "            disparity = EqualizedOdds()\n",
    "            disparity.load_data(X_test, pd.Series(y_test), sensitive_features=sens_attr_test)\n",
    "            #print(f'     Test error = {error.gamma(classify)[0]}')\n",
    "            #print(f'     Test disparity = {disparity.gamma(classify).max()}')\n",
    "            disp_bias_test += [disparity.gamma(classify).max()]\n",
    "            \n",
    "            # Learned bayes optimal classifier\n",
    "            classifier = LogisticRegression(solver = 'lbfgs', random_state = 42, penalty = 'none')\n",
    "            classifier_b = classifier.fit(X_true, y_true)                \n",
    "            #classifier_b = clone(classifier).fit(X_true, y_true)\n",
    "            acc = accuracy_score(y_test,classifier_b.predict(X_test))\n",
    "            #print(f'Learned BO classifier:')\n",
    "            #print(f'     Test accuracy = {acc}')\n",
    "            \n",
    "            m = classifier_b\n",
    "            def classify(X): return m.predict(X)\n",
    "            error = ErrorRate()\n",
    "            error.load_data(X_bias_true, pd.Series(y_bias_true), sensitive_features=df_sens)\n",
    "            disparity = EqualizedOdds()\n",
    "            disparity.load_data(X_bias_true, pd.Series(y_bias_true), sensitive_features=df_sens)\n",
    "            #print(f'     Train error = {error.gamma(classify)[0]}')\n",
    "            #print(f'     Train disparity = {disparity.gamma(classify).max()}')\n",
    "            disp_bo_train += [disparity.gamma(classify).max()]\n",
    "\n",
    "            m = classifier_b\n",
    "            def classify(X): return m.predict(X)\n",
    "            error = ErrorRate()\n",
    "            error.load_data(X_test, pd.Series(y_test), sensitive_features=sens_attr_test)\n",
    "            disparity = EqualizedOdds()\n",
    "            disparity.load_data(X_test, pd.Series(y_test), sensitive_features=sens_attr_test)\n",
    "            #print(f'     Test error = {error.gamma(classify)[0]}')\n",
    "            #print(f'     Test disparity = {disparity.gamma(classify).max()}')\n",
    "            disp_bo_test += [disparity.gamma(classify).max()]\n",
    "            \n",
    "            \n",
    "            # Exact BO optimal classifier\n",
    "            #print(f'Exact BO classifier:')\n",
    "            acc = accuracy_score(y_test,exact_bo_labels_test)\n",
    "            #print(f'     Test accuracy = {acc}')\n",
    "            acc = accuracy_score(y_true,exact_bo_labels_train)\n",
    "            #print(f'     Train accuracy = {acc}')\n",
    "\n",
    "            if apply_fairness:\n",
    "                constraint = EqualizedOdds()\n",
    "                classifier = LogisticRegression(solver = 'lbfgs', random_state = 42, penalty = 'none', fit_intercept = False, max_iter = 200)   \n",
    "\n",
    "                classifier_mitigated_bias = GridSearch(estimator=classifier,\n",
    "                                                       constraints=constraint,\n",
    "                                                       selection_rule='tradeoff_optimization',\n",
    "                                                       constraint_weight=0.5,\n",
    "                                                       grid_size=10,\n",
    "                                                       grid_limit=2.0,\n",
    "                                                       grid_offset=None,\n",
    "                                                       grid=None,\n",
    "                                                       sample_weight_name='sample_weight')\n",
    "                                                       \n",
    "                classifier_mitigated_bias.fit(X_bias_true, y_bias_true, sensitive_features = df_sens)\n",
    "                \n",
    "                acc = accuracy_score(y_test,classifier_mitigated_bias.predict(X_test))\n",
    "                #print(f'Mitigated bias classifier:')\n",
    "                #print(f'     Test accuracy = {acc}')\n",
    "                \n",
    "                m = classifier_mitigated_bias\n",
    "                def classify(X): return m.predict(X)\n",
    "                error = ErrorRate()\n",
    "                error.load_data(X_bias_true, pd.Series(y_bias_true), sensitive_features=df_sens)\n",
    "                disparity = EqualizedOdds()\n",
    "                disparity.load_data(X_bias_true, pd.Series(y_bias_true), sensitive_features=df_sens)\n",
    "                #print(f'     Train error = {error.gamma(classify)[0]}')\n",
    "                #print(f'     Train disparity = {disparity.gamma(classify).max()}')\n",
    "                disp_mitigated_train += [disparity.gamma(classify).max()]\n",
    "                \n",
    "                m = classifier_mitigated_bias\n",
    "                def classify(X): return m.predict(X)\n",
    "                error = ErrorRate()\n",
    "                error.load_data(X_test, pd.Series(y_test), sensitive_features=sens_attr_test)\n",
    "                disparity = EqualizedOdds()\n",
    "                disparity.load_data(X_test, pd.Series(y_test), sensitive_features=sens_attr_test)\n",
    "                #print(f'     Test error = {error.gamma(classify)[0]}')\n",
    "                #print(f'     Test disparity = {disparity.gamma(classify).max()}')\n",
    "                disp_mitigated_test += [disparity.gamma(classify).max()]\n",
    "                \n",
    "                # Alternative fidelity of intervention model to no intervention model\n",
    "                alt_fid_train = accuracy_score(classifier_mitigated_bias.predict(X_bias_true),classifier_bias.predict(X_bias_true))\n",
    "                alt_fid_test = accuracy_score(classifier_mitigated_bias.predict(X_test),classifier_bias.predict(X_test))\n",
    "                #print(f'Alternative fidelity of intervention model to no intervention model: train = {alt_fid_train}, test = {alt_fid_test}')\n",
    "                \n",
    "\n",
    "            else:\n",
    "                classifier_mitigated_bias = clone(classifier_bias)\n",
    "                classifier_mitigated_bias.fit(X_bias_true, y_bias_true)\n",
    "                \n",
    "                # NOTE: disparities are the same as for classifier_bias\n",
    "\n",
    "                \n",
    "            # Fidelity in this step\n",
    "            fid = accuracy_score(classifier_mitigated_bias.predict(X_test),classifier_b.predict(X_test))\n",
    "            #print(f'Test set fidelity of learned BO classifier and mitigated_bias_classifier: {fid}')\n",
    "            fid = accuracy_score(classifier_mitigated_bias.predict(X_test),exact_bo_labels_test)\n",
    "            #print(f'Test set fidelity of exact BO classifier and mitigated biased classifier: {fid}')\n",
    "            fid = accuracy_score(classifier_bias.predict(X_test),classifier_b.predict(X_test))\n",
    "            #print(f'Test set fidelity of learned BO classifier and biased classifier: {fid}')\n",
    "            \n",
    "            # fidelity check\n",
    "            test_maj += [accuracy_score(classifier_mitigated_bias.predict(X_test_maj), classifier_b.predict(X_test_maj))]\n",
    "            test_min += [accuracy_score(classifier_mitigated_bias.predict(X_test_min), classifier_b.predict(X_test_min))]\n",
    "            total += [accuracy_score(classifier_mitigated_bias.predict(X_test), classifier_b.predict(X_test))]\n",
    "                \n",
    "            if verbose:\n",
    "                print(\"Finished Iteration: \", count)\n",
    "                count +=1\n",
    "        \n",
    "        #print(f'Fidelity test maj: {test_maj}')\n",
    "        #print(f'Fidelity test min: {test_maj}')\n",
    "        \n",
    "        total_fidel_maj.append(test_maj)\n",
    "        total_fidel_min.append(test_min)\n",
    "        total_fidel.append(total)\n",
    "        \n",
    "        total_disp_bias_train.append(disp_bias_train)\n",
    "        total_disp_bo_train.append(disp_bo_train)\n",
    "        total_disp_mitigated_train.append(disp_mitigated_train)\n",
    "\n",
    "        total_disp_bias_test.append(disp_bias_test)\n",
    "        total_disp_bo_test.append(disp_bo_test)\n",
    "        total_disp_mitigated_test.append(disp_mitigated_test)\n",
    "        \n",
    "        if verbose:\n",
    "                print(\"Finished Total Iteration: \", i+1)\n",
    "    \n",
    "    mean_fidel_maj = np.mean(total_fidel_maj, axis = 0)\n",
    "    mean_fidel_min = np.mean(total_fidel_min, axis = 0)\n",
    "    mean_fidel = np.mean(total_fidel, axis = 0)\n",
    "    \n",
    "    mean_disp_bias_train = np.mean(total_disp_bias_train, axis = 0)\n",
    "    mean_disp_bo_train = np.mean(total_disp_bo_train, axis = 0)\n",
    "    mean_disp_mitigated_train = np.mean(total_disp_mitigated_train, axis = 0)\n",
    "    \n",
    "    mean_disp_bias_test = np.mean(total_disp_bias_test, axis = 0)\n",
    "    mean_disp_bo_test = np.mean(total_disp_bo_test, axis = 0)\n",
    "    mean_disp_mitigated_test = np.mean(total_disp_mitigated_test, axis = 0)\n",
    "    \n",
    "    y_err_fidel_maj = np.std(total_fidel_maj, axis = 0)\n",
    "    y_err_fidel_min = np.std(total_fidel_min, axis = 0)\n",
    "    y_err_fidel = np.std(total_fidel, axis = 0)\n",
    "    \n",
    "    df = pd.DataFrame({\"Biased Train\" : mean_disp_bias_train,\n",
    "                       \"BO Train\" : mean_disp_bo_train,\n",
    "                       \"Mitigated Train\" : mean_disp_mitigated_train,\n",
    "                       \"Biased Test\" : mean_disp_bias_test,\n",
    "                       \"BO Test\" : mean_disp_bo_test,\n",
    "                       \"Mitigated Test\" : mean_disp_mitigated_test})\n",
    "   \n",
    "    return bias_amts, mean_fidel_maj, mean_fidel_min, mean_fidel, y_err_fidel_maj, y_err_fidel_min, y_err_fidel, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aeb42019-2897-46ba-82c2-dc1bf85608d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fc155cbf-30ac-4b4b-ae71-ecbb2ef26f6c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diff:  0.008458333333333345\n",
      "Beta:  0.01 \n",
      "\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  57 \tRate:  0.00962187711006077\n",
      "Finished Iteration:  0\n",
      "Beta:  0.1 \n",
      "\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  614 \tRate:  0.10364618501012829\n",
      "Finished Iteration:  1\n",
      "Beta:  0.2 \n",
      "\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  1155 \tRate:  0.1949696151249156\n",
      "Finished Iteration:  2\n",
      "Beta:  0.3 \n",
      "\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  1816 \tRate:  0.3065496286293045\n",
      "Finished Iteration:  3\n",
      "Beta:  0.4 \n",
      "\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  2375 \tRate:  0.40091154625253206\n",
      "Finished Iteration:  4\n",
      "Beta:  0.5 \n",
      "\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  2969 \tRate:  0.5011816340310601\n",
      "Finished Iteration:  5\n",
      "Beta:  0.6 \n",
      "\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  3592 \tRate:  0.6063470627954085\n",
      "Finished Iteration:  6\n",
      "Beta:  0.7 \n",
      "\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  4159 \tRate:  0.7020594193112761\n",
      "Finished Iteration:  7\n",
      "Beta:  0.8 \n",
      "\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  4732 \tRate:  0.7987846049966238\n",
      "Finished Iteration:  8\n",
      "Beta:  0.9 \n",
      "\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  5328 \tRate:  0.8993923024983119\n",
      "Finished Iteration:  9\n",
      "Beta:  1.0 \n",
      "\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  5924 \tRate:  1.0\n",
      "Finished Iteration:  10\n",
      "Finished Total Iteration:  1\n",
      "Diff:  0.010937499999999989\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  73 \tRate:  0.012182910547396529\n",
      "Finished Iteration:  0\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  638 \tRate:  0.10647530040053405\n",
      "Finished Iteration:  1\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  1193 \tRate:  0.19909879839786382\n",
      "Finished Iteration:  2\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  1760 \tRate:  0.2937249666221629\n",
      "Finished Iteration:  3\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  2387 \tRate:  0.3983644859813084\n",
      "Finished Iteration:  4\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  2946 \tRate:  0.4916555407209613\n",
      "Finished Iteration:  5\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  3626 \tRate:  0.6051401869158879\n",
      "Finished Iteration:  6\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  4179 \tRate:  0.697429906542056\n",
      "Finished Iteration:  7\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  4856 \tRate:  0.8104138851802403\n",
      "Finished Iteration:  8\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  5406 \tRate:  0.9022029372496663\n",
      "Finished Iteration:  9\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  5992 \tRate:  1.0\n",
      "Finished Iteration:  10\n",
      "Finished Total Iteration:  2\n",
      "Diff:  8.333333333332416e-05\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  62 \tRate:  0.01034540297013182\n",
      "Finished Iteration:  0\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  621 \tRate:  0.10362089103954614\n",
      "Finished Iteration:  1\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  1229 \tRate:  0.20507258468212916\n",
      "Finished Iteration:  2\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  1814 \tRate:  0.30268646754546974\n",
      "Finished Iteration:  3\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  2363 \tRate:  0.3942933422326047\n",
      "Finished Iteration:  4\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  2989 \tRate:  0.4987485399632905\n",
      "Finished Iteration:  5\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  3581 \tRate:  0.5975304521942266\n",
      "Finished Iteration:  6\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  4181 \tRate:  0.6976472551309861\n",
      "Finished Iteration:  7\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  4777 \tRate:  0.797096612714834\n",
      "Finished Iteration:  8\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  5408 \tRate:  0.9023861171366594\n",
      "Finished Iteration:  9\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  5993 \tRate:  1.0\n",
      "Finished Iteration:  10\n",
      "Finished Total Iteration:  3\n",
      "Diff:  -0.00520833333333337\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  55 \tRate:  0.009102946044356174\n",
      "Finished Iteration:  0\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  594 \tRate:  0.09831181727904667\n",
      "Finished Iteration:  1\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  1205 \tRate:  0.1994372724263489\n",
      "Finished Iteration:  2\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  1817 \tRate:  0.3007282356835485\n",
      "Finished Iteration:  3\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  2418 \tRate:  0.4001986097318769\n",
      "Finished Iteration:  4\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  3064 \tRate:  0.5071168487255876\n",
      "Finished Iteration:  5\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  3570 \tRate:  0.5908639523336644\n",
      "Finished Iteration:  6\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  4235 \tRate:  0.7009268454154254\n",
      "Finished Iteration:  7\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  4850 \tRate:  0.8027143330023171\n",
      "Finished Iteration:  8\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  5432 \tRate:  0.8990400529625951\n",
      "Finished Iteration:  9\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  6042 \tRate:  1.0\n",
      "Finished Iteration:  10\n",
      "Finished Total Iteration:  4\n",
      "Diff:  -0.00585416666666666\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  63 \tRate:  0.010524557300367525\n",
      "Finished Iteration:  0\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  581 \tRate:  0.0970598062145005\n",
      "Finished Iteration:  1\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  1201 \tRate:  0.20063481456732377\n",
      "Finished Iteration:  2\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  1780 \tRate:  0.29736050785165385\n",
      "Finished Iteration:  3\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  2383 \tRate:  0.39809555629802873\n",
      "Finished Iteration:  4\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  2949 \tRate:  0.49264951553625125\n",
      "Finished Iteration:  5\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  3546 \tRate:  0.5923822251921149\n",
      "Finished Iteration:  6\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  4170 \tRate:  0.696625459405279\n",
      "Finished Iteration:  7\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  4752 \tRate:  0.7938523220848647\n",
      "Finished Iteration:  8\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  5427 \tRate:  0.9066154360173738\n",
      "Finished Iteration:  9\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  5986 \tRate:  1.0\n",
      "Finished Iteration:  10\n",
      "Finished Total Iteration:  5\n",
      "Diff:  0.0014583333333333393\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  58 \tRate:  0.009636152184748297\n",
      "Finished Iteration:  0\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  614 \tRate:  0.10201030071440438\n",
      "Finished Iteration:  1\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  1241 \tRate:  0.20618042864263167\n",
      "Finished Iteration:  2\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  1810 \tRate:  0.30071440438611063\n",
      "Finished Iteration:  3\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  2414 \tRate:  0.4010632995514205\n",
      "Finished Iteration:  4\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  3051 \tRate:  0.5068948330287423\n",
      "Finished Iteration:  5\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  3678 \tRate:  0.6110649609569696\n",
      "Finished Iteration:  6\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  4145 \tRate:  0.6886526000996843\n",
      "Finished Iteration:  7\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  4846 \tRate:  0.8051171290912111\n",
      "Finished Iteration:  8\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  5441 \tRate:  0.9039707592623359\n",
      "Finished Iteration:  9\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  6019 \tRate:  1.0\n",
      "Finished Iteration:  10\n",
      "Finished Total Iteration:  6\n",
      "Diff:  -0.009749999999999981\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  57 \tRate:  0.00949050949050949\n",
      "Finished Iteration:  0\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  601 \tRate:  0.10006660006660006\n",
      "Finished Iteration:  1\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  1204 \tRate:  0.20046620046620048\n",
      "Finished Iteration:  2\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  1820 \tRate:  0.30303030303030304\n",
      "Finished Iteration:  3\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  2407 \tRate:  0.40076590076590074\n",
      "Finished Iteration:  4\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  2938 \tRate:  0.48917748917748916\n",
      "Finished Iteration:  5\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  3617 \tRate:  0.6022311022311022\n",
      "Finished Iteration:  6\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  4197 \tRate:  0.6988011988011988\n",
      "Finished Iteration:  7\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  4845 \tRate:  0.8066933066933067\n",
      "Finished Iteration:  8\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  5428 \tRate:  0.9037629037629038\n",
      "Finished Iteration:  9\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  6006 \tRate:  1.0\n",
      "Finished Iteration:  10\n",
      "Finished Total Iteration:  7\n",
      "Diff:  0.0009583333333333388\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  72 \tRate:  0.012074459164849907\n",
      "Finished Iteration:  0\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  558 \tRate:  0.09357705852758678\n",
      "Finished Iteration:  1\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  1241 \tRate:  0.2081167197719269\n",
      "Finished Iteration:  2\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  1809 \tRate:  0.30337078651685395\n",
      "Finished Iteration:  3\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  2407 \tRate:  0.40365587791380175\n",
      "Finished Iteration:  4\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  2960 \tRate:  0.4963944323327184\n",
      "Finished Iteration:  5\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  3613 \tRate:  0.6059030689250378\n",
      "Finished Iteration:  6\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  4160 \tRate:  0.6976354184135503\n",
      "Finished Iteration:  7\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  4814 \tRate:  0.8073117558276035\n",
      "Finished Iteration:  8\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  5344 \tRate:  0.8961931913466376\n",
      "Finished Iteration:  9\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  5963 \tRate:  1.0\n",
      "Finished Iteration:  10\n",
      "Finished Total Iteration:  8\n",
      "Diff:  -0.005312499999999942\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  39 \tRate:  0.006467661691542288\n",
      "Finished Iteration:  0\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  574 \tRate:  0.09519071310116087\n",
      "Finished Iteration:  1\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  1220 \tRate:  0.2023217247097844\n",
      "Finished Iteration:  2\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  1844 \tRate:  0.30580431177446105\n",
      "Finished Iteration:  3\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  2377 \tRate:  0.39419568822553896\n",
      "Finished Iteration:  4\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  2971 \tRate:  0.49270315091210615\n",
      "Finished Iteration:  5\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  3641 \tRate:  0.6038142620232172\n",
      "Finished Iteration:  6\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  4206 \tRate:  0.6975124378109453\n",
      "Finished Iteration:  7\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  4814 \tRate:  0.7983416252072969\n",
      "Finished Iteration:  8\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  5458 \tRate:  0.9051409618573798\n",
      "Finished Iteration:  9\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  6030 \tRate:  1.0\n",
      "Finished Iteration:  10\n",
      "Finished Total Iteration:  9\n",
      "Diff:  -0.004708333333333314\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  50 \tRate:  0.008315316813570598\n",
      "Finished Iteration:  0\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  603 \tRate:  0.10028272077166141\n",
      "Finished Iteration:  1\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  1164 \tRate:  0.1935805754199235\n",
      "Finished Iteration:  2\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  1791 \tRate:  0.2978546482620988\n",
      "Finished Iteration:  3\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  2388 \tRate:  0.3971395310161317\n",
      "Finished Iteration:  4\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  2973 \tRate:  0.4944287377349077\n",
      "Finished Iteration:  5\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  3600 \tRate:  0.598702810577083\n",
      "Finished Iteration:  6\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  4210 \tRate:  0.7001496757026443\n",
      "Finished Iteration:  7\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  4857 \tRate:  0.8077498752702478\n",
      "Finished Iteration:  8\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  5378 \tRate:  0.8943954764676534\n",
      "Finished Iteration:  9\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  6013 \tRate:  1.0\n",
      "Finished Iteration:  10\n",
      "Finished Total Iteration:  10\n"
     ]
    }
   ],
   "source": [
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "eta = 0.2\n",
    "n = 30000\n",
    "r = 0.2\n",
    "num_iters = 10\n",
    "\n",
    "bias_amts, mean_fidel_maj, mean_fidel_min, mean_fidel, y_err_fidel_maj, y_err_fidel_min, y_err_fidel, df = \\\n",
    "measurement(r = r, n = n, apply_fairness=True,verbose=True, num_iters=num_iters, diff_base = False, inter = False)\n",
    "\n",
    "df.to_csv('disp_featmissing_no_inter.csv', index = True)\n",
    "df_res = pd.DataFrame(np.array([bias_amts, mean_fidel_maj, mean_fidel_min, mean_fidel, y_err_fidel_maj, y_err_fidel_min, y_err_fidel]).T, columns=['bias_amts', 'mean_fidel_maj', 'mean_fidel_min', 'mean_fidel', 'y_err_fidel_maj', 'y_err_fidel_min', 'y_err_fidel'])\n",
    "df_res.to_csv(f'fid_featmissing_no_inter.csv', index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070066c7-979c-4c74-b16e-3e57edfa99bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bias_amts = np.divide(list(range(0,11,1)),10)\n",
    "bias_amts[0] = 0.01 # doesn't work with 0\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.errorbar(bias_amts, mean_fidel_maj27, yerr = y_err_fidel_maj27, label = 'Fidelity Majority', color = \"red\")\n",
    "plt.errorbar(bias_amts, mean_fidel_min27, yerr = y_err_fidel_min27, label = 'Fidelity Minority', color = \"green\")\n",
    "plt.xlabel(\"Eta Minority Value\")\n",
    "plt.ylabel(\"Fidelity\")\n",
    "plt.ylim(0.9, 1)\n",
    "plt.legend()\n",
    "plt.title(\"Fidelity Plot\")\n",
    "plt.savefig('featmissing_no_inter.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5c795647-4294-4493-b70f-d13db39da17a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diff:  0.008458333333333345\n",
      "Beta:  0.01 \n",
      "\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  27 \tRate:  0.009202453987730062\n",
      "Finished Iteration:  0\n",
      "Beta:  0.1 \n",
      "\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  299 \tRate:  0.10190865712338106\n",
      "Finished Iteration:  1\n",
      "Beta:  0.2 \n",
      "\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  616 \tRate:  0.20995228357191548\n",
      "Finished Iteration:  2\n",
      "Beta:  0.3 \n",
      "\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  816 \tRate:  0.278118609406953\n",
      "Finished Iteration:  3\n",
      "Beta:  0.4 \n",
      "\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  1186 \tRate:  0.4042263122017723\n",
      "Finished Iteration:  4\n",
      "Beta:  0.5 \n",
      "\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  1442 \tRate:  0.49147920927062033\n",
      "Finished Iteration:  5\n",
      "Beta:  0.6 \n",
      "\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  1781 \tRate:  0.6070211315610089\n",
      "Finished Iteration:  6\n",
      "Beta:  0.7 \n",
      "\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  2025 \tRate:  0.6901840490797546\n",
      "Finished Iteration:  7\n",
      "Beta:  0.8 \n",
      "\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  2347 \tRate:  0.799931833674165\n",
      "Finished Iteration:  8\n",
      "Beta:  0.9 \n",
      "\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  2656 \tRate:  0.9052488070892979\n",
      "Finished Iteration:  9\n",
      "Beta:  1.0 \n",
      "\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  2934 \tRate:  1.0\n",
      "Finished Iteration:  10\n",
      "Finished Total Iteration:  1\n",
      "Diff:  0.010937499999999989\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  27 \tRate:  0.00914014895057549\n",
      "Finished Iteration:  0\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  285 \tRate:  0.0964793500338524\n",
      "Finished Iteration:  1\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  599 \tRate:  0.20277589708869329\n",
      "Finished Iteration:  2\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  880 \tRate:  0.2979011509817197\n",
      "Finished Iteration:  3\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  1146 \tRate:  0.3879485443466486\n",
      "Finished Iteration:  4\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  1506 \tRate:  0.5098171970209885\n",
      "Finished Iteration:  5\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  1787 \tRate:  0.6049424509140149\n",
      "Finished Iteration:  6\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  2061 \tRate:  0.6976980365605958\n",
      "Finished Iteration:  7\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  2369 \tRate:  0.8019634394041977\n",
      "Finished Iteration:  8\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  2643 \tRate:  0.8947190250507786\n",
      "Finished Iteration:  9\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  2954 \tRate:  1.0\n",
      "Finished Iteration:  10\n",
      "Finished Total Iteration:  2\n",
      "Diff:  8.333333333332416e-05\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  32 \tRate:  0.01068804275217101\n",
      "Finished Iteration:  0\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  301 \tRate:  0.10053440213760854\n",
      "Finished Iteration:  1\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  546 \tRate:  0.18236472945891782\n",
      "Finished Iteration:  2\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  891 \tRate:  0.29759519038076154\n",
      "Finished Iteration:  3\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  1225 \tRate:  0.4091516366065464\n",
      "Finished Iteration:  4\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  1543 \tRate:  0.5153640614562458\n",
      "Finished Iteration:  5\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  1822 \tRate:  0.6085504342017368\n",
      "Finished Iteration:  6\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  2073 \tRate:  0.6923847695390781\n",
      "Finished Iteration:  7\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  2401 \tRate:  0.801937207748831\n",
      "Finished Iteration:  8\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  2671 \tRate:  0.8921175684702739\n",
      "Finished Iteration:  9\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  2994 \tRate:  1.0\n",
      "Finished Iteration:  10\n",
      "Finished Total Iteration:  3\n",
      "Diff:  -0.00520833333333337\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  20 \tRate:  0.006578947368421052\n",
      "Finished Iteration:  0\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  328 \tRate:  0.10789473684210527\n",
      "Finished Iteration:  1\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  601 \tRate:  0.19769736842105262\n",
      "Finished Iteration:  2\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  919 \tRate:  0.3023026315789474\n",
      "Finished Iteration:  3\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  1224 \tRate:  0.4026315789473684\n",
      "Finished Iteration:  4\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  1480 \tRate:  0.4868421052631579\n",
      "Finished Iteration:  5\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  1806 \tRate:  0.594078947368421\n",
      "Finished Iteration:  6\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  2101 \tRate:  0.6911184210526315\n",
      "Finished Iteration:  7\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  2465 \tRate:  0.8108552631578947\n",
      "Finished Iteration:  8\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  2715 \tRate:  0.8930921052631579\n",
      "Finished Iteration:  9\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  3040 \tRate:  1.0\n",
      "Finished Iteration:  10\n",
      "Finished Total Iteration:  4\n",
      "Diff:  -0.00585416666666666\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  26 \tRate:  0.00850507033038927\n",
      "Finished Iteration:  0\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  290 \tRate:  0.09486424599280341\n",
      "Finished Iteration:  1\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  615 \tRate:  0.2011776251226693\n",
      "Finished Iteration:  2\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  968 \tRate:  0.31665031076218514\n",
      "Finished Iteration:  3\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  1205 \tRate:  0.3941772980045797\n",
      "Finished Iteration:  4\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  1517 \tRate:  0.4962381419692509\n",
      "Finished Iteration:  5\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  1832 \tRate:  0.5992803402028132\n",
      "Finished Iteration:  6\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  2174 \tRate:  0.7111547268563951\n",
      "Finished Iteration:  7\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  2453 \tRate:  0.8024206738632647\n",
      "Finished Iteration:  8\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  2740 \tRate:  0.8963035655871769\n",
      "Finished Iteration:  9\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  3057 \tRate:  1.0\n",
      "Finished Iteration:  10\n",
      "Finished Total Iteration:  5\n",
      "Diff:  0.0014583333333333393\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  35 \tRate:  0.01172136637642331\n",
      "Finished Iteration:  0\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  313 \tRate:  0.10482250502344273\n",
      "Finished Iteration:  1\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  556 \tRate:  0.18620227729403885\n",
      "Finished Iteration:  2\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  874 \tRate:  0.2926992632283992\n",
      "Finished Iteration:  3\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  1235 \tRate:  0.41359678499665103\n",
      "Finished Iteration:  4\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  1470 \tRate:  0.49229738780977894\n",
      "Finished Iteration:  5\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  1796 \tRate:  0.6014735432016075\n",
      "Finished Iteration:  6\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  2086 \tRate:  0.6985934360348292\n",
      "Finished Iteration:  7\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  2395 \tRate:  0.8020763563295379\n",
      "Finished Iteration:  8\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  2675 \tRate:  0.8958472873409243\n",
      "Finished Iteration:  9\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  2986 \tRate:  1.0\n",
      "Finished Iteration:  10\n",
      "Finished Total Iteration:  6\n",
      "Diff:  -0.009749999999999981\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  36 \tRate:  0.012028065486134313\n",
      "Finished Iteration:  0\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  298 \tRate:  0.09956565319077848\n",
      "Finished Iteration:  1\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  572 \tRate:  0.19111259605746742\n",
      "Finished Iteration:  2\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  931 \tRate:  0.31105913798864016\n",
      "Finished Iteration:  3\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  1200 \tRate:  0.4009355162044771\n",
      "Finished Iteration:  4\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  1513 \tRate:  0.5055128633478115\n",
      "Finished Iteration:  5\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  1782 \tRate:  0.5953892415636485\n",
      "Finished Iteration:  6\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  2117 \tRate:  0.7073170731707317\n",
      "Finished Iteration:  7\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  2410 \tRate:  0.8052121617106582\n",
      "Finished Iteration:  8\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  2706 \tRate:  0.9041095890410958\n",
      "Finished Iteration:  9\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  2993 \tRate:  1.0\n",
      "Finished Iteration:  10\n",
      "Finished Total Iteration:  7\n",
      "Diff:  0.0009583333333333388\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  27 \tRate:  0.009168081494057725\n",
      "Finished Iteration:  0\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  294 \tRate:  0.099830220713073\n",
      "Finished Iteration:  1\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  568 \tRate:  0.19286926994906622\n",
      "Finished Iteration:  2\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  858 \tRate:  0.29134125636672326\n",
      "Finished Iteration:  3\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  1146 \tRate:  0.3891341256366723\n",
      "Finished Iteration:  4\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  1423 \tRate:  0.4831918505942275\n",
      "Finished Iteration:  5\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  1788 \tRate:  0.6071307300509338\n",
      "Finished Iteration:  6\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  2078 \tRate:  0.7056027164685909\n",
      "Finished Iteration:  7\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  2348 \tRate:  0.7972835314091681\n",
      "Finished Iteration:  8\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  2670 \tRate:  0.9066213921901528\n",
      "Finished Iteration:  9\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  2945 \tRate:  1.0\n",
      "Finished Iteration:  10\n",
      "Finished Total Iteration:  8\n",
      "Diff:  -0.005312499999999942\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  31 \tRate:  0.01028533510285335\n",
      "Finished Iteration:  0\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  300 \tRate:  0.099535500995355\n",
      "Finished Iteration:  1\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  606 \tRate:  0.20106171201061712\n",
      "Finished Iteration:  2\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  909 \tRate:  0.30159256801592566\n",
      "Finished Iteration:  3\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  1220 \tRate:  0.404777704047777\n",
      "Finished Iteration:  4\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  1527 \tRate:  0.506635700066357\n",
      "Finished Iteration:  5\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  1792 \tRate:  0.5945587259455872\n",
      "Finished Iteration:  6\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  2117 \tRate:  0.7023888520238886\n",
      "Finished Iteration:  7\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  2393 \tRate:  0.7939615129396151\n",
      "Finished Iteration:  8\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  2713 \tRate:  0.9001327140013271\n",
      "Finished Iteration:  9\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  3014 \tRate:  1.0\n",
      "Finished Iteration:  10\n",
      "Finished Total Iteration:  9\n",
      "Diff:  -0.004708333333333314\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  26 \tRate:  0.008686936184430337\n",
      "Finished Iteration:  0\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  296 \tRate:  0.09889742733043769\n",
      "Finished Iteration:  1\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  596 \tRate:  0.19913130638155696\n",
      "Finished Iteration:  2\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  906 \tRate:  0.3027063147343802\n",
      "Finished Iteration:  3\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  1213 \tRate:  0.4052789842966923\n",
      "Finished Iteration:  4\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  1514 \tRate:  0.5058469762779819\n",
      "Finished Iteration:  5\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  1827 \tRate:  0.6104243234213164\n",
      "Finished Iteration:  6\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  2082 \tRate:  0.6956231206147678\n",
      "Finished Iteration:  7\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  2396 \tRate:  0.8005345806882727\n",
      "Finished Iteration:  8\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  2697 \tRate:  0.9011025726695623\n",
      "Finished Iteration:  9\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  2993 \tRate:  1.0\n",
      "Finished Iteration:  10\n",
      "Finished Total Iteration:  10\n"
     ]
    }
   ],
   "source": [
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "eta = 0.2\n",
    "n = 30000\n",
    "r = 0.2\n",
    "num_iters = 10\n",
    "\n",
    "bias_amts, mean_fidel_maj, mean_fidel_min, mean_fidel, y_err_fidel_maj, y_err_fidel_min, y_err_fidel, df = \\\n",
    "measurement(r = r, n = n, apply_fairness=True,verbose=True, num_iters=num_iters, diff_base = False, inter = True)\n",
    "\n",
    "df.to_csv('disp_featmissing_inter.csv', index = True)\n",
    "df_res = pd.DataFrame(np.array([bias_amts, mean_fidel_maj, mean_fidel_min, mean_fidel, y_err_fidel_maj, y_err_fidel_min, y_err_fidel]).T, columns=['bias_amts', 'mean_fidel_maj', 'mean_fidel_min', 'mean_fidel', 'y_err_fidel_maj', 'y_err_fidel_min', 'y_err_fidel'])\n",
    "df_res.to_csv(f'fid_featmissing_inter.csv', index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f42439-f076-4ab9-a435-da96d4e5405f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_amts = np.divide(list(range(0,11,1)),10)\n",
    "bias_amts[0] = 0.01 # doesn't work with 0\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.errorbar(bias_amts, mean_fidel_maj28, yerr = y_err_fidel_maj28, label = 'Fidelity Majority', color = \"red\")\n",
    "plt.errorbar(bias_amts, mean_fidel_min28, yerr = y_err_fidel_min28, label = 'Fidelity Minority', color = \"green\")\n",
    "plt.xlabel(\"Eta Minority Value\")\n",
    "plt.ylabel(\"Fidelity\")\n",
    "plt.ylim(0.9, 1)\n",
    "plt.legend()\n",
    "plt.title(\"Fidelity Plot\")\n",
    "plt.savefig('featmissing_inter.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "48847578-9444-4f60-a690-6b1277c34644",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diff:  0.2084166666666667\n",
      "Beta:  0.01 \n",
      "\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  27 \tRate:  0.009202453987730062\n",
      "Finished Iteration:  0\n",
      "Beta:  0.1 \n",
      "\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  299 \tRate:  0.10190865712338106\n",
      "Finished Iteration:  1\n",
      "Beta:  0.2 \n",
      "\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  616 \tRate:  0.20995228357191548\n",
      "Finished Iteration:  2\n",
      "Beta:  0.3 \n",
      "\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  816 \tRate:  0.278118609406953\n",
      "Finished Iteration:  3\n",
      "Beta:  0.4 \n",
      "\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  1186 \tRate:  0.4042263122017723\n",
      "Finished Iteration:  4\n",
      "Beta:  0.5 \n",
      "\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  1442 \tRate:  0.49147920927062033\n",
      "Finished Iteration:  5\n",
      "Beta:  0.6 \n",
      "\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  1781 \tRate:  0.6070211315610089\n",
      "Finished Iteration:  6\n",
      "Beta:  0.7 \n",
      "\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  2025 \tRate:  0.6901840490797546\n",
      "Finished Iteration:  7\n",
      "Beta:  0.8 \n",
      "\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  2347 \tRate:  0.799931833674165\n",
      "Finished Iteration:  8\n",
      "Beta:  0.9 \n",
      "\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  2656 \tRate:  0.9052488070892979\n",
      "Finished Iteration:  9\n",
      "Beta:  1.0 \n",
      "\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  2934 \tRate:  1.0\n",
      "Finished Iteration:  10\n",
      "Finished Total Iteration:  1\n",
      "Diff:  0.21193750000000006\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  27 \tRate:  0.00914014895057549\n",
      "Finished Iteration:  0\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  285 \tRate:  0.0964793500338524\n",
      "Finished Iteration:  1\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  599 \tRate:  0.20277589708869329\n",
      "Finished Iteration:  2\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  880 \tRate:  0.2979011509817197\n",
      "Finished Iteration:  3\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  1146 \tRate:  0.3879485443466486\n",
      "Finished Iteration:  4\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  1506 \tRate:  0.5098171970209885\n",
      "Finished Iteration:  5\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  1787 \tRate:  0.6049424509140149\n",
      "Finished Iteration:  6\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  2061 \tRate:  0.6976980365605958\n",
      "Finished Iteration:  7\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  2369 \tRate:  0.8019634394041977\n",
      "Finished Iteration:  8\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  2643 \tRate:  0.8947190250507786\n",
      "Finished Iteration:  9\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  2954 \tRate:  1.0\n",
      "Finished Iteration:  10\n",
      "Finished Total Iteration:  2\n",
      "Diff:  0.20008333333333328\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  32 \tRate:  0.01068804275217101\n",
      "Finished Iteration:  0\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  301 \tRate:  0.10053440213760854\n",
      "Finished Iteration:  1\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  546 \tRate:  0.18236472945891782\n",
      "Finished Iteration:  2\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  891 \tRate:  0.29759519038076154\n",
      "Finished Iteration:  3\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  1225 \tRate:  0.4091516366065464\n",
      "Finished Iteration:  4\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  1543 \tRate:  0.5153640614562458\n",
      "Finished Iteration:  5\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  1822 \tRate:  0.6085504342017368\n",
      "Finished Iteration:  6\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  2073 \tRate:  0.6923847695390781\n",
      "Finished Iteration:  7\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  2401 \tRate:  0.801937207748831\n",
      "Finished Iteration:  8\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  2671 \tRate:  0.8921175684702739\n",
      "Finished Iteration:  9\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  2994 \tRate:  1.0\n",
      "Finished Iteration:  10\n",
      "Finished Total Iteration:  3\n",
      "Diff:  0.19435416666666672\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  20 \tRate:  0.006578947368421052\n",
      "Finished Iteration:  0\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  328 \tRate:  0.10789473684210527\n",
      "Finished Iteration:  1\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  601 \tRate:  0.19769736842105262\n",
      "Finished Iteration:  2\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  919 \tRate:  0.3023026315789474\n",
      "Finished Iteration:  3\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  1224 \tRate:  0.4026315789473684\n",
      "Finished Iteration:  4\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  1480 \tRate:  0.4868421052631579\n",
      "Finished Iteration:  5\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  1806 \tRate:  0.594078947368421\n",
      "Finished Iteration:  6\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  2101 \tRate:  0.6911184210526315\n",
      "Finished Iteration:  7\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  2465 \tRate:  0.8108552631578947\n",
      "Finished Iteration:  8\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  2715 \tRate:  0.8930921052631579\n",
      "Finished Iteration:  9\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  3040 \tRate:  1.0\n",
      "Finished Iteration:  10\n",
      "Finished Total Iteration:  4\n",
      "Diff:  0.19643750000000004\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  26 \tRate:  0.00850507033038927\n",
      "Finished Iteration:  0\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  290 \tRate:  0.09486424599280341\n",
      "Finished Iteration:  1\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  615 \tRate:  0.2011776251226693\n",
      "Finished Iteration:  2\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  968 \tRate:  0.31665031076218514\n",
      "Finished Iteration:  3\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  1205 \tRate:  0.3941772980045797\n",
      "Finished Iteration:  4\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  1517 \tRate:  0.4962381419692509\n",
      "Finished Iteration:  5\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  1832 \tRate:  0.5992803402028132\n",
      "Finished Iteration:  6\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  2174 \tRate:  0.7111547268563951\n",
      "Finished Iteration:  7\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  2453 \tRate:  0.8024206738632647\n",
      "Finished Iteration:  8\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  2740 \tRate:  0.8963035655871769\n",
      "Finished Iteration:  9\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  3057 \tRate:  1.0\n",
      "Finished Iteration:  10\n",
      "Finished Total Iteration:  5\n",
      "Diff:  0.20560416666666664\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  35 \tRate:  0.01172136637642331\n",
      "Finished Iteration:  0\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  313 \tRate:  0.10482250502344273\n",
      "Finished Iteration:  1\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  556 \tRate:  0.18620227729403885\n",
      "Finished Iteration:  2\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  874 \tRate:  0.2926992632283992\n",
      "Finished Iteration:  3\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  1235 \tRate:  0.41359678499665103\n",
      "Finished Iteration:  4\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  1470 \tRate:  0.49229738780977894\n",
      "Finished Iteration:  5\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  1796 \tRate:  0.6014735432016075\n",
      "Finished Iteration:  6\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  2086 \tRate:  0.6985934360348292\n",
      "Finished Iteration:  7\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  2395 \tRate:  0.8020763563295379\n",
      "Finished Iteration:  8\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  2675 \tRate:  0.8958472873409243\n",
      "Finished Iteration:  9\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  2986 \tRate:  1.0\n",
      "Finished Iteration:  10\n",
      "Finished Total Iteration:  6\n",
      "Diff:  0.19258333333333333\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  36 \tRate:  0.012028065486134313\n",
      "Finished Iteration:  0\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  298 \tRate:  0.09956565319077848\n",
      "Finished Iteration:  1\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  572 \tRate:  0.19111259605746742\n",
      "Finished Iteration:  2\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  931 \tRate:  0.31105913798864016\n",
      "Finished Iteration:  3\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  1200 \tRate:  0.4009355162044771\n",
      "Finished Iteration:  4\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  1513 \tRate:  0.5055128633478115\n",
      "Finished Iteration:  5\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  1782 \tRate:  0.5953892415636485\n",
      "Finished Iteration:  6\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  2117 \tRate:  0.7073170731707317\n",
      "Finished Iteration:  7\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  2410 \tRate:  0.8052121617106582\n",
      "Finished Iteration:  8\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  2706 \tRate:  0.9041095890410958\n",
      "Finished Iteration:  9\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  2993 \tRate:  1.0\n",
      "Finished Iteration:  10\n",
      "Finished Total Iteration:  7\n",
      "Diff:  0.1993333333333333\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  27 \tRate:  0.009168081494057725\n",
      "Finished Iteration:  0\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  294 \tRate:  0.099830220713073\n",
      "Finished Iteration:  1\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  568 \tRate:  0.19286926994906622\n",
      "Finished Iteration:  2\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  858 \tRate:  0.29134125636672326\n",
      "Finished Iteration:  3\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  1146 \tRate:  0.3891341256366723\n",
      "Finished Iteration:  4\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  1423 \tRate:  0.4831918505942275\n",
      "Finished Iteration:  5\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  1788 \tRate:  0.6071307300509338\n",
      "Finished Iteration:  6\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  2078 \tRate:  0.7056027164685909\n",
      "Finished Iteration:  7\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  2348 \tRate:  0.7972835314091681\n",
      "Finished Iteration:  8\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  2670 \tRate:  0.9066213921901528\n",
      "Finished Iteration:  9\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  2945 \tRate:  1.0\n",
      "Finished Iteration:  10\n",
      "Finished Total Iteration:  8\n",
      "Diff:  0.19466666666666677\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  31 \tRate:  0.01028533510285335\n",
      "Finished Iteration:  0\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  300 \tRate:  0.099535500995355\n",
      "Finished Iteration:  1\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  606 \tRate:  0.20106171201061712\n",
      "Finished Iteration:  2\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  909 \tRate:  0.30159256801592566\n",
      "Finished Iteration:  3\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  1220 \tRate:  0.404777704047777\n",
      "Finished Iteration:  4\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  1527 \tRate:  0.506635700066357\n",
      "Finished Iteration:  5\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  1792 \tRate:  0.5945587259455872\n",
      "Finished Iteration:  6\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  2117 \tRate:  0.7023888520238886\n",
      "Finished Iteration:  7\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  2393 \tRate:  0.7939615129396151\n",
      "Finished Iteration:  8\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  2713 \tRate:  0.9001327140013271\n",
      "Finished Iteration:  9\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  3014 \tRate:  1.0\n",
      "Finished Iteration:  10\n",
      "Finished Total Iteration:  9\n",
      "Diff:  0.19658333333333333\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  26 \tRate:  0.008686936184430337\n",
      "Finished Iteration:  0\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  296 \tRate:  0.09889742733043769\n",
      "Finished Iteration:  1\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  596 \tRate:  0.19913130638155696\n",
      "Finished Iteration:  2\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  906 \tRate:  0.3027063147343802\n",
      "Finished Iteration:  3\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  1213 \tRate:  0.4052789842966923\n",
      "Finished Iteration:  4\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  1514 \tRate:  0.5058469762779819\n",
      "Finished Iteration:  5\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  1827 \tRate:  0.6104243234213164\n",
      "Finished Iteration:  6\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  2082 \tRate:  0.6956231206147678\n",
      "Finished Iteration:  7\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  2396 \tRate:  0.8005345806882727\n",
      "Finished Iteration:  8\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  2697 \tRate:  0.9011025726695623\n",
      "Finished Iteration:  9\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Num Flipped:  2993 \tRate:  1.0\n",
      "Finished Iteration:  10\n",
      "Finished Total Iteration:  10\n"
     ]
    }
   ],
   "source": [
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "eta = 0.2\n",
    "n = 30000\n",
    "r = 0.2\n",
    "num_iters = 10\n",
    "\n",
    "bias_amts, mean_fidel_maj, mean_fidel_min, mean_fidel, y_err_fidel_maj, y_err_fidel_min, y_err_fidel, df = \\\n",
    "measurement(r = r, n = n, apply_fairness=True,verbose=True, num_iters=num_iters, diff_base = True, inter = True)\n",
    "\n",
    "df.to_csv('disp_featmissing_diff_base.csv', index = True)\n",
    "df_res = pd.DataFrame(np.array([bias_amts, mean_fidel_maj, mean_fidel_min, mean_fidel, y_err_fidel_maj, y_err_fidel_min, y_err_fidel]).T, columns=['bias_amts', 'mean_fidel_maj', 'mean_fidel_min', 'mean_fidel', 'y_err_fidel_maj', 'y_err_fidel_min', 'y_err_fidel'])\n",
    "df_res.to_csv(f'fid_featmissing_diff_base.csv', index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcc46d7-ec05-45fe-9ec3-c45729984da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_amts = np.divide(list(range(0,11,1)),10)\n",
    "bias_amts[0] = 0.01 # doesn't work with 0\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.errorbar(bias_amts, mean_fidel_maj29, yerr = y_err_fidel_maj29, label = 'Fidelity Majority', color = \"red\")\n",
    "plt.errorbar(bias_amts, mean_fidel_min29, yerr = y_err_fidel_min29, label = 'Fidelity Minority', color = \"green\")\n",
    "plt.xlabel(\"Eta Minority Value\")\n",
    "plt.ylabel(\"Fidelity\")\n",
    "plt.ylim(0.9, 1)\n",
    "plt.legend()\n",
    "plt.title(\"Fidelity Plot\")\n",
    "plt.savefig('featmissing_diff_base.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0ba98002-8e7d-47fa-ba13-5eb67b749a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff_base_rates(r, n, verbose = False, num_iters = 10):\n",
    "    \n",
    "    beta = 1\n",
    "    bias_amts = [-0.5, -0.4, -0.3, -0.2, -0.1, 0, .1, .2, .3, .4, .5]\n",
    "    \n",
    "    total_fidel_maj = []\n",
    "    total_fidel_min = []\n",
    "    total_fidel = []\n",
    "    \n",
    "    total_disp_bias_train = []\n",
    "    total_disp_bo_train = []\n",
    "    total_disp_mitigated_train = []\n",
    "    \n",
    "    total_disp_bias_test = []\n",
    "    total_disp_bo_test = []\n",
    "    total_disp_mitigated_test = []\n",
    "    \n",
    "    vals = [-4, -1.7, -1.1, -0.7, -0.3, 0, 0.35, 0.7, 1.15, 1.75, 4]\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "\n",
    "        test_maj = []\n",
    "        test_min = []\n",
    "        total = []\n",
    "        \n",
    "        disp_bias_train = []\n",
    "        disp_bo_train = []\n",
    "        disp_mitigated_train = []\n",
    "        \n",
    "        disp_bias_test = []\n",
    "        disp_bo_test = []\n",
    "        disp_mitigated_test = []\n",
    "        \n",
    "        count = 0\n",
    "        \n",
    "        for diff in vals:\n",
    "            \n",
    "            arr = np.ones(3)*diff\n",
    "            outcome_continuous, df_synthetic = true_label_generation(r=r, eta=eta, n=2*n, maj_means = arr)\n",
    "\n",
    "            threshold = 0.5\n",
    "            exact_bo_labels = np.where(outcome_continuous < threshold, 0, 1)\n",
    "            exact_bo_labels_train = np.array(exact_bo_labels[range(0,n)])\n",
    "            exact_bo_labels_test = np.array(exact_bo_labels[range(n,len(df_synthetic))])\n",
    "\n",
    "            # split into train and test\n",
    "            df_train = df_synthetic.loc[range(0,n), :]\n",
    "\n",
    "            df_train_transf, maj_list, min_list = transform(df_train, True, True)\n",
    "\n",
    "            df_test = df_synthetic.loc[range(n, len(df_synthetic)),:]\n",
    "            df_test_transf, maj_list, min_list = transform(df_test, True, False)\n",
    "\n",
    "            df_test_maj = df_test_transf.loc[maj_list]\n",
    "            df_test_min = df_test_transf.loc[min_list]\n",
    "\n",
    "            # format training data\n",
    "            X_true = df_train_transf.iloc[:, :-1].values\n",
    "            y_true = df_train_transf.iloc[:, -1].values\n",
    "\n",
    "            # format test data\n",
    "            X_test = df_test_transf.iloc[:, :-1].values\n",
    "            X_test_maj = df_test_maj.iloc[:, :-1].values\n",
    "            X_test_min = df_test_min.iloc[:, :-1].values\n",
    "            y_test = df_test_transf.iloc[:, -1].values\n",
    "            y_test_maj = df_test_maj.iloc[:, -1].values\n",
    "            y_test_min = df_test_min.iloc[:, -1].values\n",
    "\n",
    "            sens_attr_test = df_test['cat']\n",
    "\n",
    "            df_train_copy = df_train.copy()\n",
    "\n",
    "            df_majority = df_train_copy[df_train_copy['cat'] == 1]\n",
    "            df_minority = df_train_copy[df_train_copy['cat'] == 0]\n",
    "\n",
    "            # unfavored group with negative label\n",
    "            df_minority_negative = df_minority[df_minority['outcome'] == 0.0]\n",
    "\n",
    "            # unfavored group with positive label (preferred)\n",
    "            df_minority_positive = df_minority[df_minority['outcome'] == 1.0]\n",
    "\n",
    "            # data frame without positively labeled examples from minority class\n",
    "            df_total = pd.concat([df_majority, df_minority_negative])\n",
    "\n",
    "            # under-sampling process\n",
    "            df_undersampled = under(df_minority_positive, beta)\n",
    "\n",
    "            # combine undersampled and original favored class to create dataset\n",
    "            df_concat = pd.concat([df_total,df_undersampled]).sample(frac=1, random_state = 42)\n",
    "\n",
    "            # format data\n",
    "            X_bias_true, y_bias_true, df_sens = transform(df_concat)\n",
    "\n",
    "            # model trained on biased data\n",
    "            classifier = LogisticRegression(solver = 'lbfgs', random_state = 42, penalty = 'none')\n",
    "            classifier_bias = classifier.fit(X_bias_true, y_bias_true)\n",
    "\n",
    "            acc = accuracy_score(y_test,classifier_bias.predict(X_test))\n",
    "            #print(f'Biased classifier:')\n",
    "            #print(f'     Test accuracy = {acc}')\n",
    "\n",
    "            m = classifier_bias\n",
    "            def classify(X): return m.predict(X)\n",
    "            error = ErrorRate()\n",
    "            error.load_data(X_bias_true, pd.Series(y_bias_true), sensitive_features=df_sens)\n",
    "            disparity = EqualizedOdds()\n",
    "            disparity.load_data(X_bias_true, pd.Series(y_bias_true), sensitive_features=df_sens)\n",
    "            #print(f'     Train error = {error.gamma(classify)[0]}')\n",
    "            #print(f'     Train disparity = {disparity.gamma(classify).max()}')\n",
    "            disp_bias_train += [disparity.gamma(classify).max()]\n",
    "\n",
    "            m = classifier_bias\n",
    "            def classify(X): return m.predict(X)\n",
    "            error = ErrorRate()\n",
    "            error.load_data(X_test, pd.Series(y_test), sensitive_features=sens_attr_test)\n",
    "            disparity = EqualizedOdds()\n",
    "            disparity.load_data(X_test, pd.Series(y_test), sensitive_features=sens_attr_test)\n",
    "            #print(f'     Test error = {error.gamma(classify)[0]}')\n",
    "            #print(f'     Test disparity = {disparity.gamma(classify).max()}')\n",
    "            disp_bias_test += [disparity.gamma(classify).max()]\n",
    "\n",
    "            # Learned bayes optimal classifier\n",
    "            classifier = LogisticRegression(solver = 'lbfgs', random_state = 42, penalty = 'none')\n",
    "            classifier_b = classifier.fit(X_true, y_true)                \n",
    "            #classifier_b = clone(classifier).fit(X_true, y_true)\n",
    "            acc = accuracy_score(y_test,classifier_b.predict(X_test))\n",
    "            #print(f'Learned BO classifier:')\n",
    "            #print(f'     Test accuracy = {acc}')\n",
    "\n",
    "            m = classifier_b\n",
    "            def classify(X): return m.predict(X)\n",
    "            error = ErrorRate()\n",
    "            error.load_data(X_bias_true, pd.Series(y_bias_true), sensitive_features=df_sens)\n",
    "            disparity = EqualizedOdds()\n",
    "            disparity.load_data(X_bias_true, pd.Series(y_bias_true), sensitive_features=df_sens)\n",
    "            #print(f'     Train error = {error.gamma(classify)[0]}')\n",
    "            #print(f'     Train disparity = {disparity.gamma(classify).max()}')\n",
    "            disp_bo_train += [disparity.gamma(classify).max()]\n",
    "\n",
    "            m = classifier_b\n",
    "            def classify(X): return m.predict(X)\n",
    "            error = ErrorRate()\n",
    "            error.load_data(X_test, pd.Series(y_test), sensitive_features=sens_attr_test)\n",
    "            disparity = EqualizedOdds()\n",
    "            disparity.load_data(X_test, pd.Series(y_test), sensitive_features=sens_attr_test)\n",
    "            #print(f'     Test error = {error.gamma(classify)[0]}')\n",
    "            #print(f'     Test disparity = {disparity.gamma(classify).max()}')\n",
    "            disp_bo_test += [disparity.gamma(classify).max()]\n",
    "\n",
    "\n",
    "            # Exact BO optimal classifier\n",
    "            #print(f'Exact BO classifier:')\n",
    "            acc = accuracy_score(y_test,exact_bo_labels_test)\n",
    "            #print(f'     Test accuracy = {acc}')\n",
    "            acc = accuracy_score(y_true,exact_bo_labels_train)\n",
    "            #print(f'     Train accuracy = {acc}')\n",
    "\n",
    "            constraint = EqualizedOdds()\n",
    "            classifier = LogisticRegression(solver = 'lbfgs', random_state = 42, penalty = 'none', fit_intercept = False, max_iter = 200)   \n",
    "\n",
    "            classifier_mitigated_bias = GridSearch(estimator=classifier,\n",
    "                                                   constraints=constraint,\n",
    "                                                   selection_rule='tradeoff_optimization',\n",
    "                                                   constraint_weight=0.5,\n",
    "                                                   grid_size=10,\n",
    "                                                   grid_limit=2.0,\n",
    "                                                   grid_offset=None,\n",
    "                                                   grid=None,\n",
    "                                                   sample_weight_name='sample_weight')\n",
    "\n",
    "            classifier_mitigated_bias.fit(X_bias_true, y_bias_true, sensitive_features = df_sens)\n",
    "\n",
    "            acc = accuracy_score(y_test,classifier_mitigated_bias.predict(X_test))\n",
    "            #print(f'Mitigated bias classifier:')\n",
    "            #print(f'     Test accuracy = {acc}')\n",
    "\n",
    "            m = classifier_mitigated_bias\n",
    "            def classify(X): return m.predict(X)\n",
    "            error = ErrorRate()\n",
    "            error.load_data(X_bias_true, pd.Series(y_bias_true), sensitive_features=df_sens)\n",
    "            disparity = EqualizedOdds()\n",
    "            disparity.load_data(X_bias_true, pd.Series(y_bias_true), sensitive_features=df_sens)\n",
    "            #print(f'     Train error = {error.gamma(classify)[0]}')\n",
    "            #print(f'     Train disparity = {disparity.gamma(classify).max()}')\n",
    "            disp_mitigated_train += [disparity.gamma(classify).max()]\n",
    "\n",
    "            m = classifier_mitigated_bias\n",
    "            def classify(X): return m.predict(X)\n",
    "            error = ErrorRate()\n",
    "            error.load_data(X_test, pd.Series(y_test), sensitive_features=sens_attr_test)\n",
    "            disparity = EqualizedOdds()\n",
    "            disparity.load_data(X_test, pd.Series(y_test), sensitive_features=sens_attr_test)\n",
    "            #print(f'     Test error = {error.gamma(classify)[0]}')\n",
    "            #print(f'     Test disparity = {disparity.gamma(classify).max()}')\n",
    "            disp_mitigated_test += [disparity.gamma(classify).max()]\n",
    "\n",
    "            # Fidelity in this step\n",
    "            fid = accuracy_score(classifier_mitigated_bias.predict(X_test),classifier_b.predict(X_test))\n",
    "            #print(f'Test set fidelity of learned BO classifier and mitigated_bias_classifier: {fid}')\n",
    "            fid = accuracy_score(classifier_mitigated_bias.predict(X_test),exact_bo_labels_test)\n",
    "            #print(f'Test set fidelity of exact BO classifier and mitigated biased classifier: {fid}')\n",
    "            fid = accuracy_score(classifier_bias.predict(X_test),classifier_b.predict(X_test))\n",
    "            #print(f'Test set fidelity of learned BO classifier and biased classifier: {fid}')\n",
    "\n",
    "            # fidelity check\n",
    "            test_maj += [accuracy_score(classifier_mitigated_bias.predict(X_test_maj), classifier_b.predict(X_test_maj))]\n",
    "            test_min += [accuracy_score(classifier_mitigated_bias.predict(X_test_min), classifier_b.predict(X_test_min))]\n",
    "            total += [accuracy_score(classifier_mitigated_bias.predict(X_test), classifier_b.predict(X_test))]\n",
    "\n",
    "            if verbose:\n",
    "                print(\"Finished Iteration: \", count)\n",
    "                count +=1\n",
    "        \n",
    "        #print(f'Fidelity test maj: {test_maj}')\n",
    "        #print(f'Fidelity test min: {test_maj}')\n",
    "        \n",
    "        total_fidel_maj.append(test_maj)\n",
    "        total_fidel_min.append(test_min)\n",
    "        total_fidel.append(total)\n",
    "        \n",
    "        total_disp_bias_train.append(disp_bias_train)\n",
    "        total_disp_bo_train.append(disp_bo_train)\n",
    "        total_disp_mitigated_train.append(disp_mitigated_train)\n",
    "\n",
    "        total_disp_bias_test.append(disp_bias_test)\n",
    "        total_disp_bo_test.append(disp_bo_test)\n",
    "        total_disp_mitigated_test.append(disp_mitigated_test)\n",
    "        \n",
    "        if verbose:\n",
    "                print(\"Finished Total Iteration: \", i+1)\n",
    "    \n",
    "    mean_fidel_maj = np.mean(total_fidel_maj, axis = 0)\n",
    "    mean_fidel_min = np.mean(total_fidel_min, axis = 0)\n",
    "    mean_fidel = np.mean(total_fidel, axis = 0)\n",
    "    \n",
    "    mean_disp_bias_train = np.mean(total_disp_bias_train, axis = 0)\n",
    "    mean_disp_bo_train = np.mean(total_disp_bo_train, axis = 0)\n",
    "    mean_disp_mitigated_train = np.mean(total_disp_mitigated_train, axis = 0)\n",
    "    \n",
    "    mean_disp_bias_test = np.mean(total_disp_bias_test, axis = 0)\n",
    "    mean_disp_bo_test = np.mean(total_disp_bo_test, axis = 0)\n",
    "    mean_disp_mitigated_test = np.mean(total_disp_mitigated_test, axis = 0)\n",
    "    \n",
    "    y_err_fidel_maj = np.std(total_fidel_maj, axis = 0)\n",
    "    y_err_fidel_min = np.std(total_fidel_min, axis = 0)\n",
    "    y_err_fidel = np.std(total_fidel, axis = 0)\n",
    "    \n",
    "    df = pd.DataFrame({\"Biased Train\" : mean_disp_bias_train,\n",
    "                       \"BO Train\" : mean_disp_bo_train,\n",
    "                       \"Mitigated Train\" : mean_disp_mitigated_train,\n",
    "                       \"Biased Test\" : mean_disp_bias_test,\n",
    "                       \"BO Test\" : mean_disp_bo_test,\n",
    "                       \"Mitigated Test\" : mean_disp_mitigated_test})\n",
    "   \n",
    "    return bias_amts, mean_fidel_maj, mean_fidel_min, mean_fidel, y_err_fidel_maj, y_err_fidel_min, y_err_fidel, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "eae1cef5-b091-4681-bf4f-515c6e17de94",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diff:  -0.49295833333333333\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  0\n",
      "Diff:  -0.39225\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  1\n",
      "Diff:  -0.3004583333333334\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  2\n",
      "Diff:  -0.20485416666666667\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  3\n",
      "Diff:  -0.0952708333333333\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  4\n",
      "Diff:  0.0014583333333333393\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  5\n",
      "Diff:  0.09785416666666669\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  6\n",
      "Diff:  0.1993333333333333\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  7\n",
      "Diff:  0.30108333333333337\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  8\n",
      "Diff:  0.40564583333333337\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  9\n",
      "Diff:  0.49364583333333334\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  10\n",
      "Finished Total Iteration:  1\n",
      "Diff:  -0.5011875\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  0\n",
      "Diff:  -0.39989583333333334\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  1\n",
      "Diff:  -0.2945\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  2\n",
      "Diff:  -0.20052083333333331\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  3\n",
      "Diff:  -0.08320833333333333\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  4\n",
      "Diff:  -6.250000000007638e-05\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  5\n",
      "Diff:  0.10693750000000002\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  6\n",
      "Diff:  0.2016458333333334\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  7\n",
      "Diff:  0.30237499999999995\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  8\n",
      "Diff:  0.40366666666666673\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  9\n",
      "Diff:  0.4910416666666666\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  10\n",
      "Finished Total Iteration:  2\n",
      "Diff:  -0.49604166666666666\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  0\n",
      "Diff:  -0.3936458333333333\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  1\n",
      "Diff:  -0.2955208333333333\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  2\n",
      "Diff:  -0.19808333333333333\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  3\n",
      "Diff:  -0.09604166666666669\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  4\n",
      "Diff:  -0.0023749999999999605\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  5\n",
      "Diff:  0.10722916666666665\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  6\n",
      "Diff:  0.20631250000000007\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  7\n",
      "Diff:  0.29808333333333337\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  8\n",
      "Diff:  0.4070625\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  9\n",
      "Diff:  0.499875\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  10\n",
      "Finished Total Iteration:  3\n",
      "Diff:  -0.49877083333333333\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  0\n",
      "Diff:  -0.40568750000000003\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  1\n",
      "Diff:  -0.2929375\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  2\n",
      "Diff:  -0.19279166666666664\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  3\n",
      "Diff:  -0.08947916666666667\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  4\n",
      "Diff:  -0.004270833333333335\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  5\n",
      "Diff:  0.09143749999999995\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  6\n",
      "Diff:  0.20558333333333328\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  7\n",
      "Diff:  0.2971666666666667\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  8\n",
      "Diff:  0.3970208333333334\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  9\n",
      "Diff:  0.4968541666666667\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  10\n",
      "Finished Total Iteration:  4\n",
      "Diff:  -0.5056041666666666\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  0\n",
      "Diff:  -0.4130625\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  1\n",
      "Diff:  -0.2976249999999999\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  2\n",
      "Diff:  -0.19756250000000003\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  3\n",
      "Diff:  -0.09297916666666672\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  4\n",
      "Diff:  -0.008145833333333297\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  5\n",
      "Diff:  0.09893750000000001\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  6\n",
      "Diff:  0.20785416666666662\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  7\n",
      "Diff:  0.30966666666666665\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  8\n",
      "Diff:  0.40724999999999995\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  9\n",
      "Diff:  0.5047083333333333\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  10\n",
      "Finished Total Iteration:  5\n",
      "Diff:  -0.49945833333333334\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  0\n",
      "Diff:  -0.39849999999999997\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  1\n",
      "Diff:  -0.30014583333333333\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  2\n",
      "Diff:  -0.20235416666666667\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  3\n",
      "Diff:  -0.0866041666666667\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  4\n",
      "Diff:  0.00447916666666659\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  5\n",
      "Diff:  0.10110416666666666\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  6\n",
      "Diff:  0.20070833333333338\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  7\n",
      "Diff:  0.3075833333333333\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  8\n",
      "Diff:  0.4086458333333334\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  9\n",
      "Diff:  0.4940416666666667\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  10\n",
      "Finished Total Iteration:  6\n",
      "Diff:  -0.4988125\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  0\n",
      "Diff:  -0.4016875\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  1\n",
      "Diff:  -0.2967708333333333\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  2\n",
      "Diff:  -0.20664583333333336\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  3\n",
      "Diff:  -0.0977291666666667\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  4\n",
      "Diff:  -0.005708333333333426\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  5\n",
      "Diff:  0.0996041666666666\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  6\n",
      "Diff:  0.20477083333333335\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  7\n",
      "Diff:  0.3035416666666667\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  8\n",
      "Diff:  0.4059166666666667\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  9\n",
      "Diff:  0.5010208333333332\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  10\n",
      "Finished Total Iteration:  7\n",
      "Diff:  -0.4943958333333333\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  0\n",
      "Diff:  -0.40004166666666663\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  1\n",
      "Diff:  -0.29289583333333336\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  2\n",
      "Diff:  -0.2021041666666667\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  3\n",
      "Diff:  -0.0904166666666667\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  4\n",
      "Diff:  -0.0001666666666666483\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  5\n",
      "Diff:  0.10758333333333331\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  6\n",
      "Diff:  0.19595833333333335\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  7\n",
      "Diff:  0.3059375\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  8\n",
      "Diff:  0.40879166666666666\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  9\n",
      "Diff:  0.5065208333333333\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  10\n",
      "Finished Total Iteration:  8\n",
      "Diff:  -0.498\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  0\n",
      "Diff:  -0.4101041666666666\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  1\n",
      "Diff:  -0.2992708333333334\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  2\n",
      "Diff:  -0.20306250000000003\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  3\n",
      "Diff:  -0.09083333333333338\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  4\n",
      "Diff:  -0.005770833333333392\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  5\n",
      "Diff:  0.10043749999999996\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  6\n",
      "Diff:  0.19914583333333324\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  7\n",
      "Diff:  0.3083541666666667\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  8\n",
      "Diff:  0.4094791666666666\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  9\n",
      "Diff:  0.49635416666666665\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  10\n",
      "Finished Total Iteration:  9\n",
      "Diff:  -0.5053958333333333\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  0\n",
      "Diff:  -0.39687500000000003\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  1\n",
      "Diff:  -0.29708333333333325\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  2\n",
      "Diff:  -0.20504166666666662\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  3\n",
      "Diff:  -0.0935833333333333\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  4\n",
      "Diff:  0.001437500000000036\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  5\n",
      "Diff:  0.1067083333333333\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  6\n",
      "Diff:  0.19806249999999992\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  7\n",
      "Diff:  0.306375\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  8\n",
      "Diff:  0.413\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  9\n",
      "Diff:  0.4944791666666667\n",
      "Total Deleted:  0 \t % Deleted:  0.0\n",
      "Finished Iteration:  10\n",
      "Finished Total Iteration:  10\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "eta = 0.2\n",
    "n = 30000\n",
    "r = 0.2\n",
    "num_iters = 10\n",
    "\n",
    "bias_amts, mean_fidel_maj, mean_fidel_min, mean_fidel, y_err_fidel_maj, y_err_fidel_min, y_err_fidel, df = \\\n",
    "diff_base_rates(r = r, n = n, verbose=True, num_iters=num_iters)\n",
    "\n",
    "df.to_csv('disp_diff_base_rates.csv', index = True)\n",
    "df_res = pd.DataFrame(np.array([bias_amts, mean_fidel_maj, mean_fidel_min, mean_fidel, y_err_fidel_maj, y_err_fidel_min, y_err_fidel]).T, columns=['bias_amts', 'mean_fidel_maj', 'mean_fidel_min', 'mean_fidel', 'y_err_fidel_maj', 'y_err_fidel_min', 'y_err_fidel'])\n",
    "df_res.to_csv(f'fid_diff_base_rates.csv', index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e06588-c835-4980-bfb0-1699dc39482c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_amts = [-0.5, -0.4, -0.3, -0.2, -0.1, 0, .1, .2, .3, .4, .5]\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.errorbar(bias_amts, mean_fidel_maj, yerr = y_err_fidel_maj, label = 'Fidelity Majority', color = \"red\")\n",
    "plt.errorbar(bias_amts, mean_fidel_min, yerr = y_err_fidel_min, label = 'Fidelity Minority', color = \"green\")\n",
    "plt.xlabel(\"Difference in Proportion of Positive Labels (Majority - Minority)\")\n",
    "plt.ylabel(\"Fidelity\")\n",
    "#plt.xlim(-0.4, 0.5)\n",
    "#plt.ylim(0.9, 1)\n",
    "#plt.xlim(1.05, -0.05)\n",
    "plt.legend()\n",
    "plt.title(\"Fidelity Plot\")\n",
    "plt.savefig('diff_base_rates.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b8827a-9501-4419-aaee-b6892cff64c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8476c66e-26a9-4782-9ca4-92d0f7985345",
   "metadata": {},
   "source": [
    "## Code for Final Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bbe19d-9a58-4afb-8aea-30b1d83e0fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# representation bias\n",
    "df_repr_no_inter = pd.read_csv('fid_repr_no_inter.csv')\n",
    "df_repr_inter = pd.read_csv('fid_repr_inter.csv')\n",
    "df_repr_diff_base = pd.read_csv('fid_repr_diff_base.csv')\n",
    "\n",
    "# label noise bias\n",
    "\n",
    "# feature missingness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee85f6ab-85be-4517-a6a4-018b0fcd1bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "plt.errorbar(df_repr.bias_amts, df_repr.mean_fidel_maj, yerr = df_repr.y_err_fidel_maj, label = 'Fidelity Majority', color = \"red\")\n",
    "plt.errorbar(df_repr.bias_amts, df_repr.mean_fidel_min, yerr = df_repr.y_err_fidel_min, label = 'Fidelity Minority', color = \"green\")\n",
    "plt.xlabel(\"Beta Value for Minority\")\n",
    "plt.ylabel(\"Fidelity\")\n",
    "plt.xlim(1.05, -0.05)\n",
    "plt.ylim(0.92, 1)\n",
    "plt.legend()\n",
    "plt.title(\"Fidelity Plot\")\n",
    "plt.savefig('repr_no_inter.jpg')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
